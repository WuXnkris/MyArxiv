{"2025-03-06T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2503.04725v1","updated":"2025-03-06T18:59:48Z","published":"2025-03-06T18:59:48Z","title":"L$^2$M: Mutual Information Scaling Law for Long-Context Language\n  Modeling","summary":"  We rigorously establish a bipartite mutual information scaling law in natural\nlanguage that governs long-range dependencies. This scaling law, which we show\nis distinct from and scales independently of the conventional two-point mutual\ninformation, is the key to understanding long-context language modeling. Using\nthis scaling law, we formulate the Long-context Language Modeling (L$^2$M)\ncondition, which relates a model's capacity for effective long context length\nmodeling to the scaling of its latent state size for storing past information.\nOur results are validated through experiments on both transformers and state\nspace models. This work establishes a theoretical foundation that guides the\ndevelopment of large language models toward longer context lengths.\n","authors":["Zhuo Chen","Oriol Mayné i Comas","Zhuotao Jin","Di Luo","Marin Soljačić"],"pdf_url":"https://arxiv.org/pdf/2503.04725v1.pdf","comment":"29 pages, 12 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.04724v1","updated":"2025-03-06T18:59:38Z","published":"2025-03-06T18:59:38Z","title":"LLMVoX: Autoregressive Streaming Text-to-Speech Model for Any LLM","summary":"  Recent advancements in speech-to-speech dialogue systems leverage LLMs for\nmultimodal interactions, yet they remain hindered by fine-tuning requirements,\nhigh computational overhead, and text-speech misalignment. Existing\nspeech-enabled LLMs often degrade conversational quality by modifying the LLM,\nthereby compromising its linguistic capabilities. In contrast, we propose\nLLMVoX, a lightweight 30M-parameter, LLM-agnostic, autoregressive streaming TTS\nsystem that generates high-quality speech with low latency, while fully\npreserving the capabilities of the base LLM. Our approach achieves a\nsignificantly lower Word Error Rate compared to speech-enabled LLMs, while\noperating at comparable latency and UTMOS score. By decoupling speech synthesis\nfrom LLM processing via a multi-queue token streaming system, LLMVoX supports\nseamless, infinite-length dialogues. Its plug-and-play design also facilitates\nextension to various tasks with different backbones. Furthermore, LLMVoX\ngeneralizes to new languages with only dataset adaptation, attaining a low\nCharacter Error Rate on an Arabic speech task. Additionally, we have integrated\nLLMVoX with a Vision-Language Model to create an omni-model with speech, text,\nand vision capabilities, without requiring additional multimodal training. Our\ncode base and project page is available at https://mbzuai-oryx.github.io/LLMVoX .\n","authors":["Sambal Shikhar","Mohammed Irfan Kurpath","Sahal Shaji Mullappilly","Jean Lahoud","Fahad Khan","Rao Muhammad Anwer","Salman Khan","Hisham Cholakkal"],"pdf_url":"https://arxiv.org/pdf/2503.04724v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04723v1","updated":"2025-03-06T18:59:37Z","published":"2025-03-06T18:59:37Z","title":"Shifting Long-Context LLMs Research from Input to Output","summary":"  Recent advancements in long-context Large Language Models (LLMs) have\nprimarily concentrated on processing extended input contexts, resulting in\nsignificant strides in long-context comprehension. However, the equally\ncritical aspect of generating long-form outputs has received comparatively less\nattention. This paper advocates for a paradigm shift in NLP research toward\naddressing the challenges of long-output generation. Tasks such as novel\nwriting, long-term planning, and complex reasoning require models to understand\nextensive contexts and produce coherent, contextually rich, and logically\nconsistent extended text. These demands highlight a critical gap in current LLM\ncapabilities. We underscore the importance of this under-explored domain and\ncall for focused efforts to develop foundational LLMs tailored for generating\nhigh-quality, long-form outputs, which hold immense potential for real-world\napplications.\n","authors":["Yuhao Wu","Yushi Bai","Zhiqing Hu","Shangqing Tu","Ming Shan Hee","Juanzi Li","Roy Ka-Wei Lee"],"pdf_url":"https://arxiv.org/pdf/2503.04723v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.04722v1","updated":"2025-03-06T18:59:23Z","published":"2025-03-06T18:59:23Z","title":"Enough Coin Flips Can Make LLMs Act Bayesian","summary":"  Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner.\n","authors":["Ritwik Gupta","Rodolfo Corona","Jiaxin Ge","Eric Wang","Dan Klein","Trevor Darrell","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2503.04722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04721v1","updated":"2025-03-06T18:59:16Z","published":"2025-03-06T18:59:16Z","title":"Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue\n  Models on Turn-taking Capabilities","summary":"  Spoken dialogue modeling introduces unique challenges beyond text-based\nlanguage modeling, demanding robust turn-taking, backchanneling, and real-time\ninteraction. Although most Spoken Dialogue Models (SDMs) rely on half-duplex\nprocessing (handling speech one turn at a time), emerging full-duplex SDMs can\nlisten and speak simultaneously, enabling more natural and engaging\nconversations. However, current evaluations of such models remain limited,\noften focusing on turn-based metrics or high-level corpus analyses (e.g., turn\ngaps, pauses). To address this gap, we present Full-Duplex-Bench, a new\nbenchmark that systematically evaluates key conversational behaviors: pause\nhandling, backchanneling, turn-taking, and interruption management. Our\nframework uses automatic metrics for consistent and reproducible assessments of\nSDMs' interactive performance. By offering an open and standardized evaluation\nbenchmark, we aim to advance spoken dialogue modeling and encourage the\ndevelopment of more interactive and natural dialogue systems.\n","authors":["Guan-Ting Lin","Jiachen Lian","Tingle Li","Qirui Wang","Gopala Anumanchipalli","Alexander H. Liu","Hung-yi Lee"],"pdf_url":"https://arxiv.org/pdf/2503.04721v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.11807v7","updated":"2025-03-06T18:58:23Z","published":"2024-03-18T14:04:47Z","title":"How Far Are We on the Decision-Making of LLMs? Evaluating LLMs' Gaming\n  Ability in Multi-Agent Environments","summary":"  Decision-making is a complex process requiring diverse abilities, making it\nan excellent framework for evaluating Large Language Models (LLMs). Researchers\nhave examined LLMs' decision-making through the lens of Game Theory. However,\nexisting evaluation mainly focus on two-player scenarios where an LLM competes\nagainst another. Additionally, previous benchmarks suffer from test set leakage\ndue to their static design. We introduce GAMA($\\gamma$)-Bench, a new framework\nfor evaluating LLMs' Gaming Ability in Multi-Agent environments. It includes\neight classical game theory scenarios and a dynamic scoring scheme specially\ndesigned to quantitatively assess LLMs' performance. $\\gamma$-Bench allows\nflexible game settings and adapts the scoring system to different game\nparameters, enabling comprehensive evaluation of robustness, generalizability,\nand strategies for improvement. Our results indicate that GPT-3.5 demonstrates\nstrong robustness but limited generalizability, which can be enhanced using\nmethods like Chain-of-Thought. We also evaluate 13 LLMs from 6 model families,\nincluding GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2.\nGemini-1.5-Pro outperforms others, scoring of $69.8$ out of $100$, followed by\nLLaMA-3.1-70B ($65.9$) and Mixtral-8x22B ($62.4$). Our code and experimental\nresults are publicly available at https://github.com/CUHK-ARISE/GAMABench.\n","authors":["Jen-tse Huang","Eric John Li","Man Ho Lam","Tian Liang","Wenxuan Wang","Youliang Yuan","Wenxiang Jiao","Xing Wang","Zhaopeng Tu","Michael R. Lyu"],"pdf_url":"https://arxiv.org/pdf/2403.11807v7.pdf","comment":"Accepted to ICLR 2025; 11 pages of main text; 26 pages of appendices;\n  Included models: GPT-3.5-{0613, 1106, 0125}, GPT-4-0125, GPT-4o-0806,\n  Gemini-{1.0, 1.5)-Pro, LLaMA-3.1-{7, 70, 405}B, Mixtral-8x{7, 22}B,\n  Qwen-2-72B"},{"id":"http://arxiv.org/abs/2503.04713v1","updated":"2025-03-06T18:57:40Z","published":"2025-03-06T18:57:40Z","title":"Scaling Rich Style-Prompted Text-to-Speech Datasets","summary":"  We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale\ndataset that annotates speech utterances with rich style captions. While rich\nabstract tags (e.g. guttural, nasal, pained) have been explored in small-scale\nhuman-annotated datasets, existing large-scale datasets only cover basic tags\n(e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech\nembedders, classifiers and an audio language model to automatically scale rich\ntag annotations for the first time. ParaSpeechCaps covers a total of 59 style\ntags, including both speaker-level intrinsic tags and utterance-level\nsituational tags. It consists of 342 hours of human-labelled data (PSC-Base)\nand 2427 hours of automatically annotated data (PSC-Scaled). We finetune\nParler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and\nachieve improved style consistency (+7.9% Consistency MOS) and speech quality\n(+15.5% Naturalness MOS) over the best performing baseline that combines\nexisting rich style tag datasets. We ablate several of our dataset design\nchoices to lay the foundation for future work in this space. Our dataset,\nmodels and code are released at https://github.com/ajd12342/paraspeechcaps .\n","authors":["Anuj Diwan","Zhisheng Zheng","David Harwath","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2503.04713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04697v1","updated":"2025-03-06T18:43:29Z","published":"2025-03-06T18:43:29Z","title":"L1: Controlling How Long A Reasoning Model Thinks With Reinforcement\n  Learning","summary":"  Reasoning language models have shown an uncanny ability to improve\nperformance at test-time by ``thinking longer''-that is, by generating longer\nchain-of-thought sequences and hence using more compute. However, the length of\ntheir chain-of-thought reasoning is not controllable, making it impossible to\nallocate test-time compute to achieve a desired level of performance. We\nintroduce Length Controlled Policy Optimization (LCPO), a simple reinforcement\nlearning method that optimizes for accuracy and adherence to user-specified\nlength constraints. We use LCPO to train L1, a reasoning language model that\nproduces outputs satisfying a length constraint given in its prompt. L1's\nlength control allows for smoothly trading off computational cost and accuracy\non a wide range of tasks, and outperforms the state-of-the-art S1 method for\nlength control. Furthermore, we uncover an unexpected short chain-of-thought\ncapability in models trained with LCPO. For instance, our 1.5B L1 model\nsurpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise\ncontrol over reasoning length, allowing for fine-grained allocation of\ntest-time compute and accuracy. We release code and models at\nhttps://www.cmu-l3.github.io/l1\n","authors":["Pranjal Aggarwal","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2503.04697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02694v3","updated":"2025-03-06T18:41:54Z","published":"2024-10-03T17:20:11Z","title":"HELMET: How to Evaluate Long-Context Language Models Effectively and\n  Thoroughly","summary":"  Many benchmarks exist for evaluating long-context language models (LCLMs),\nyet developers often rely on synthetic tasks such as needle-in-a-haystack\n(NIAH) or an arbitrary subset of tasks. However, it remains unclear whether\nthese benchmarks reflect the diverse downstream applications of LCLMs, and such\ninconsistencies further complicate model comparison. We investigate the\nunderlying reasons behind these practices and find that existing benchmarks\noften provide noisy signals due to limited coverage of applications,\ninsufficient context lengths, unreliable metrics, and incompatibility with base\nmodels. In this work, we introduce HELMET (How to Evaluate Long-context Models\nEffectively and Thoroughly), a comprehensive benchmark encompassing seven\ndiverse, application-centric categories. We also address several issues in\nprevious benchmarks by adding controllable lengths up to 128K tokens,\nmodel-based evaluation for reliable metrics, and few-shot prompting for\nrobustly evaluating base models. Consequently, we demonstrate that HELMET\noffers more reliable and consistent rankings of frontier LCLMs. Through a\ncomprehensive study of 59 LCLMs, we find that (1) synthetic tasks like NIAH do\nnot reliably predict downstream performance; (2) the diverse categories in\nHELMET exhibit distinct trends and low correlations with each other; and (3)\nwhile most LCLMs achieve perfect NIAH scores, open-source models significantly\nlag behind closed ones when tasks require full-context reasoning or following\ncomplex instructions -- the gap widens as length increases. Finally, we\nrecommend using our RAG tasks for fast model development, as they are easy to\nrun and better predict other downstream performance; ultimately, we advocate\nfor a holistic evaluation across diverse tasks.\n","authors":["Howard Yen","Tianyu Gao","Minmin Hou","Ke Ding","Daniel Fleischer","Peter Izsak","Moshe Wasserblat","Danqi Chen"],"pdf_url":"https://arxiv.org/pdf/2410.02694v3.pdf","comment":"ICLR 2025. Project page: https://princeton-nlp.github.io/HELMET/"},{"id":"http://arxiv.org/abs/2503.04693v1","updated":"2025-03-06T18:40:00Z","published":"2025-03-06T18:40:00Z","title":"UIPE: Enhancing LLM Unlearning by Removing Knowledge Related to\n  Forgetting Targets","summary":"  Large Language Models (LLMs) inevitably acquire harmful information during\ntraining on massive datasets. LLM unlearning aims to eliminate the influence of\nsuch harmful information while maintaining the model's overall performance.\nExisting unlearning methods, represented by gradient ascent-based approaches,\nprimarily focus on forgetting target data while overlooking the crucial impact\nof logically related knowledge on the effectiveness of unlearning. In this\npaper, through both theoretical and experimental analyses, we first demonstrate\nthat a key reason for the suboptimal unlearning performance is that models can\nreconstruct the target content through reasoning with logically related\nknowledge. To address this issue, we propose Unlearning Improvement via\nParameter Extrapolation (UIPE), a method that removes knowledge highly\ncorrelated with the forgetting targets. Experimental results show that UIPE\nsignificantly enhances the performance of various mainstream LLM unlearning\nmethods on the TOFU benchmark.\n","authors":["Wenyu Wang","Mengqi Zhang","Xiaotian Ye","Zhaochun Ren","Zhumin Chen","Pengjie Ren"],"pdf_url":"https://arxiv.org/pdf/2503.04693v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04691v1","updated":"2025-03-06T18:35:39Z","published":"2025-03-06T18:35:39Z","title":"Quantifying the Reasoning Abilities of LLMs on Real-world Clinical Cases","summary":"  The latest reasoning-enhanced large language models (reasoning LLMs), such as\nDeepSeek-R1 and OpenAI-o3, have demonstrated remarkable success. However, the\napplication of such reasoning enhancements to the highly professional medical\ndomain has not been clearly evaluated, particularly regarding with not only\nassessing the final generation but also examining the quality of their\nreasoning processes. In this study, we present MedR-Bench, a reasoning-focused\nmedical evaluation benchmark comprising 1,453 structured patient cases with\nreasoning references mined from case reports. Our benchmark spans 13 body\nsystems and 10 specialty disorders, encompassing both common and rare diseases.\nIn our evaluation, we introduce a versatile framework consisting of three\ncritical clinical stages: assessment recommendation, diagnostic\ndecision-making, and treatment planning, comprehensively capturing the LLMs'\nperformance across the entire patient journey in healthcare. For metrics, we\npropose a novel agentic system, Reasoning Evaluator, designed to automate and\nobjectively quantify free-text reasoning responses in a scalable manner from\nthe perspectives of efficiency, factuality, and completeness by dynamically\nsearching and performing cross-referencing checks. As a result, we assess five\nstate-of-the-art reasoning LLMs, including DeepSeek-R1, OpenAI-o3-mini, and\nothers. Our results reveal that current LLMs can handle relatively simple\ndiagnostic tasks with sufficient critical assessment results, achieving\naccuracy generally over 85%. However, they still struggle with more complex\ntasks, such as assessment recommendation and treatment planning. In reasoning,\ntheir reasoning processes are generally reliable, with factuality scores\nexceeding 90%, though they often omit critical reasoning steps. Our study\nclearly reveals further development directions for current clinical LLMs.\n","authors":["Pengcheng Qiu","Chaoyi Wu","Shuyu Liu","Weike Zhao","Ya Zhang","Yanfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2503.04691v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04685v1","updated":"2025-03-06T18:27:41Z","published":"2025-03-06T18:27:41Z","title":"DIMSUM: Discourse in Mathematical Reasoning as a Supervision Module","summary":"  We look at reasoning on GSM8k, a dataset of short texts presenting primary\nschool, math problems. We find, with Mirzadeh et al. (2024), that current LLM\nprogress on the data set may not be explained by better reasoning but by\nexposure to a broader pretraining data distribution. We then introduce a novel\ninformation source for helping models with less data or inferior training\nreason better: discourse structure. We show that discourse structure improves\nperformance for models like Llama2 13b by up to 160%. Even for models that have\nmost likely memorized the data set, adding discourse structural information to\nthe model still improves predictions and dramatically improves large model\nperformance on out of distribution examples.\n","authors":["Krish Sharma","Niyar R Barman","Nicholas Asher","Akshay Chaturvedi"],"pdf_url":"https://arxiv.org/pdf/2503.04685v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04675v1","updated":"2025-03-06T18:12:33Z","published":"2025-03-06T18:12:33Z","title":"LLM-guided Plan and Retrieval: A Strategic Alignment for Interpretable\n  User Satisfaction Estimation in Dialogue","summary":"  Understanding user satisfaction with conversational systems, known as User\nSatisfaction Estimation (USE), is essential for assessing dialogue quality and\nenhancing user experiences. However, existing methods for USE face challenges\ndue to limited understanding of underlying reasons for user dissatisfaction and\nthe high costs of annotating user intentions. To address these challenges, we\npropose PRAISE (Plan and Retrieval Alignment for Interpretable Satisfaction\nEstimation), an interpretable framework for effective user satisfaction\nprediction. PRAISE operates through three key modules. The Strategy Planner\ndevelops strategies, which are natural language criteria for classifying user\nsatisfaction. The Feature Retriever then incorporates knowledge on user\nsatisfaction from Large Language Models (LLMs) and retrieves relevance features\nfrom utterances. Finally, the Score Analyzer evaluates strategy predictions and\nclassifies user satisfaction. Experimental results demonstrate that PRAISE\nachieves state-of-the-art performance on three benchmarks for the USE task.\nBeyond its superior performance, PRAISE offers additional benefits. It enhances\ninterpretability by providing instance-level explanations through effective\nalignment of utterances with strategies. Moreover, PRAISE operates more\nefficiently than existing approaches by eliminating the need for LLMs during\nthe inference phase.\n","authors":["Sangyeop Kim","Sohhyung Park","Jaewon Jung","Jinseok Kim","Sungzoon Cho"],"pdf_url":"https://arxiv.org/pdf/2503.04675v1.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2502.02067v2","updated":"2025-03-06T18:09:38Z","published":"2025-02-04T07:32:39Z","title":"AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement","summary":"  An embodied agent assisting humans is often asked to complete new tasks, and\nthere may not be sufficient time or labeled examples to train the agent to\nperform these new tasks. Large Language Models (LLMs) trained on considerable\nknowledge across many domains can be used to predict a sequence of abstract\nactions for completing such tasks, although the agent may not be able to\nexecute this sequence due to task-, agent-, or domain-specific constraints. Our\nframework addresses these challenges by leveraging the generic predictions\nprovided by LLM and the prior domain knowledge encoded in a Knowledge Graph\n(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits\nand uses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation in the context of cooking and cleaning tasks in\nsimulation domains, we demonstrate that the interplay between LLM, KG, and\nhuman input leads to substantial performance gains compared with just using the\nLLM. Project website{\\S}: https://sssshivvvv.github.io/adaptbot/\n","authors":["Shivam Singh","Karthik Swaminathan","Nabanita Dash","Ramandeep Singh","Snehasis Banerjee","Mohan Sridharan","Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2502.02067v2.pdf","comment":"Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2025"},{"id":"http://arxiv.org/abs/2503.04667v1","updated":"2025-03-06T17:59:51Z","published":"2025-03-06T17:59:51Z","title":"An Information-theoretic Multi-task Representation Learning Framework\n  for Natural Language Understanding","summary":"  This paper proposes a new principled multi-task representation learning\nframework (InfoMTL) to extract noise-invariant sufficient representations for\nall tasks. It ensures sufficiency of shared representations for all tasks and\nmitigates the negative effect of redundant features, which can enhance language\nunderstanding of pre-trained language models (PLMs) under the multi-task\nparadigm. Firstly, a shared information maximization principle is proposed to\nlearn more sufficient shared representations for all target tasks. It can avoid\nthe insufficiency issue arising from representation compression in the\nmulti-task paradigm. Secondly, a task-specific information minimization\nprinciple is designed to mitigate the negative effect of potential redundant\nfeatures in the input for each task. It can compress task-irrelevant redundant\ninformation and preserve necessary information relevant to the target for\nmulti-task prediction. Experiments on six classification benchmarks show that\nour method outperforms 12 comparative multi-task methods under the same\nmulti-task settings, especially in data-constrained and noisy scenarios.\nExtensive experiments demonstrate that the learned representations are more\nsufficient, data-efficient, and robust.\n","authors":["Dou Hu","Lingwei Wei","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2503.04667v1.pdf","comment":"11 pages, accepted to AAAI 2025 (main conference), the code is\n  available at https://github.com/zerohd4869/InfoMTL"},{"id":"http://arxiv.org/abs/2502.16600v4","updated":"2025-03-06T17:56:40Z","published":"2025-02-23T15:00:53Z","title":"Diagnosing Moral Reasoning Acquisition in Language Models: Pragmatics\n  and Generalization","summary":"  Ensuring that Large Language Models (LLMs) return just responses which adhere\nto societal values is crucial for their broader application. Prior research has\nshown that LLMs often fail to perform satisfactorily on tasks requiring moral\ncognizance, such as ethics-based judgments. While current approaches have\nfocused on fine-tuning LLMs with curated datasets to improve their capabilities\non such tasks, choosing the optimal learning paradigm to enhance the ethical\nresponses of LLMs remains an open research debate. In this work, we aim to\naddress this fundamental question: can current learning paradigms enable LLMs\nto acquire sufficient moral reasoning capabilities? Drawing from distributional\nsemantics theory and the pragmatic nature of moral discourse, our analysis\nindicates that performance improvements follow a mechanism similar to that of\nsemantic-level tasks, and therefore remain affected by the pragmatic nature of\nmorals latent in discourse, a phenomenon we name the pragmatic dilemma. We\nconclude that this pragmatic dilemma imposes significant limitations on the\ngeneralization ability of current learning paradigms, making it the primary\nbottleneck for moral reasoning acquisition in LLMs.\n","authors":["Guangliang Liu","Lei Jiang","Xitong Zhang","Kristen Marie Johnson"],"pdf_url":"https://arxiv.org/pdf/2502.16600v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00799v6","updated":"2025-03-06T17:43:10Z","published":"2024-06-02T16:53:21Z","title":"Get my drift? Catching LLM Task Drift with Activation Deltas","summary":"  LLMs are commonly used in retrieval-augmented applications to execute user\ninstructions based on data from external sources. For example, modern search\nengines use LLMs to answer queries based on relevant search results; email\nplugins summarize emails by processing their content through an LLM. However,\nthe potentially untrusted provenance of these data sources can lead to prompt\ninjection attacks, where the LLM is manipulated by natural language\ninstructions embedded in the external data, causing it to deviate from the\nuser's original instruction(s). We define this deviation as task drift. Task\ndrift is a significant concern as it allows attackers to exfiltrate data or\ninfluence the LLM's output for other users. We study LLM activations as a\nsolution to detect task drift, showing that activation deltas - the difference\nin activations before and after processing external data - are strongly\ncorrelated with this phenomenon. Through two probing methods, we demonstrate\nthat a simple linear classifier can detect drift with near-perfect ROC AUC on\nan out-of-distribution test set. We evaluate these methods by making minimal\nassumptions about how users' tasks, system prompts, and attacks can be phrased.\nWe observe that this approach generalizes surprisingly well to unseen task\ndomains, such as prompt injections, jailbreaks, and malicious instructions,\nwithout being trained on any of these attacks. Interestingly, the fact that\nthis solution does not require any modifications to the LLM (e.g.,\nfine-tuning), as well as its compatibility with existing meta-prompting\nsolutions, makes it cost-efficient and easy to deploy. To encourage further\nresearch on activation-based task inspection, decoding, and interpretability,\nwe release our large-scale TaskTracker toolkit, featuring a dataset of over\n500K instances, representations from six SoTA language models, and a suite of\ninspection tools.\n","authors":["Sahar Abdelnabi","Aideen Fay","Giovanni Cherubin","Ahmed Salem","Mario Fritz","Andrew Paverd"],"pdf_url":"https://arxiv.org/pdf/2406.00799v6.pdf","comment":"SaTML 2025"},{"id":"http://arxiv.org/abs/2503.04647v1","updated":"2025-03-06T17:33:01Z","published":"2025-03-06T17:33:01Z","title":"Implicit Cross-Lingual Rewarding for Efficient Multilingual Preference\n  Alignment","summary":"  Direct Preference Optimization (DPO) has become a prominent method for\naligning Large Language Models (LLMs) with human preferences. While DPO has\nenabled significant progress in aligning English LLMs, multilingual preference\nalignment is hampered by data scarcity. To address this, we propose a novel\napproach that $\\textit{captures}$ learned preferences from well-aligned English\nmodels by implicit rewards and $\\textit{transfers}$ them to other languages\nthrough iterative training. Specifically, we derive an implicit reward model\nfrom the logits of an English DPO-aligned model and its corresponding reference\nmodel. This reward model is then leveraged to annotate preference relations in\ncross-lingual instruction-following pairs, using English instructions to\nevaluate multilingual responses. The annotated data is subsequently used for\nmultilingual DPO fine-tuning, facilitating preference knowledge transfer from\nEnglish to other languages. Fine-tuning Llama3 for two iterations resulted in a\n12.72% average improvement in Win Rate and a 5.97% increase in Length Control\nWin Rate across all training languages on the X-AlpacaEval leaderboard. Our\nfindings demonstrate that leveraging existing English-aligned models can enable\nefficient and effective multilingual preference alignment, significantly\nreducing the need for extensive multilingual preference data. The code is\navailable at https://github.com/ZNLP/Implicit-Cross-Lingual-Rewarding\n","authors":["Wen Yang","Junhong Wu","Chen Wang","Chengqing Zong","Jiajun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04647v1.pdf","comment":"Work in progress"},{"id":"http://arxiv.org/abs/2503.04644v1","updated":"2025-03-06T17:32:22Z","published":"2025-03-06T17:32:22Z","title":"IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in\n  Expert-Domain Information Retrieval","summary":"  We introduce IFIR, the first comprehensive benchmark designed to evaluate\ninstruction-following information retrieval (IR) in expert domains. IFIR\nincludes 2,426 high-quality examples and covers eight subsets across four\nspecialized domains: finance, law, healthcare, and science literature. Each\nsubset addresses one or more domain-specific retrieval tasks, replicating\nreal-world scenarios where customized instructions are critical. IFIR enables a\ndetailed analysis of instruction-following retrieval capabilities by\nincorporating instructions at different levels of complexity. We also propose a\nnovel LLM-based evaluation method to provide a more precise and reliable\nassessment of model performance in following instructions. Through extensive\nexperiments on 15 frontier retrieval models, including those based on LLMs, our\nresults reveal that current models face significant challenges in effectively\nfollowing complex, domain-specific instructions. We further provide in-depth\nanalyses to highlight these limitations, offering valuable insights to guide\nfuture advancements in retriever development.\n","authors":["Tingyu Song","Guo Gan","Mingsheng Shang","Yilun Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.04644v1.pdf","comment":"NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2503.04636v1","updated":"2025-03-06T17:24:06Z","published":"2025-03-06T17:24:06Z","title":"Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models\n  via Watermarking","summary":"  As open-source large language models (LLMs) like Llama3 become more capable,\nit is crucial to develop watermarking techniques to detect their potential\nmisuse. Existing watermarking methods either add watermarks during LLM\ninference, which is unsuitable for open-source LLMs, or primarily target\nclassification LLMs rather than recent generative LLMs. Adapting these\nwatermarks to open-source LLMs for misuse detection remains an open challenge.\nThis work defines two misuse scenarios for open-source LLMs: intellectual\nproperty (IP) violation and LLM Usage Violation. Then, we explore the\napplication of inference-time watermark distillation and backdoor watermarking\nin these contexts. We propose comprehensive evaluation methods to assess the\nimpact of various real-world further fine-tuning scenarios on watermarks and\nthe effect of these watermarks on LLM performance. Our experiments reveal that\nbackdoor watermarking could effectively detect IP Violation, while\ninference-time watermark distillation is applicable in both scenarios but less\nrobust to further fine-tuning and has a more significant impact on LLM\nperformance compared to backdoor watermarking. Exploring more advanced\nwatermarking methods for open-source LLMs to detect their misuse should be an\nimportant future direction.\n","authors":["Yijie Xu","Aiwei Liu","Xuming Hu","Lijie Wen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.04636v1.pdf","comment":"Accepted by the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04629v1","updated":"2025-03-06T17:15:48Z","published":"2025-03-06T17:15:48Z","title":"SurveyForge: On the Outline Heuristics, Memory-Driven Generation, and\n  Multi-dimensional Evaluation for Automated Survey Writing","summary":"  Survey paper plays a crucial role in scientific research, especially given\nthe rapid growth of research publications. Recently, researchers have begun\nusing LLMs to automate survey generation for better efficiency. However, the\nquality gap between LLM-generated surveys and those written by human remains\nsignificant, particularly in terms of outline quality and citation accuracy. To\nclose these gaps, we introduce SurveyForge, which first generates the outline\nby analyzing the logical structure of human-written outlines and referring to\nthe retrieved domain-related articles. Subsequently, leveraging high-quality\npapers retrieved from memory by our scholar navigation agent, SurveyForge can\nautomatically generate and refine the content of the generated article.\nMoreover, to achieve a comprehensive evaluation, we construct SurveyBench,\nwhich includes 100 human-written survey papers for win-rate comparison and\nassesses AI-generated survey papers across three dimensions: reference,\noutline, and content quality. Experiments demonstrate that SurveyForge can\noutperform previous works such as AutoSurvey.\n","authors":["Xiangchao Yan","Shiyang Feng","Jiakang Yuan","Renqiu Xia","Bin Wang","Bo Zhang","Lei Bai"],"pdf_url":"https://arxiv.org/pdf/2503.04629v1.pdf","comment":"Code and dataset are available for downloading at:\n  https://github.com/Alpha-Innovator/SurveyForge 22 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.04625v1","updated":"2025-03-06T17:11:51Z","published":"2025-03-06T17:11:51Z","title":"START: Self-taught Reasoner with Tools","summary":"  Large reasoning models (LRMs) like OpenAI-o1 and DeepSeek-R1 have\ndemonstrated remarkable capabilities in complex reasoning tasks through the\nutilization of long Chain-of-thought (CoT). However, these models often suffer\nfrom hallucinations and inefficiencies due to their reliance solely on internal\nreasoning processes. In this paper, we introduce START (Self-Taught Reasoner\nwith Tools), a novel tool-integrated long CoT reasoning LLM that significantly\nenhances reasoning capabilities by leveraging external tools. Through code\nexecution, START is capable of performing complex computations, self-checking,\nexploring diverse methods, and self-debugging, thereby addressing the\nlimitations of LRMs. The core innovation of START lies in its self-learning\nframework, which comprises two key techniques: 1) Hint-infer: We demonstrate\nthat inserting artificially designed hints (e.g., ``Wait, maybe using Python\nhere is a good idea.'') during the inference process of a LRM effectively\nstimulates its ability to utilize external tools without the need for any\ndemonstration data. Hint-infer can also serve as a simple and effective\nsequential test-time scaling method; 2) Hint Rejection Sampling Fine-Tuning\n(Hint-RFT): Hint-RFT combines Hint-infer and RFT by scoring, filtering, and\nmodifying the reasoning trajectories with tool invocation generated by a LRM\nvia Hint-infer, followed by fine-tuning the LRM. Through this framework, we\nhave fine-tuned the QwQ-32B model to achieve START. On PhD-level science QA\n(GPQA), competition-level math benchmarks (AMC23, AIME24, AIME25), and the\ncompetition-level code benchmark (LiveCodeBench), START achieves accuracy rates\nof 63.6%, 95.0%, 66.7%, 47.1%, and 47.3%, respectively. It significantly\noutperforms the base QwQ-32B and achieves performance comparable to the\nstate-of-the-art open-weight model R1-Distill-Qwen-32B and the proprietary\nmodel o1-Preview.\n","authors":["Chengpeng Li","Mingfeng Xue","Zhenru Zhang","Jiaxi Yang","Beichen Zhang","Xiang Wang","Bowen Yu","Binyuan Hui","Junyang Lin","Dayiheng Liu"],"pdf_url":"https://arxiv.org/pdf/2503.04625v1.pdf","comment":"38 pages, 5 figures and 6 tables"},{"id":"http://arxiv.org/abs/2503.04619v1","updated":"2025-03-06T17:05:33Z","published":"2025-03-06T17:05:33Z","title":"SynGraph: A Dynamic Graph-LLM Synthesis Framework for Sparse Streaming\n  User Sentiment Modeling","summary":"  User reviews on e-commerce platforms exhibit dynamic sentiment patterns\ndriven by temporal and contextual factors. Traditional sentiment analysis\nmethods focus on static reviews, failing to capture the evolving temporal\nrelationship between user sentiment rating and textual content. Sentiment\nanalysis on streaming reviews addresses this limitation by modeling and\npredicting the temporal evolution of user sentiments. However, it suffers from\ndata sparsity, manifesting in temporal, spatial, and combined forms. In this\npaper, we introduce SynGraph, a novel framework designed to address data\nsparsity in sentiment analysis on streaming reviews. SynGraph alleviates data\nsparsity by categorizing users into mid-tail, long-tail, and extreme scenarios\nand incorporating LLM-augmented enhancements within a dynamic graph-based\nstructure. Experiments on real-world datasets demonstrate its effectiveness in\naddressing sparsity and improving sentiment modeling in streaming reviews.\n","authors":["Xin Zhang","Qiyu Wei","Yingjie Zhu","Linhai Zhang","Deyu Zhou","Sophia Ananiadou"],"pdf_url":"https://arxiv.org/pdf/2503.04619v1.pdf","comment":"18 pages, 17 figures"},{"id":"http://arxiv.org/abs/2503.04618v1","updated":"2025-03-06T17:03:17Z","published":"2025-03-06T17:03:17Z","title":"Better Process Supervision with Bi-directional Rewarding Signals","summary":"  Process supervision, i.e., evaluating each step, is critical for complex\nlarge language model (LLM) reasoning and test-time searching with increased\ninference compute. Existing approaches, represented by process reward models\n(PRMs), primarily focus on rewarding signals up to the current step, exhibiting\na one-directional nature and lacking a mechanism to model the distance to the\nfinal target. To address this problem, we draw inspiration from the A*\nalgorithm, which states that an effective supervisory signal should\nsimultaneously consider the incurred cost and the estimated cost for reaching\nthe target. Building on this key insight, we introduce BiRM, a novel process\nsupervision model that not only evaluates the correctness of previous steps but\nalso models the probability of future success. We conduct extensive experiments\non mathematical reasoning tasks and demonstrate that BiRM provides more precise\nevaluations of LLM reasoning steps, achieving an improvement of 3.1% on\nGaokao2023 over PRM under the Best-of-N sampling method. Besides, in\nsearch-based strategies, BiRM provides more comprehensive guidance and\noutperforms ORM by 5.0% and PRM by 3.8% respectively on MATH-500.\n","authors":["Wenxiang Chen","Wei He","Zhiheng Xi","Honglin Guo","Boyang Hong","Jiazheng Zhang","Rui Zheng","Nijun Li","Tao Gui","Yun Li","Qi Zhang","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2503.04618v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04615v1","updated":"2025-03-06T16:59:18Z","published":"2025-03-06T16:59:18Z","title":"HalluCounter: Reference-free LLM Hallucination Detection in the Wild!","summary":"  Response consistency-based, reference-free hallucination detection (RFHD)\nmethods do not depend on internal model states, such as generation\nprobabilities or gradients, which Grey-box models typically rely on but are\ninaccessible in closed-source LLMs. However, their inability to capture\nquery-response alignment patterns often results in lower detection accuracy.\nAdditionally, the lack of large-scale benchmark datasets spanning diverse\ndomains remains a challenge, as most existing datasets are limited in size and\nscope. To this end, we propose HalluCounter, a novel reference-free\nhallucination detection method that utilizes both response-response and\nquery-response consistency and alignment patterns. This enables the training of\na classifier that detects hallucinations and provides a confidence score and an\noptimal response for user queries. Furthermore, we introduce HalluCounterEval,\na benchmark dataset comprising both synthetically generated and human-curated\nsamples across multiple domains. Our method outperforms state-of-the-art\napproaches by a significant margin, achieving over 90\\% average confidence in\nhallucination detection across datasets.\n","authors":["Ashok Urlana","Gopichand Kanumolu","Charaka Vinayak Kumar","Bala Mallikarjunarao Garlapati","Rahul Mishra"],"pdf_url":"https://arxiv.org/pdf/2503.04615v1.pdf","comment":"30 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.04611v1","updated":"2025-03-06T16:57:26Z","published":"2025-03-06T16:57:26Z","title":"Towards Data-Efficient Language Models: A Child-Inspired Approach to\n  Language Learning","summary":"  In this work, we explain our approach employed in the BabyLM Challenge, which\nuses various methods of training language models (LMs) with significantly less\ndata compared to traditional large language models (LLMs) and are inspired by\nhow human children learn. While a human child is exposed to far less linguistic\ninput than an LLM, they still achieve remarkable language understanding and\ngeneration abilities. To this end, we develop a model trained on a curated\ndataset consisting of 10 million words, primarily sourced from child-directed\ntranscripts. The 2024 BabyLM Challenge initial dataset of 10M words is filtered\nto 8.5M. Next, it is supplemented with a randomly selected subset of TVR\ndataset consisting of 1.5M words of television dialogues. The latter dataset\nensures that similar to children, the model is also exposed to language through\nmedia. Furthermore, we reduce the vocabulary size to 32,000 tokens, aligning it\nwith the limited vocabulary of children in the early stages of language\nacquisition. We use curriculum learning and is able to match the baseline on\ncertain benchmarks while surpassing the baseline on others. Additionally,\nincorporating common LLM training datasets, such as MADLAD-400, degrades\nperformance. These findings underscore the importance of dataset selection,\nvocabulary scaling, and curriculum learning in creating more data-efficient\nlanguage models that better mimic human learning processes.\n","authors":["Mohammad Amin Ghanizadeh","Mohammad Javad Dousti"],"pdf_url":"https://arxiv.org/pdf/2503.04611v1.pdf","comment":"5 pages"},{"id":"http://arxiv.org/abs/2503.04606v1","updated":"2025-03-06T16:53:14Z","published":"2025-03-06T16:53:14Z","title":"The Best of Both Worlds: Integrating Language Models and Diffusion\n  Models for Video Generation","summary":"  Recent advancements in text-to-video (T2V) generation have been driven by two\ncompeting paradigms: autoregressive language models and diffusion models.\nHowever, each paradigm has intrinsic limitations: language models struggle with\nvisual quality and error accumulation, while diffusion models lack semantic\nunderstanding and causal modeling. In this work, we propose LanDiff, a hybrid\nframework that synergizes the strengths of both paradigms through\ncoarse-to-fine generation. Our architecture introduces three key innovations:\n(1) a semantic tokenizer that compresses 3D visual features into compact 1D\ndiscrete representations through efficient semantic compression, achieving a\n$\\sim$14,000$\\times$ compression ratio; (2) a language model that generates\nsemantic tokens with high-level semantic relationships; (3) a streaming\ndiffusion model that refines coarse semantics into high-fidelity videos.\nExperiments show that LanDiff, a 5B model, achieves a score of 85.43 on the\nVBench T2V benchmark, surpassing the state-of-the-art open-source models\nHunyuan Video (13B) and other commercial models such as Sora, Keling, and\nHailuo. Furthermore, our model also achieves state-of-the-art performance in\nlong video generation, surpassing other open-source models in this field. Our\ndemo can be viewed at https://landiff.github.io/.\n","authors":["Aoxiong Yin","Kai Shen","Yichong Leng","Xu Tan","Xinyu Zhou","Juncheng Li","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2503.04606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04598v1","updated":"2025-03-06T16:40:48Z","published":"2025-03-06T16:40:48Z","title":"HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization","summary":"  Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. %Code\nwill be made publicly available. Code is available at\nhttps://github.com/BryceZhuo/HybridNorm.\n","authors":["Zhijian Zhuo","Yutao Zeng","Ya Wang","Sijun Zhang","Jian Yang","Xiaoqing Li","Xun Zhou","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.00053v3","updated":"2025-03-06T16:28:55Z","published":"2024-10-30T19:09:02Z","title":"ACC-Collab: An Actor-Critic Approach to Multi-Agent LLM Collaboration","summary":"  Large language models (LLMs) have demonstrated a remarkable ability to serve\nas general-purpose tools for various language-based tasks. Recent works have\ndemonstrated that the efficacy of such models can be improved through iterative\ndialog between multiple models. While these paradigms show promise in improving\nmodel efficacy, most works in this area treat collaboration as an emergent\nbehavior, rather than a learned behavior. In doing so, current multi-agent\nframeworks rely on collaborative behaviors to have been sufficiently trained\ninto off-the-shelf models. To address this limitation, we propose ACC-Collab,\nan Actor-Critic based learning framework to produce a two-agent team (an\nactor-agent and a critic-agent) specialized in collaboration. We demonstrate\nthat ACC-Collab outperforms SotA multi-agent techniques on a wide array of\nbenchmarks.\n","authors":["Andrew Estornell","Jean-Francois Ton","Yuanshun Yao","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.00053v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02972v2","updated":"2025-03-06T16:16:07Z","published":"2025-03-04T19:57:47Z","title":"LINGOLY-TOO: Disentangling Memorisation from Reasoning with Linguistic\n  Templatisation and Orthographic Obfuscation","summary":"  Assessing the reasoning capabilities of large language models (LLMs) is\nsusceptible to overestimation due to data exposure of evaluation benchmarks. We\nintroduce a framework for producing linguistic reasoning problems that reduces\nthe effect of memorisation in model performance estimates and apply this\nframework to develop LINGOLY-TOO, a challenging benchmark for linguistic\nreasoning. By developing orthographic templates, we dynamically obfuscate the\nwriting systems of real languages to generate numerousquestion variations.\nThese variations preserve the reasoning steps required for each solution while\nreducing the likelihood of specific problem instances appearing in model\ntraining data. Our experiments demonstrate that frontier models, including\nClaud 3.7 Sonnet, o1-preview and DeepSeek R1, struggle with advanced reasoning.\nOur analysis also shows that LLMs exhibit noticeable variance in accuracy\nacross permutations of the same problem, and on average perform better on\nquestions appearing in their original orthography. Our findings highlight the\nopaque nature of response generation in LLMs and provide evidence that prior\ndata exposure contributes to over estimating the reasoning capabilities of\nfrontier models.\n","authors":["Jude Khouja","Karolina Korgul","Simi Hellsten","Lingyi Yang","Vlad Neacs","Harry Mayne","Ryan Kearns","Andrew Bean","Adam Mahdi"],"pdf_url":"https://arxiv.org/pdf/2503.02972v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17504v2","updated":"2025-03-06T16:14:45Z","published":"2025-02-21T19:22:10Z","title":"Protein Large Language Models: A Comprehensive Survey","summary":"  Protein-specific large language models (Protein LLMs) are revolutionizing\nprotein science by enabling more efficient protein structure prediction,\nfunction annotation, and design. While existing surveys focus on specific\naspects or applications, this work provides the first comprehensive overview of\nProtein LLMs, covering their architectures, training datasets, evaluation\nmetrics, and diverse applications. Through a systematic analysis of over 100\narticles, we propose a structured taxonomy of state-of-the-art Protein LLMs,\nanalyze how they leverage large-scale protein sequence data for improved\naccuracy, and explore their potential in advancing protein engineering and\nbiomedical research. Additionally, we discuss key challenges and future\ndirections, positioning Protein LLMs as essential tools for scientific\ndiscovery in protein science. Resources are maintained at\nhttps://github.com/Yijia-Xiao/Protein-LLM-Survey.\n","authors":["Yijia Xiao","Wanjia Zhao","Junkai Zhang","Yiqiao Jin","Han Zhang","Zhicheng Ren","Renliang Sun","Haixin Wang","Guancheng Wan","Pan Lu","Xiao Luo","Yu Zhang","James Zou","Yizhou Sun","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.17504v2.pdf","comment":"24 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2404.12464v9","updated":"2025-03-06T16:13:04Z","published":"2024-04-18T18:48:50Z","title":"NormAd: A Framework for Measuring the Cultural Adaptability of Large\n  Language Models","summary":"  To be effectively and safely deployed to global user populations, large\nlanguage models (LLMs) may need to adapt outputs to user values and cultures,\nnot just know about them. We introduce NormAd, an evaluation framework to\nassess LLMs' cultural adaptability, specifically measuring their ability to\njudge social acceptability across varying levels of cultural norm specificity,\nfrom abstract values to explicit social norms. As an instantiation of our\nframework, we create NormAd-Eti, a benchmark of 2.6k situational descriptions\nrepresenting social-etiquette related cultural norms from 75 countries. Through\ncomprehensive experiments on NormAd-Eti, we find that LLMs struggle to\naccurately judge social acceptability across these varying degrees of cultural\ncontexts and show stronger adaptability to English-centric cultures over those\nfrom the Global South. Even in the simplest setting where the relevant social\nnorms are provided, the best LLMs' performance (< 82\\%) lags behind humans (>\n95\\%). In settings with abstract values and country information, model\nperformance drops substantially (< 60\\%), while human accuracy remains high (>\n90\\%). Furthermore, we find that models are better at recognizing socially\nacceptable versus unacceptable situations. Our findings showcase the current\npitfalls in socio-cultural reasoning of LLMs which hinder their adaptability\nfor global audiences.\n","authors":["Abhinav Rao","Akhila Yerukola","Vishwa Shah","Katharina Reinecke","Maarten Sap"],"pdf_url":"https://arxiv.org/pdf/2404.12464v9.pdf","comment":"Accepted at NAACL 2025"},{"id":"http://arxiv.org/abs/2503.01804v2","updated":"2025-03-06T16:07:43Z","published":"2025-03-03T18:33:46Z","title":"$\\texttt{SEM-CTRL}$: Semantically Controlled Decoding","summary":"  Ensuring both syntactic and semantic correctness in Large Language Model\n(LLM) outputs remains a significant challenge, despite being critical for\nreal-world deployment. In this paper, we introduce $\\texttt{SEM-CTRL}$, a\nunified approach that enforces rich context-sensitive constraints and task- and\ninstance-specific semantics directly on an LLM decoder. Our approach integrates\ntoken-level MCTS, which is guided by specific syntactic and semantic\nconstraints. The constraints over the desired outputs are expressed using\nAnswer Set Grammars -- a logic-based formalism that generalizes\ncontext-sensitive grammars while incorporating background knowledge to\nrepresent task-specific semantics. We show that our approach guarantees correct\ncompletions for any off-the-shelf LLM without the need for fine-tuning. We\nevaluate $\\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar\nsynthesis, combinatorial reasoning, and planning. Our results demonstrate that\n$\\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform\nlarger variants and state-of-the-art reasoning models (e.g., o1-preview) while\nsimultaneously guaranteeing solution correctness.\n","authors":["Mohammad Albinhassan","Pranava Madhyastha","Alessandra Russo"],"pdf_url":"https://arxiv.org/pdf/2503.01804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00153v2","updated":"2025-03-06T15:50:28Z","published":"2024-09-30T18:52:53Z","title":"Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with\n  Gaussian Distribution","summary":"  Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks.\n","authors":["Haiyan Zhao","Heng Zhao","Bo Shen","Ali Payani","Fan Yang","Mengnan Du"],"pdf_url":"https://arxiv.org/pdf/2410.00153v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04556v1","updated":"2025-03-06T15:47:19Z","published":"2025-03-06T15:47:19Z","title":"Compositional Causal Reasoning Evaluation in Language Models","summary":"  Causal reasoning and compositional reasoning are two core aspirations in\ngenerative AI. Measuring the extent of these behaviors requires principled\nevaluation methods. We explore a unified perspective that considers both\nbehaviors simultaneously, termed compositional causal reasoning (CCR): the\nability to infer how causal measures compose and, equivalently, how causal\nquantities propagate through graphs. We instantiate a framework for the\nsystematic evaluation of CCR for the average treatment effect and the\nprobability of necessity and sufficiency. As proof of concept, we demonstrate\nthe design of CCR tasks for language models in the LLama, Phi, and GPT\nfamilies. On a math word problem, our framework revealed a range of\ntaxonomically distinct error patterns. Additionally, CCR errors increased with\nthe complexity of causal paths for all models except o1.\n","authors":["Jacqueline R. M. A. Maasch","Alihan Hüyük","Xinnuo Xu","Aditya V. Nori","Javier Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2503.04556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09990v2","updated":"2025-03-06T15:38:31Z","published":"2025-02-14T08:22:51Z","title":"X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability","summary":"  Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.\n","authors":["Xiaoya Lu","Dongrui Liu","Yi Yu","Luxin Xu","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2502.09990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04554v1","updated":"2025-03-06T15:37:31Z","published":"2025-03-06T15:37:31Z","title":"Compositional Translation: A Novel LLM-based Approach for Low-resource\n  Machine Translation","summary":"  The ability of generative large language models (LLMs) to perform in-context\nlearning has given rise to a large body of research into how best to prompt\nmodels for various natural language processing tasks. Machine Translation (MT)\nhas been shown to benefit from in-context examples, in particular when they are\nsemantically similar to the sentence to translate. In this paper, we propose a\nnew LLM-based translation paradigm, compositional translation, to replace naive\nfew-shot MT with similarity-based demonstrations. An LLM is used to decompose a\nsentence into simpler phrases, and then to translate each phrase with the help\nof retrieved demonstrations. Finally, the LLM is prompted to translate the\ninitial sentence with the help of the self-generated phrase-translation pairs.\nOur intuition is that this approach should improve translation because these\nshorter phrases should be intrinsically easier to translate and easier to match\nwith relevant examples. This is especially beneficial in low-resource\nscenarios, and more generally whenever the selection pool is small or out of\ndomain. We show that compositional translation boosts LLM translation\nperformance on a wide range of popular MT benchmarks, including FLORES 200,\nNTREX 128 and TICO-19. Code and outputs are available at\nhttps://github.com/ArmelRandy/compositional-translation\n","authors":["Armel Zebaze","Benoît Sagot","Rachel Bawden"],"pdf_url":"https://arxiv.org/pdf/2503.04554v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20984v2","updated":"2025-03-06T15:36:48Z","published":"2025-02-28T11:52:02Z","title":"UoR-NCL at SemEval-2025 Task 1: Using Generative LLMs and CLIP Models\n  for Multilingual Multimodal Idiomaticity Representation","summary":"  SemEval-2025 Task 1 focuses on ranking images based on their alignment with a\ngiven nominal compound that may carry idiomatic meaning in both English and\nBrazilian Portuguese. To address this challenge, this work uses generative\nlarge language models (LLMs) and multilingual CLIP models to enhance idiomatic\ncompound representations. LLMs generate idiomatic meanings for potentially\nidiomatic compounds, enriching their semantic interpretation. These meanings\nare then encoded using multilingual CLIP models, serving as representations for\nimage ranking. Contrastive learning and data augmentation techniques are\napplied to fine-tune these embeddings for improved performance. Experimental\nresults show that multimodal representations extracted through this method\noutperformed those based solely on the original nominal compounds. The\nfine-tuning approach shows promising outcomes but is less effective than using\nembeddings without fine-tuning. The source code used in this paper is available\nat https://github.com/tongwu17/SemEval-2025-Task1-UoR-NCL.\n","authors":["Thanet Markchom","Tong Wu","Liting Huang","Huizhi Liang"],"pdf_url":"https://arxiv.org/pdf/2502.20984v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04548v1","updated":"2025-03-06T15:34:27Z","published":"2025-03-06T15:34:27Z","title":"An Empirical Study on Eliciting and Improving R1-like Reasoning Models","summary":"  In this report, we present the third technical report on the development of\nslow-thinking models as part of the STILL project. As the technical pathway\nbecomes clearer, scaling RL training has become a central technique for\nimplementing such reasoning models. We systematically experiment with and\ndocument the effects of various factors influencing RL training, conducting\nexperiments on both base models and fine-tuned models. Specifically, we\ndemonstrate that our RL training approach consistently improves the Qwen2.5-32B\nbase models, enhancing both response length and test accuracy. Furthermore, we\nshow that even when a model like DeepSeek-R1-Distill-Qwen-1.5B has already\nachieved a high performance level, it can be further refined through RL\ntraining, reaching an accuracy of 39.33% on AIME 2024. Beyond RL training, we\nalso explore the use of tool manipulation, finding that it significantly boosts\nthe reasoning performance of large reasoning models. This approach achieves a\nremarkable accuracy of 86.67% with greedy search on AIME 2024, underscoring its\neffectiveness in enhancing model capabilities. We release our resources at the\nSTILL project website: https://github.com/RUCAIBox/Slow_Thinking_with_LLMs.\n","authors":["Zhipeng Chen","Yingqian Min","Beichen Zhang","Jie Chen","Jinhao Jiang","Daixuan Cheng","Wayne Xin Zhao","Zheng Liu","Xu Miao","Yang Lu","Lei Fang","Zhongyuan Wang","Ji-Rong Wen"],"pdf_url":"https://arxiv.org/pdf/2503.04548v1.pdf","comment":"Technical Report on Slow Thinking with LLMs: Part III"},{"id":"http://arxiv.org/abs/2503.04543v1","updated":"2025-03-06T15:29:13Z","published":"2025-03-06T15:29:13Z","title":"Keeping Yourself is Important in Downstream Tuning Multimodal Large\n  Language Model","summary":"  Multi-modal Large Language Models (MLLMs) integrate visual and linguistic\nreasoning to address complex tasks such as image captioning and visual question\nanswering. While MLLMs demonstrate remarkable versatility, MLLMs appears\nlimited performance on special applications. But tuning MLLMs for downstream\ntasks encounters two key challenges: Task-Expert Specialization, where\ndistribution shifts between pre-training and target datasets constrain target\nperformance, and Open-World Stabilization, where catastrophic forgetting erases\nthe model general knowledge. In this work, we systematically review recent\nadvancements in MLLM tuning methodologies, classifying them into three\nparadigms: (I) Selective Tuning, (II) Additive Tuning, and (III)\nReparameterization Tuning. Furthermore, we benchmark these tuning strategies\nacross popular MLLM architectures and diverse downstream tasks to establish\nstandardized evaluation analysis and systematic tuning principles. Finally, we\nhighlight several open challenges in this domain and propose future research\ndirections. To facilitate ongoing progress in this rapidly evolving field, we\nprovide a public repository that continuously tracks developments:\nhttps://github.com/WenkeHuang/Awesome-MLLM-Tuning.\n","authors":["Wenke Huang","Jian Liang","Xianda Guo","Yiyang Fang","Guancheng Wan","Xuankun Rong","Chi Wen","Zekun Shi","Qingyun Li","Didi Zhu","Yanbiao Ma","Ke Liang","Bin Yang","He Li","Jiawei Shao","Mang Ye","Bo Du"],"pdf_url":"https://arxiv.org/pdf/2503.04543v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07180v5","updated":"2025-03-06T15:26:56Z","published":"2024-11-11T17:57:30Z","title":"Gumbel Counterfactual Generation From Language Models","summary":"  Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to\n\\emph{intervene} on these models. To understand the impact of interventions\nprecisely, it is useful to examine \\emph{counterfactuals} -- e.g., how a given\nsentence would have appeared had it been generated by the model following a\nspecific intervention. We highlight that counterfactual reasoning is\nconceptually distinct from interventions, as articulated in Pearl's causal\nhierarchy. Based on this observation, we propose a framework for generating\ntrue string counterfactuals by reformulating language models as a structural\nequation model using the Gumbel-max trick, which we called Gumbel\ncounterfactual generation. This reformulation allows us to model the joint\ndistribution over original strings and their counterfactuals resulting from the\nsame instantiation of the sampling noise. We develop an algorithm based on\nhindsight Gumbel sampling that allows us to infer the latent noise variables\nand generate counterfactuals of observed strings. Our experiments demonstrate\nthat the approach produces meaningful counterfactuals while at the same time\nshowing that commonly used intervention techniques have considerable undesired\nside effects.\n","authors":["Shauli Ravfogel","Anej Svete","Vésteinn Snæbjarnarson","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2411.07180v5.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2411.12580v2","updated":"2025-03-06T15:14:17Z","published":"2024-11-19T15:47:12Z","title":"Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models","summary":"  The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning.\n","authors":["Laura Ruis","Maximilian Mozes","Juhan Bae","Siddhartha Rao Kamalakara","Dwarak Talupuru","Acyr Locatelli","Robert Kirk","Tim Rocktäschel","Edward Grefenstette","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2411.12580v2.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.00367v2","updated":"2025-03-06T15:08:32Z","published":"2025-03-01T06:29:00Z","title":"Approaching the Limits to EFL Writing Enhancement with AI-generated Text\n  and Diverse Learners","summary":"  Generative artificial intelligence (AI) chatbots, such as ChatGPT, are\nreshaping how English as a foreign language (EFL) students write since students\ncan compose texts by integrating their own words with AI-generated text. This\nstudy investigated how 59 Hong Kong secondary school students with varying\nlevels of academic achievement interacted with AI-generated text to compose a\nfeature article, exploring whether any interaction patterns benefited the\noverall quality of the article. Through content analysis, multiple linear\nregression and cluster analysis, we found the overall number of words --\nwhether AI- or human-generated -- is the main predictor of writing quality.\nHowever, the impact varies by students' competence to write independently, for\ninstance, by using their own words accurately and coherently to compose a text,\nand to follow specific interaction patterns with AI-generated text. Therefore,\nalthough composing texts with human words and AI-generated text may become\nprevalent in EFL writing classrooms, without educators' careful attention to\nEFL writing pedagogy and AI literacy, high-achieving students stand to benefit\nmore from using AI-generated text than low-achieving students.\n","authors":["David James Woo","Hengky Susanto","Chi Ho Yeung","Kai Guo"],"pdf_url":"https://arxiv.org/pdf/2503.00367v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04490v1","updated":"2025-03-06T14:38:20Z","published":"2025-03-06T14:38:20Z","title":"Large Language Models in Bioinformatics: A Survey","summary":"  Large Language Models (LLMs) are revolutionizing bioinformatics, enabling\nadvanced analysis of DNA, RNA, proteins, and single-cell data. This survey\nprovides a systematic review of recent advancements, focusing on genomic\nsequence modeling, RNA structure prediction, protein function inference, and\nsingle-cell transcriptomics. Meanwhile, we also discuss several key challenges,\nincluding data scarcity, computational complexity, and cross-omics integration,\nand explore future directions such as multimodal learning, hybrid AI models,\nand clinical applications. By offering a comprehensive perspective, this paper\nunderscores the transformative potential of LLMs in driving innovations in\nbioinformatics and precision medicine.\n","authors":["Zhenyu Wang","Zikang Wang","Jiyue Jiang","Pengan Chen","Xiangyu Shi","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2503.04490v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04482v1","updated":"2025-03-06T14:30:55Z","published":"2025-03-06T14:30:55Z","title":"Generalized Interpolating Discrete Diffusion","summary":"  While state-of-the-art language models achieve impressive results through\nnext-token prediction, they have inherent limitations such as the inability to\nrevise already generated tokens. This has prompted exploration of alternative\napproaches such as discrete diffusion. However, masked diffusion, which has\nemerged as a popular choice due to its simplicity and effectiveness,\nreintroduces this inability to revise words. To overcome this, we generalize\nmasked diffusion and derive the theoretical backbone of a family of general\ninterpolating discrete diffusion (GIDD) processes offering greater flexibility\nin the design of the noising processes. Leveraging a novel diffusion ELBO, we\nachieve compute-matched state-of-the-art performance in diffusion language\nmodeling. Exploiting GIDD's flexibility, we explore a hybrid approach combining\nmasking and uniform noise, leading to improved sample quality and unlocking the\nability for the model to correct its own mistakes, an area where autoregressive\nmodels notoriously have struggled. Our code and models are open-source:\nhttps://github.com/dvruette/gidd/\n","authors":["Dimitri von Rütte","Janis Fluri","Yuhui Ding","Antonio Orvieto","Bernhard Schölkopf","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2503.04482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04463v1","updated":"2025-03-06T14:15:07Z","published":"2025-03-06T14:15:07Z","title":"Guiding LLMs to Generate High-Fidelity and High-Quality Counterfactual\n  Explanations for Text Classification","summary":"  The need for interpretability in deep learning has driven interest in\ncounterfactual explanations, which identify minimal changes to an instance that\nchange a model's prediction. Current counterfactual (CF) generation methods\nrequire task-specific fine-tuning and produce low-quality text. Large Language\nModels (LLMs), though effective for high-quality text generation, struggle with\nlabel-flipping counterfactuals (i.e., counterfactuals that change the\nprediction) without fine-tuning. We introduce two simple classifier-guided\napproaches to support counterfactual generation by LLMs, eliminating the need\nfor fine-tuning while preserving the strengths of LLMs. Despite their\nsimplicity, our methods outperform state-of-the-art counterfactual generation\nmethods and are effective across different LLMs, highlighting the benefits of\nguiding counterfactual generation by LLMs with classifier information. We\nfurther show that data augmentation by our generated CFs can improve a\nclassifier's robustness. Our analysis reveals a critical issue in\ncounterfactual generation by LLMs: LLMs rely on parametric knowledge rather\nthan faithfully following the classifier.\n","authors":["Van Bach Nguyen","Christin Seifert","Jörg Schlötterer"],"pdf_url":"https://arxiv.org/pdf/2503.04463v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04449v1","updated":"2025-03-06T14:04:30Z","published":"2025-03-06T14:04:30Z","title":"Quantifying patterns of punctuation in modern Chinese prose","summary":"  Recent research shows that punctuation patterns in texts exhibit universal\nfeatures across languages. Analysis of Western classical literature reveals\nthat the distribution of spaces between punctuation marks aligns with a\ndiscrete Weibull distribution, typically used in survival analysis. By\nextending this analysis to Chinese literature represented here by three notable\ncontemporary works, it is shown that Zipf's law applies to Chinese texts\nsimilarly to Western texts, where punctuation patterns also improve adherence\nto the law. Additionally, the distance distribution between punctuation marks\nin Chinese texts follows the Weibull model, though larger spacing is less\nfrequent than in English translations. Sentence-ending punctuation,\nrepresenting sentence length, diverges more from this pattern, reflecting\ngreater flexibility in sentence length. This variability supports the formation\nof complex, multifractal sentence structures, particularly evident in Gao\nXingjian's \"Soul Mountain\". These findings demonstrate that both Chinese and\nWestern texts share universal punctuation and word distribution patterns,\nunderscoring their broad applicability across languages.\n","authors":["Michał Dolina","Jakub Dec","Stanisław Drożdż","Jarosław Kwapień","Jin Liu","Tomasz Stanisz"],"pdf_url":"https://arxiv.org/pdf/2503.04449v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04439v1","updated":"2025-03-06T13:55:33Z","published":"2025-03-06T13:55:33Z","title":"A Dataset for Analysing News Framing in Chinese Media","summary":"  Framing is an essential device in news reporting, allowing the writer to\ninfluence public perceptions of current affairs. While there are existing\nautomatic news framing detection datasets in various languages, none of them\nfocus on news framing in the Chinese language which has complex character\nmeanings and unique linguistic features. This study introduces the first\nChinese News Framing dataset, to be used as either a stand-alone dataset or a\nsupplementary resource to the SemEval-2023 task 3 dataset. We detail its\ncreation and we run baseline experiments to highlight the need for such a\ndataset and create benchmarks for future research, providing results obtained\nthrough fine-tuning XLM-RoBERTa-Base and using GPT-4o in the zero-shot setting.\nWe find that GPT-4o performs significantly worse than fine-tuned XLM-RoBERTa\nacross all languages. For the Chinese language, we obtain an F1-micro (the\nperformance metric for SemEval task 3, subtask 2) score of 0.719 using only\nsamples from our Chinese News Framing dataset and a score of 0.753 when we\naugment the SemEval dataset with Chinese news framing samples. With positive\nnews frame detection results, this dataset is a valuable resource for detecting\nnews frames in the Chinese language and is a valuable supplement to the\nSemEval-2023 task 3 dataset.\n","authors":["Owen Cook","Yida Mu","Xinye Yang","Xingyi Song","Kalina Bontcheva"],"pdf_url":"https://arxiv.org/pdf/2503.04439v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13959v2","updated":"2025-03-06T13:51:24Z","published":"2025-01-21T06:32:25Z","title":"Assisting Mathematical Formalization with A Learning-based Premise\n  Retriever","summary":"  Premise selection is a crucial yet challenging step in mathematical\nformalization, especially for users with limited experience. Due to the lack of\navailable formalization projects, existing approaches that leverage language\nmodels often suffer from data scarcity. In this work, we introduce an\ninnovative method for training a premise retriever to support the formalization\nof mathematics. Our approach employs a BERT model to embed proof states and\npremises into a shared latent space. The retrieval model is trained within a\ncontrastive learning framework and incorporates a domain-specific tokenizer\nalong with a fine-grained similarity computation method. Experimental results\nshow that our model is highly competitive compared to existing baselines,\nachieving strong performance while requiring fewer computational resources.\nPerformance is further enhanced through the integration of a re-ranking module.\nTo streamline the formalization process, we will release a search engine that\nenables users to query Mathlib theorems directly using proof states,\nsignificantly improving accessibility and efficiency. Codes are available at\nhttps://github.com/ruc-ai4math/Premise-Retrieval.\n","authors":["Yicheng Tao","Haotian Liu","Shanwen Wang","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2501.13959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07978v4","updated":"2025-03-06T13:29:24Z","published":"2023-11-14T08:10:14Z","title":"AfroBench: How Good are Large Language Models on African Languages?","summary":"  Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/\n","authors":["Jessica Ojo","Odunayo Ogundepo","Akintunde Oladipo","Kelechi Ogueji","Jimmy Lin","Pontus Stenetorp","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2311.07978v4.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2503.04421v1","updated":"2025-03-06T13:26:58Z","published":"2025-03-06T13:26:58Z","title":"Revisiting the Othello World Model Hypothesis","summary":"  Li et al. (2023) used the Othello board game as a test case for the ability\nof GPT-2 to induce world models, and were followed up by Nanda et al. (2023b).\nWe briefly discuss the original experiments, expanding them to include more\nlanguage models with more comprehensive probing. Specifically, we analyze\nsequences of Othello board states and train the model to predict the next move\nbased on previous moves. We evaluate seven language models (GPT-2, T5, Bart,\nFlan-T5, Mistral, LLaMA-2, and Qwen2.5) on the Othello task and conclude that\nthese models not only learn to play Othello, but also induce the Othello board\nlayout. We find that all models achieve up to 99% accuracy in unsupervised\ngrounding and exhibit high similarity in the board features they learned. This\nprovides considerably stronger evidence for the Othello World Model Hypothesis\nthan previous works.\n","authors":["Yifei Yuan","Anders Søgaard"],"pdf_url":"https://arxiv.org/pdf/2503.04421v1.pdf","comment":"ICLR World Models Workshop"},{"id":"http://arxiv.org/abs/2503.04413v1","updated":"2025-03-06T13:10:57Z","published":"2025-03-06T13:10:57Z","title":"Can Large Language Models Predict Antimicrobial Resistance Gene?","summary":"  This study demonstrates that generative large language models can be utilized\nin a more flexible manner for DNA sequence analysis and classification tasks\ncompared to traditional transformer encoder-based models. While recent\nencoder-based models such as DNABERT and Nucleotide Transformer have shown\nsignificant performance in DNA sequence classification, transformer\ndecoder-based generative models have not yet been extensively explored in this\nfield. This study evaluates how effectively generative Large Language Models\nhandle DNA sequences with various labels and analyzes performance changes when\nadditional textual information is provided. Experiments were conducted on\nantimicrobial resistance genes, and the results show that generative Large\nLanguage Models can offer comparable or potentially better predictions,\ndemonstrating flexibility and accuracy when incorporating both sequence and\ntextual information. The code and data used in this work are available at the\nfollowing GitHub repository: https://github.com/biocomgit/llm4dna.\n","authors":["Hyunwoo Yoo"],"pdf_url":"https://arxiv.org/pdf/2503.04413v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04405v1","updated":"2025-03-06T12:59:11Z","published":"2025-03-06T12:59:11Z","title":"Comparative Study of Zero-Shot Cross-Lingual Transfer for Bodo POS and\n  NER Tagging Using Gemini 2.0 Flash Thinking Experimental Model","summary":"  Named Entity Recognition (NER) and Part-of-Speech (POS) tagging are critical\ntasks for Natural Language Processing (NLP), yet their availability for\nlow-resource languages (LRLs) like Bodo remains limited. This article presents\na comparative empirical study investigating the effectiveness of Google's\nGemini 2.0 Flash Thinking Experiment model for zero-shot cross-lingual transfer\nof POS and NER tagging to Bodo. We explore two distinct methodologies: (1)\ndirect translation of English sentences to Bodo followed by tag transfer, and\n(2) prompt-based tag transfer on parallel English-Bodo sentence pairs. Both\nmethods leverage the machine translation and cross-lingual understanding\ncapabilities of Gemini 2.0 Flash Thinking Experiment to project English POS and\nNER annotations onto Bodo text in CONLL-2003 format. Our findings reveal the\ncapabilities and limitations of each approach, demonstrating that while both\nmethods show promise for bootstrapping Bodo NLP, prompt-based transfer exhibits\nsuperior performance, particularly for NER. We provide a detailed analysis of\nthe results, highlighting the impact of translation quality, grammatical\ndivergences, and the inherent challenges of zero-shot cross-lingual transfer.\nThe article concludes by discussing future research directions, emphasizing the\nneed for hybrid approaches, few-shot fine-tuning, and the development of\ndedicated Bodo NLP resources to achieve high-accuracy POS and NER tagging for\nthis low-resource language.\n","authors":["Sanjib Narzary","Bihung Brahma","Haradip Mahilary","Mahananda Brahma","Bidisha Som","Sukumar Nandi"],"pdf_url":"https://arxiv.org/pdf/2503.04405v1.pdf","comment":"Submitted to SpringerNature MTAP journal. This article has not been\n  reviewed yet. Submitting for public review!"},{"id":"http://arxiv.org/abs/2406.12753v2","updated":"2025-03-06T12:55:25Z","published":"2024-06-18T16:20:53Z","title":"OlympicArena: Benchmarking Multi-discipline Cognitive Reasoning for\n  Superintelligent AI","summary":"  The evolution of Artificial Intelligence (AI) has been significantly\naccelerated by advancements in Large Language Models (LLMs) and Large\nMultimodal Models (LMMs), gradually showcasing potential cognitive reasoning\nabilities in problem-solving and scientific discovery (i.e., AI4Science) once\nexclusive to human intellect. To comprehensively evaluate current models'\nperformance in cognitive reasoning abilities, we introduce OlympicArena, which\nincludes 11,163 bilingual problems across both text-only and interleaved\ntext-image modalities. These challenges encompass a wide range of disciplines\nspanning seven fields and 62 international Olympic competitions, rigorously\nexamined for data leakage. We argue that the challenges in Olympic competition\nproblems are ideal for evaluating AI's cognitive reasoning due to their\ncomplexity and interdisciplinary nature, which are essential for tackling\ncomplex scientific challenges and facilitating discoveries. Beyond evaluating\nperformance across various disciplines using answer-only criteria, we conduct\ndetailed experiments and analyses from multiple perspectives. We delve into the\nmodels' cognitive reasoning abilities, their performance across different\nmodalities, and their outcomes in process-level evaluations, which are vital\nfor tasks requiring complex reasoning with lengthy solutions. Our extensive\nevaluations reveal that even advanced models like GPT-4o only achieve a 39.97%\noverall accuracy, illustrating current AI limitations in complex reasoning and\nmultimodal integration. Through the OlympicArena, we aim to advance AI towards\nsuperintelligence, equipping it to address more complex challenges in science\nand beyond. We also provide a comprehensive set of resources to support AI\nresearch, including a benchmark dataset, an open-source annotation platform, a\ndetailed evaluation tool, and a leaderboard with automatic submission features.\n","authors":["Zhen Huang","Zengzhi Wang","Shijie Xia","Xuefeng Li","Haoyang Zou","Ruijie Xu","Run-Ze Fan","Lyumanshan Ye","Ethan Chern","Yixin Ye","Yikai Zhang","Yuqing Yang","Ting Wu","Binjie Wang","Shichao Sun","Yang Xiao","Yiyuan Li","Fan Zhou","Steffi Chern","Yiwei Qin","Yan Ma","Jiadi Su","Yixiu Liu","Yuxiang Zheng","Shaoting Zhang","Dahua Lin","Yu Qiao","Pengfei Liu"],"pdf_url":"https://arxiv.org/pdf/2406.12753v2.pdf","comment":"Accepted by NeurIPS 2024"},{"id":"http://arxiv.org/abs/2404.05569v3","updated":"2025-03-06T12:54:37Z","published":"2024-04-08T14:43:13Z","title":"360$^\\circ$REA: Towards A Reusable Experience Accumulation with\n  360° Assessment for Multi-Agent System","summary":"  Large language model agents have demonstrated remarkable advancements across\nvarious complex tasks. Recent works focus on optimizing the agent team or\nemploying self-reflection to iteratively solve complex tasks. Since these\nagents are all based on the same LLM, only conducting self-evaluation or\nremoving underperforming agents does not substantively enhance the capability\nof the agents. We argue that a comprehensive evaluation and accumulating\nexperience from evaluation feedback is an effective approach to improving\nsystem performance. In this paper, we propose Reusable Experience Accumulation\nwith 360$^\\circ$ Assessment (360$^\\circ$REA), a hierarchical multi-agent\nframework inspired by corporate organizational practices. The framework employs\na novel 360$^\\circ$ performance assessment method for multi-perspective\nperformance evaluation with fine-grained assessment. To enhance the capability\nof agents in addressing complex tasks, we introduce dual-level experience pool\nfor agents to accumulate experience through fine-grained assessment. Extensive\nexperiments on complex task datasets demonstrate the effectiveness of\n360$^\\circ$REA.\n","authors":["Shen Gao","Hao Li","Chengrui Huang","Quan Tu","Zhiliang Tian","Minlie Huang","Shuo Shang"],"pdf_url":"https://arxiv.org/pdf/2404.05569v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20742v2","updated":"2025-03-06T12:50:44Z","published":"2025-02-28T05:47:34Z","title":"Structured Preference Optimization for Vision-Language Long-Horizon Task\n  Planning","summary":"  Existing methods for vision-language task planning excel in short-horizon\ntasks but often fall short in complex, long-horizon planning within dynamic\nenvironments. These challenges primarily arise from the difficulty of\neffectively training models to produce high-quality reasoning processes for\nlong-horizon tasks. To address this, we propose Structured Preference\nOptimization (SPO), which aims to enhance reasoning and action selection in\nlong-horizon task planning through structured preference evaluation and\noptimized training strategies. Specifically, SPO introduces: 1)\nPreference-Based Scoring and Optimization, which systematically evaluates\nreasoning chains based on task relevance, visual grounding, and historical\nconsistency; and 2) Curriculum-Guided Training, where the model progressively\nadapts from simple to complex tasks, improving its generalization ability in\nlong-horizon scenarios and enhancing reasoning robustness. To advance research\nin vision-language long-horizon task planning, we introduce ExtendaBench, a\ncomprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat\n2.0, categorized into ultra-short, short, medium, and long tasks. Experimental\nresults demonstrate that SPO significantly improves reasoning quality and final\ndecision accuracy, outperforming prior methods on long-horizon tasks and\nunderscoring the effectiveness of preference-driven optimization in\nvision-language task planning. Specifically, SPO achieves a +5.98% GCR and\n+4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement\nin Habitat over the best-performing baselines.\n","authors":["Xiwen Liang","Min Lin","Weiqi Ruan","Rongtao Xu","Yuecheng Liu","Jiaqi Chen","Bingqian Lin","Yuzheng Zhuang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2502.20742v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2503.04396v1","updated":"2025-03-06T12:50:14Z","published":"2025-03-06T12:50:14Z","title":"TableLoRA: Low-rank Adaptation on Table Structure Understanding for\n  Large Language Models","summary":"  Tabular data are crucial in many fields and their understanding by large\nlanguage models (LLMs) under high parameter efficiency paradigm is important.\nHowever, directly applying parameter-efficient fine-tuning (PEFT) techniques to\ntabular tasks presents significant challenges, particularly in terms of better\ntable serialization and the representation of two-dimensional structured\ninformation within a one-dimensional sequence. To address this, we propose\nTableLoRA, a module designed to improve LLMs' understanding of table structure\nduring PEFT. It incorporates special tokens for serializing tables with special\ntoken encoder and uses 2D LoRA to encode low-rank information on cell\npositions. Experiments on four tabular-related datasets demonstrate that\nTableLoRA consistently outperforms vanilla LoRA and surpasses various table\nencoding methods tested in control experiments. These findings reveal that\nTableLoRA, as a table-specific LoRA, enhances the ability of LLMs to process\ntabular data effectively, especially in low-parameter settings, demonstrating\nits potential as a robust solution for handling table-related tasks.\n","authors":["Xinyi He","Yihao Liu","Mengyu Zhou","Yeye He","Haoyu Dong","Shi Han","Zejian Yuan","Dongmei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04395v1","updated":"2025-03-06T12:47:54Z","published":"2025-03-06T12:47:54Z","title":"Shaping Shared Languages: Human and Large Language Models' Inductive\n  Biases in Emergent Communication","summary":"  Languages are shaped by the inductive biases of their users. Using a\nclassical referential game, we investigate how artificial languages evolve when\noptimised for inductive biases in humans and large language models (LLMs) via\nHuman-Human, LLM-LLM and Human-LLM experiments. We show that referentially\ngrounded vocabularies emerge that enable reliable communication in all\nconditions, even when humans and LLMs collaborate. Comparisons between\nconditions reveal that languages optimised for LLMs subtly differ from those\noptimised for humans. Interestingly, interactions between humans and LLMs\nalleviate these differences and result in vocabularies which are more\nhuman-like than LLM-like. These findings advance our understanding of how\ninductive biases in LLMs play a role in the dynamic nature of human language\nand contribute to maintaining alignment in human and machine communication. In\nparticular, our work underscores the need to think of new methods that include\nhuman interaction in the training processes of LLMs, and shows that using\ncommunicative success as a reward signal can be a fruitful, novel direction.\n","authors":["Tom Kouwenhoven","Max Peeperkorn","Roy de Kleijn","Tessa Verhoef"],"pdf_url":"https://arxiv.org/pdf/2503.04395v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.21083v2","updated":"2025-03-06T12:38:42Z","published":"2024-10-28T14:48:05Z","title":"Stealthy Jailbreak Attacks on Large Language Models via Benign Data\n  Mirroring","summary":"  Large language model (LLM) safety is a critical issue, with numerous studies\nemploying red team testing to enhance model security. Among these, jailbreak\nmethods explore potential vulnerabilities by crafting malicious prompts that\ninduce model outputs contrary to safety alignments. Existing black-box\njailbreak methods often rely on model feedback, repeatedly submitting queries\nwith detectable malicious instructions during the attack search process.\nAlthough these approaches are effective, the attacks may be intercepted by\ncontent moderators during the search process. We propose an improved transfer\nattack method that guides malicious prompt construction by locally training a\nmirror model of the target black-box model through benign data distillation.\nThis method offers enhanced stealth, as it does not involve submitting\nidentifiable malicious instructions to the target model during the search\nphase. Our approach achieved a maximum attack success rate of 92%, or a\nbalanced value of 80% with an average of 1.5 detectable jailbreak queries per\nsample against GPT-3.5 Turbo on a subset of AdvBench. These results underscore\nthe need for more robust defense mechanisms.\n","authors":["Honglin Mu","Han He","Yuxin Zhou","Yunlong Feng","Yang Xu","Libo Qin","Xiaoming Shi","Zeming Liu","Xudong Han","Qi Shi","Qingfu Zhu","Wanxiang Che"],"pdf_url":"https://arxiv.org/pdf/2410.21083v2.pdf","comment":"Accepted by NAACL 2025"},{"id":"http://arxiv.org/abs/2503.04388v1","updated":"2025-03-06T12:38:17Z","published":"2025-03-06T12:38:17Z","title":"More Documents, Same Length: Isolating the Challenge of Multiple\n  Documents in RAG","summary":"  Retrieval-augmented generation (RAG) provides LLMs with relevant documents.\nAlthough previous studies noted that retrieving many documents can degrade\nperformance, they did not isolate how the quantity of documents affects\nperformance while controlling for context length. We evaluate various language\nmodels on custom datasets derived from a multi-hop QA task. We keep the context\nlength and position of relevant information constant while varying the number\nof documents, and find that increasing the document count in RAG settings poses\nsignificant challenges for LLMs. Additionally, our results indicate that\nprocessing multiple documents is a separate challenge from handling long\ncontexts. We also make the datasets and code available:\nhttps://github.com/shaharl6000/MoreDocsSameLen .\n","authors":["Shahar Levy","Nir Mazor","Lihi Shalmon","Michael Hassid","Gabriel Stanovsky"],"pdf_url":"https://arxiv.org/pdf/2503.04388v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2503.04381v1","updated":"2025-03-06T12:33:20Z","published":"2025-03-06T12:33:20Z","title":"TRACT: Regression-Aware Fine-tuning Meets Chain-of-Thought Reasoning for\n  LLM-as-a-Judge","summary":"  The LLM-as-a-judge paradigm uses large language models (LLMs) for automated\ntext evaluation, where a numerical assessment is assigned by an LLM to the\ninput text following scoring rubrics. Existing methods for LLM-as-a-judge use\ncross-entropy (CE) loss for fine-tuning, which neglects the numeric nature of\nscore prediction. Recent work addresses numerical prediction limitations of LLM\nfine-tuning through regression-aware fine-tuning, which, however, does not\nconsider chain-of-thought (CoT) reasoning for score prediction. In this paper,\nwe introduce TRACT (Two-stage Regression-Aware fine-tuning with CoT), a method\ncombining CoT reasoning with regression-aware training. TRACT consists of two\nstages: first, seed LLM is fine-tuned to generate CoTs, which serve as\nsupervision for the second stage fine-tuning. The training objective of TRACT\ncombines the CE loss for learning the CoT reasoning capabilities, and the\nregression-aware loss for the score prediction. Experiments across four\nLLM-as-a-judge datasets and two LLMs show that TRACT significantly outperforms\nexisting methods. Extensive ablation studies validate the importance of each\ncomponent in TRACT.\n","authors":["Cheng-Han Chiang","Hung-yi Lee","Michal Lukasik"],"pdf_url":"https://arxiv.org/pdf/2503.04381v1.pdf","comment":"Codes and models are available at https://github.com/d223302/TRACT"},{"id":"http://arxiv.org/abs/2503.04378v1","updated":"2025-03-06T12:30:24Z","published":"2025-03-06T12:30:24Z","title":"Dedicated Feedback and Edit Models Empower Inference-Time Scaling for\n  Open-Ended General-Domain Tasks","summary":"  Inference-Time Scaling has been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models for\ninference-time scaling require tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect data\nfor and train dedicated Feedback and Edit Models that are capable of performing\ninference-time scaling for open-ended general-domain tasks. In our setup, one\nmodel generates an initial response, which are given feedback by a second\nmodel, that are then used by a third model to edit the response. We show that\nperformance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo\ncan be boosted by scaling the number of initial response drafts, effective\nfeedback and edited responses. When scaled optimally, our setup based on 70B\nmodels from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7\nas of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3.\n","authors":["Zhilin Wang","Jiaqi Zeng","Olivier Delalleau","Daniel Egert","Ellie Evans","Hoo-Chang Shin","Felipe Soares","Yi Dong","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2503.04378v1.pdf","comment":"22 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.01346v2","updated":"2025-03-06T12:27:24Z","published":"2025-03-03T09:37:33Z","title":"SRAG: Structured Retrieval-Augmented Generation for Multi-Entity\n  Question Answering over Wikipedia Graph","summary":"  Multi-entity question answering (MEQA) poses significant challenges for large\nlanguage models (LLMs), which often struggle to consolidate scattered\ninformation across multiple documents. An example question might be \"What is\nthe distribution of IEEE Fellows among various fields of study?\", which\nrequires retrieving information from diverse sources e.g., Wikipedia pages. The\neffectiveness of current retrieval-augmented generation (RAG) methods is\nlimited by the LLMs' capacity to aggregate insights from numerous pages. To\naddress this gap, this paper introduces a structured RAG (SRAG) framework that\nsystematically organizes extracted entities into relational tables (e.g.,\ntabulating entities with schema columns like \"name\" and \"field of study\") and\nthen apply table-based reasoning techniques. Our approach decouples retrieval\nand reasoning, enabling LLMs to focus on structured data analysis rather than\nraw text aggregation. Extensive experiments on Wikipedia-based multi-entity QA\ntasks demonstrate that SRAG significantly outperforms state-of-the-art\nlong-context LLMs and RAG solutions, achieving a 29.6% improvement in accuracy.\nThe results underscore the efficacy of structuring unstructured data to enhance\nLLMs' reasoning capabilities.\n","authors":["Teng Lin","Yizhang Zhu","Yuyu Luo","Nan Tang"],"pdf_url":"https://arxiv.org/pdf/2503.01346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04372v1","updated":"2025-03-06T12:16:14Z","published":"2025-03-06T12:16:14Z","title":"Assumed Identities: Quantifying Gender Bias in Machine Translation of\n  Ambiguous Occupational Terms","summary":"  Machine Translation (MT) systems frequently encounter ambiguous scenarios\nwhere they must assign gender to certain occupations when translating without\nexplicit guidance or contextual cues. While individual translations in such\ncases may not be inherently biased, systematic patterns-such as the repeated\nassociation of certain professions with specific genders-can emerge, reflecting\nand perpetuating societal stereotypes. This ambiguity challenges traditional\ninstance-level single-answer evaluation approaches, as no single gold standard\ntranslation exists. To address this, we propose an approach that evaluates\ngender bias through aggregated model responses. Specifically, we introduce a\nmethodology to detect gender imbalances between source texts and translations,\na benchmarking dataset with ambiguous English inputs, and probability-based\nmetrics to quantify a model's divergence from normative standards or reference\ndistributions.\n","authors":["Orfeas Menis Mastromichalakis","Giorgos Filandrianos","Maria Symeonaki","Giorgos Stamou"],"pdf_url":"https://arxiv.org/pdf/2503.04372v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04369v1","updated":"2025-03-06T12:14:45Z","published":"2025-03-06T12:14:45Z","title":"Lost in Literalism: How Supervised Training Shapes Translationese in\n  LLMs","summary":"  Large language models (LLMs) have achieved remarkable success in machine\ntranslation, demonstrating impressive performance across diverse languages.\nHowever, translationese, characterized by overly literal and unnatural\ntranslations, remains a persistent challenge in LLM-based translation systems.\nDespite their pre-training on vast corpora of natural utterances, LLMs exhibit\ntranslationese errors and generate unexpected unnatural translations, stemming\nfrom biases introduced during supervised fine-tuning (SFT). In this work, we\nsystematically evaluate the prevalence of translationese in LLM-generated\ntranslations and investigate its roots during supervised training. We introduce\nmethods to mitigate these biases, including polishing golden references and\nfiltering unnatural training instances. Empirical evaluations demonstrate that\nthese approaches significantly reduce translationese while improving\ntranslation naturalness, validated by human evaluations and automatic metrics.\nOur findings highlight the need for training-aware adjustments to optimize LLM\ntranslation outputs, paving the way for more fluent and\ntarget-language-consistent translations. We release the data and code at\nhttps://github.com/yafuly/LLM_Translationese.\n","authors":["Yafu Li","Ronghao Zhang","Zhilin Wang","Huajian Zhang","Leyang Cui","Yongjing Yin","Tong Xiao","Yue Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04369v1.pdf","comment":"19 pages;"},{"id":"http://arxiv.org/abs/2410.01257v2","updated":"2025-03-06T12:13:14Z","published":"2024-10-02T06:05:52Z","title":"HelpSteer2-Preference: Complementing Ratings with Preferences","summary":"  Reward models are critical for aligning models to follow instructions, and\nare typically trained following one of two popular paradigms: Bradley-Terry\nstyle or Regression style. However, there is a lack of evidence that either\napproach is better than the other, when adequately matched for data. This is\nprimarily because these approaches require data collected in different (but\nincompatible) formats, meaning that adequately matched data is not available in\nexisting public datasets. To tackle this problem, we release preference\nannotations (designed for Bradley-Terry training) to complement existing\nratings (designed for Regression style training) in the HelpSteer2 dataset. To\nimprove data interpretability, preference annotations are accompanied with\nhuman-written justifications. Using this data, we conduct the first\nhead-to-head comparison of Bradley-Terry and Regression models when adequately\nmatched for data. Based on insights derived from such a comparison, we propose\na novel approach to combine Bradley-Terry and Regression reward modeling. A\nLlama-3.1-70B-Instruct model tuned with this approach scores 94.1 on\nRewardBench, emerging top of more than 140 reward models as of 1 Oct 2024. This\nreward model can then be used with REINFORCE algorithm (RLHF) to align an\nInstruct model to reach 85.0 on Arena Hard, which is No. 1 as of 1 Oct 2024. We\nopen-source this dataset (CC-BY-4.0 license) at\nhttps://huggingface.co/datasets/nvidia/HelpSteer2#preferences-new -- 1-oct-2024\nand openly release the trained Reward and Instruct models at\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Reward and\nhttps://huggingface.co/nvidia/Llama-3.1-Nemotron-70B-Instruct\n","authors":["Zhilin Wang","Alexander Bukharin","Olivier Delalleau","Daniel Egert","Gerald Shen","Jiaqi Zeng","Oleksii Kuchaiev","Yi Dong"],"pdf_url":"https://arxiv.org/pdf/2410.01257v2.pdf","comment":"Accepted to ICLR 2025; 28 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.04360v1","updated":"2025-03-06T12:04:29Z","published":"2025-03-06T12:04:29Z","title":"Exploring the Multilingual NLG Evaluation Abilities of LLM-Based\n  Evaluators","summary":"  Previous research has shown that LLMs have potential in multilingual NLG\nevaluation tasks. However, existing research has not fully explored the\ndifferences in the evaluation capabilities of LLMs across different languages.\nTo this end, this study provides a comprehensive analysis of the multilingual\nevaluation performance of 10 recent LLMs, spanning high-resource and\nlow-resource languages through correlation analysis, perturbation attacks, and\nfine-tuning. We found that 1) excluding the reference answer from the prompt\nand using large-parameter LLM-based evaluators leads to better performance\nacross various languages; 2) most LLM-based evaluators show a higher\ncorrelation with human judgments in high-resource languages than in\nlow-resource languages; 3) in the languages where they are most sensitive to\nsuch attacks, they also tend to exhibit the highest correlation with human\njudgments; and 4) fine-tuning with data from a particular language yields a\nbroadly consistent enhancement in the model's evaluation performance across\ndiverse languages. Our findings highlight the imbalance in LLMs'evaluation\ncapabilities across different languages and suggest that low-resource language\nscenarios deserve more attention.\n","authors":["Jiayi Chang","Mingqi Gao","Xinyu Hu","Xiaojun Wan"],"pdf_url":"https://arxiv.org/pdf/2503.04360v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04355v1","updated":"2025-03-06T11:59:55Z","published":"2025-03-06T11:59:55Z","title":"Layer-Specific Scaling of Positional Encodings for Superior Long-Context\n  Modeling","summary":"  Although large language models (LLMs) have achieved significant progress in\nhandling long-context inputs, they still suffer from the ``lost-in-the-middle''\nproblem, where crucial information in the middle of the context is often\nunderrepresented or lost. Our extensive experiments reveal that this issue may\narise from the rapid long-term decay in Rotary Position Embedding (RoPE). To\naddress this problem, we propose a layer-specific positional encoding scaling\nmethod that assigns distinct scaling factors to each layer, slowing down the\ndecay rate caused by RoPE to make the model pay more attention to the middle\ncontext. A specially designed genetic algorithm is employed to efficiently\nselect the optimal scaling factors for each layer by incorporating Bezier\ncurves to reduce the search space. Through comprehensive experimentation, we\ndemonstrate that our method significantly alleviates the ``lost-in-the-middle''\nproblem. Our approach results in an average accuracy improvement of up to 20%\non the Key-Value Retrieval dataset. Furthermore, we show that layer-specific\ninterpolation, as opposed to uniform interpolation across all layers, enhances\nthe model's extrapolation capabilities when combined with PI and Dynamic-NTK\npositional encoding schemes.\n","authors":["Zhenghua Wang","Yiran Ding","Changze Lv","Zhibo Xu","Tianlong Li","Tianyuan Shi","Xiaoqing Zheng","Xuanjing Huang"],"pdf_url":"https://arxiv.org/pdf/2503.04355v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.03479v2","updated":"2025-03-06T11:46:49Z","published":"2025-01-07T02:47:59Z","title":"Women, Infamous, and Exotic Beings: What Honorific Usages in Wikipedia\n  Reveal about the Socio-Cultural Norms","summary":"  Honorifics serve as powerful linguistic markers that reflect social\nhierarchies and cultural values. This paper presents a large-scale,\ncross-linguistic exploration of usage of honorific pronouns in Bengali and\nHindi Wikipedia articles, shedding light on how socio-cultural factors shape\nlanguage. Using LLM (GPT-4o), we annotated 10, 000 articles of real and\nfictional beings in each language for several sociodemographic features such as\ngender, age, fame, and exoticness, and the use of honorifics. We find that\nacross all feature combinations, use of honorifics is consistently more common\nin Bengali than Hindi. For both languages, the use non-honorific pronouns is\nmore commonly observed for infamous, juvenile, and exotic beings. Notably, we\nobserve a gender bias in use of honorifics in Hindi, with men being more\ncommonly referred to with honorifics than women.\n","authors":["Sourabrata Mukherjee","Soumya Teotia","Sougata Saha","Monojit Choudhury"],"pdf_url":"https://arxiv.org/pdf/2501.03479v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.18640v2","updated":"2025-03-06T11:45:40Z","published":"2024-10-24T11:06:29Z","title":"Weak-to-Strong Preference Optimization: Stealing Reward from Weak\n  Aligned Model","summary":"  Aligning language models (LMs) with human preferences has become a key area\nof research, enabling these models to meet diverse user needs better. Inspired\nby weak-to-strong generalization, where a strong LM fine-tuned on labels\ngenerated by a weaker model can consistently outperform its weak supervisor, we\nextend this idea to model alignment. In this work, we observe that the\nalignment behavior in weaker models can be effectively transferred to stronger\nmodels and even exhibit an amplification effect. Based on this insight, we\npropose a method called Weak-to-Strong Preference Optimization (WSPO), which\nachieves strong model alignment by learning the distribution differences before\nand after the alignment of the weak model. Experiments demonstrate that WSPO\ndelivers outstanding performance, improving the win rate of Qwen2-7B-Instruct\non Arena-Hard from 39.70 to 49.60, achieving a remarkable 47.04\nlength-controlled win rate on AlpacaEval 2, and scoring 7.33 on MT-bench. Our\nresults suggest that using the weak model to elicit a strong model with a high\nalignment ability is feasible.\n","authors":["Wenhong Zhu","Zhiwei He","Xiaofeng Wang","Pengfei Liu","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2410.18640v2.pdf","comment":"ICLR 2025(Spotlight)"},{"id":"http://arxiv.org/abs/2503.04346v1","updated":"2025-03-06T11:42:03Z","published":"2025-03-06T11:42:03Z","title":"Adding Alignment Control to Language Models","summary":"  Post-training alignment has increasingly become a crucial factor in enhancing\nthe usability of language models (LMs). However, the strength of alignment\nvaries depending on individual preferences. This paper proposes a method to\nincorporate alignment control into a single model, referred to as CLM. This\napproach adds one identity layer preceding the initial layers and performs\npreference learning only on this layer to map unaligned input token embeddings\ninto the aligned space. Experimental results demonstrate that this efficient\nfine-tuning method performs comparable to full fine-tuning. During inference,\nthe input embeddings are processed through the aligned and unaligned layers,\nwhich are then merged through the interpolation coefficient. By controlling\nthis parameter, the alignment exhibits a clear interpolation and extrapolation\nphenomenon.\n","authors":["Wenhong Zhu","Weinan Zhang","Rui Wang"],"pdf_url":"https://arxiv.org/pdf/2503.04346v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04338v1","updated":"2025-03-06T11:34:49Z","published":"2025-03-06T11:34:49Z","title":"In-depth Analysis of Graph-based RAG in a Unified Framework","summary":"  Graph-based Retrieval-Augmented Generation (RAG) has proven effective in\nintegrating external knowledge into large language models (LLMs), improving\ntheir factual accuracy, adaptability, interpretability, and trustworthiness. A\nnumber of graph-based RAG methods have been proposed in the literature.\nHowever, these methods have not been systematically and comprehensively\ncompared under the same experimental settings. In this paper, we first\nsummarize a unified framework to incorporate all graph-based RAG methods from a\nhigh-level perspective. We then extensively compare representative graph-based\nRAG methods over a range of questing-answering (QA) datasets -- from specific\nquestions to abstract questions -- and examine the effectiveness of all\nmethods, providing a thorough analysis of graph-based RAG approaches. As a\nbyproduct of our experimental analysis, we are also able to identify new\nvariants of the graph-based RAG methods over specific QA and abstract QA tasks\nrespectively, by combining existing techniques, which outperform the\nstate-of-the-art methods. Finally, based on these findings, we offer promising\nresearch opportunities. We believe that a deeper understanding of the behavior\nof existing methods can provide new valuable insights for future research.\n","authors":["Yingli Zhou","Yaodong Su","Youran Sun","Shu Wang","Taotao Wang","Runyuan He","Yongwei Zhang","Sicong Liang","Xilin Liu","Yuchi Ma","Yixiang Fang"],"pdf_url":"https://arxiv.org/pdf/2503.04338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04328v1","updated":"2025-03-06T11:27:55Z","published":"2025-03-06T11:27:55Z","title":"Solving Word-Sense Disambiguation and Word-Sense Induction with\n  Dictionary Examples","summary":"  Many less-resourced languages struggle with a lack of large, task-specific\ndatasets that are required for solving relevant tasks with modern\ntransformer-based large language models (LLMs). On the other hand, many\nlinguistic resources, such as dictionaries, are rarely used in this context\ndespite their large information contents. We show how LLMs can be used to\nextend existing language resources in less-resourced languages for two\nimportant tasks: word-sense disambiguation (WSD) and word-sense induction\n(WSI). We approach the two tasks through the related but much more accessible\nword-in-context (WiC) task where, given a pair of sentences and a target word,\na classification model is tasked with predicting whether the sense of a given\nword differs between sentences. We demonstrate that a well-trained model for\nthis task can distinguish between different word senses and can be adapted to\nsolve the WSD and WSI tasks. The advantage of using the WiC task, instead of\ndirectly predicting senses, is that the WiC task does not need pre-constructed\nsense inventories with a sufficient number of examples for each sense, which\nare rarely available in less-resourced languages. We show that sentence pairs\nfor the WiC task can be successfully generated from dictionary examples using\nLLMs. The resulting prediction models outperform existing models on WiC, WSD,\nand WSI tasks. We demonstrate our methodology on the Slovene language, where a\nmonolingual dictionary is available, but word-sense resources are tiny.\n","authors":["Tadej Škvorc","Marko Robnik-Šikonja"],"pdf_url":"https://arxiv.org/pdf/2503.04328v1.pdf","comment":"12 pages, 1 figure"},{"id":"http://arxiv.org/abs/2502.15109v2","updated":"2025-03-06T11:07:48Z","published":"2025-02-21T00:05:40Z","title":"Social Genome: Grounded Social Reasoning Abilities of Multimodal Models","summary":"  Social reasoning abilities are crucial for AI systems to effectively\ninterpret and respond to multimodal human communication and interaction within\nsocial contexts. We introduce Social Genome, the first benchmark for\nfine-grained, grounded social reasoning abilities of multimodal models. Social\nGenome contains 272 videos of interactions and 1,486 human-annotated reasoning\ntraces related to inferences about these interactions. These traces contain\n5,777 reasoning steps that reference evidence from visual cues, verbal cues,\nvocal cues, and external knowledge (contextual knowledge external to videos).\nSocial Genome is also the first modeling challenge to study external knowledge\nin social reasoning. Social Genome computes metrics to holistically evaluate\nsemantic and structural qualities of model-generated social reasoning traces.\nWe demonstrate the utility of Social Genome through experiments with\nstate-of-the-art models, identifying performance gaps and opportunities for\nfuture research to improve the grounded social reasoning abilities of\nmultimodal models.\n","authors":["Leena Mathur","Marian Qian","Paul Pu Liang","Louis-Philippe Morency"],"pdf_url":"https://arxiv.org/pdf/2502.15109v2.pdf","comment":"Under Review, 22 pages"},{"id":"http://arxiv.org/abs/2503.03417v2","updated":"2025-03-06T11:00:35Z","published":"2025-03-05T11:47:32Z","title":"When Claims Evolve: Evaluating and Enhancing the Robustness of Embedding\n  Models Against Misinformation Edits","summary":"  Online misinformation remains a critical challenge, and fact-checkers\nincreasingly rely on embedding-based methods to retrieve relevant fact-checks.\nYet, when debunked claims reappear in edited forms, the performance of these\nmethods is unclear. In this work, we introduce a taxonomy of six common\nreal-world misinformation edits and propose a perturbation framework that\ngenerates valid, natural claim variations. Our multi-stage retrieval evaluation\nreveals that standard embedding models struggle with user-introduced edits,\nwhile LLM-distilled embeddings offer improved robustness at a higher\ncomputational cost. Although a strong reranker helps mitigate some issues, it\ncannot fully compensate for first-stage retrieval gaps. Addressing these\nretrieval gaps, our train- and inference-time mitigation approaches enhance\nin-domain robustness by up to 17 percentage points and boost out-of-domain\ngeneralization by 10 percentage points over baseline models. Overall, our\nfindings provide practical improvements to claim-matching systems, enabling\nmore reliable fact-checking of evolving misinformation.\n","authors":["Jabez Magomere","Emanuele La Malfa","Manuel Tonneau","Ashkan Kazemi","Scott Hale"],"pdf_url":"https://arxiv.org/pdf/2503.03417v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04305v1","updated":"2025-03-06T10:46:15Z","published":"2025-03-06T10:46:15Z","title":"Computational Law: Datasets, Benchmarks, and Ontologies","summary":"  Recent developments in computer science and artificial intelligence have also\ncontributed to the legal domain, as revealed by the number and range of related\npublications and applications. Machine and deep learning models require\nconsiderable amount of domain-specific data for training and comparison\npurposes, in order to attain high-performance in the legal domain.\nAdditionally, semantic resources such as ontologies are valuable for building\nlarge-scale computational legal systems, in addition to ensuring\ninteroperability of such systems. Considering these aspects, we present an\nup-to-date review of the literature on datasets, benchmarks, and ontologies\nproposed for computational law. We believe that this comprehensive and recent\nreview will help researchers and practitioners when developing and testing\napproaches and systems for computational law.\n","authors":["Dilek Küçük","Fazli Can"],"pdf_url":"https://arxiv.org/pdf/2503.04305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01924v2","updated":"2025-03-06T10:39:48Z","published":"2024-05-03T08:34:13Z","title":"Semi-Parametric Retrieval via Binary Bag-of-Tokens Index","summary":"  Information retrieval has transitioned from standalone systems into essential\ncomponents across broader applications, with indexing efficiency,\ncost-effectiveness, and freshness becoming increasingly critical yet often\noverlooked. In this paper, we introduce SemI-parametric Disentangled Retrieval\n(SiDR), a bi-encoder retrieval framework that decouples retrieval index from\nneural parameters to enable efficient, low-cost, and parameter-agnostic\nindexing for emerging use cases. Specifically, in addition to using embeddings\nas indexes like existing neural retrieval methods, SiDR supports a\nnon-parametric tokenization index for search, achieving BM25-like indexing\ncomplexity with significantly better effectiveness. Our comprehensive\nevaluation across 16 retrieval benchmarks demonstrates that SiDR outperforms\nboth neural and term-based retrieval baselines under the same indexing\nworkload: (i) When using an embedding-based index, SiDR exceeds the performance\nof conventional neural retrievers while maintaining similar training\ncomplexity; (ii) When using a tokenization-based index, SiDR drastically\nreduces indexing cost and time, matching the complexity of traditional\nterm-based retrieval, while consistently outperforming BM25 on all in-domain\ndatasets; (iii) Additionally, we introduce a late parametric mechanism that\nmatches BM25 index preparation time while outperforming other neural retrieval\nbaselines in effectiveness.\n","authors":["Jiawei Zhou","Li Dong","Furu Wei","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2405.01924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04279v1","updated":"2025-03-06T10:07:51Z","published":"2025-03-06T10:07:51Z","title":"Dual-Class Prompt Generation: Enhancing Indonesian Gender-Based Hate\n  Speech Detection through Data Augmentation","summary":"  Detecting gender-based hate speech in Indonesian social media remains\nchallenging due to limited labeled datasets. While binary hate speech\nclassification has advanced, a more granular category like gender-targeted hate\nspeech is understudied because of class imbalance issues. This paper addresses\nthis gap by comparing three data augmentation techniques for Indonesian\ngender-based hate speech detection. We evaluate backtranslation, single-class\nprompt generation (using only hate speech examples), and our proposed\ndual-class prompt generation (using both hate speech and non-hate speech\nexamples). Experiments show all augmentation methods improve classification\nperformance, with our dual-class approach achieving the best results (88.5%\naccuracy, 88.1% F1-score using Random Forest). Semantic similarity analysis\nreveals dual-class prompt generation produces the most novel content, while\nT-SNE visualizations confirm these samples occupy distinct feature space\nregions while maintaining class characteristics. Our findings suggest that\nincorporating examples from both classes helps language models generate more\ndiverse yet representative samples, effectively addressing limited data\nchallenges in specialized hate speech detection.\n","authors":["Muhammad Amien Ibrahim"," Faisal","Tora Sangputra Yopie Winarto","Zefanya Delvin Sulistiya"],"pdf_url":"https://arxiv.org/pdf/2503.04279v1.pdf","comment":"Accepted to the 8th World Conference on Computing and Communication\n  Technologies (WCCCT 2025)"},{"id":"http://arxiv.org/abs/2503.04271v1","updated":"2025-03-06T10:02:25Z","published":"2025-03-06T10:02:25Z","title":"On Fact and Frequency: LLM Responses to Misinformation Expressed with\n  Uncertainty","summary":"  We study LLM judgments of misinformation expressed with uncertainty. Our\nexperiments study the response of three widely used LLMs (GPT-4o, LlaMA3,\nDeepSeek-v2) to misinformation propositions that have been verified false and\nthen are transformed into uncertain statements according to an uncertainty\ntypology. Our results show that after transformation, LLMs change their\nfactchecking classification from false to not-false in 25% of the cases.\nAnalysis reveals that the change cannot be explained by predictors to which\nhumans are expected to be sensitive, i.e., modality, linguistic cues, or\nargumentation strategy. The exception is doxastic transformations, which use\nlinguistic cue phrases such as \"It is believed ...\".To gain further insight, we\nprompt the LLM to make another judgment about the transformed misinformation\nstatements that is not related to truth value. Specifically, we study LLM\nestimates of the frequency with which people make the uncertain statement. We\nfind a small but significant correlation between judgment of fact and\nestimation of frequency.\n","authors":["Yana van de Sande","Gunes Açar","Thabo van Woudenberg","Martha Larson"],"pdf_url":"https://arxiv.org/pdf/2503.04271v1.pdf","comment":"4 pages, 1 figure, 3 tables, conference"},{"id":"http://arxiv.org/abs/2503.04240v1","updated":"2025-03-06T09:21:54Z","published":"2025-03-06T09:21:54Z","title":"DiffPO: Diffusion-styled Preference Optimization for Efficient\n  Inference-Time Alignment of Large Language Models","summary":"  Inference-time alignment provides an efficient alternative for aligning LLMs\nwith humans. However, these approaches still face challenges, such as limited\nscalability due to policy-specific value functions and latency during the\ninference phase. In this paper, we propose a novel approach, Diffusion-styled\nPreference Optimization (\\model), which provides an efficient and\npolicy-agnostic solution for aligning LLMs with humans. By directly performing\nalignment at sentence level, \\model~avoids the time latency associated with\ntoken-level generation. Designed as a plug-and-play module, \\model~can be\nseamlessly integrated with various base models to enhance their alignment.\nExtensive experiments on AlpacaEval 2, MT-bench, and HH-RLHF demonstrate that\n\\model~achieves superior alignment performance across various settings,\nachieving a favorable trade-off between alignment quality and inference-time\nlatency. Furthermore, \\model~demonstrates model-agnostic scalability,\nsignificantly improving the performance of large models such as Llama-3-70B.\n","authors":["Ruizhe Chen","Wenhao Chai","Zhifei Yang","Xiaotian Zhang","Joey Tianyi Zhou","Tony Quek","Soujanya Poria","Zuozhu Liu"],"pdf_url":"https://arxiv.org/pdf/2503.04240v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04232v1","updated":"2025-03-06T09:14:02Z","published":"2025-03-06T09:14:02Z","title":"Tgea: An error-annotated dataset and benchmark tasks for text generation\n  from pretrained language models","summary":"  In order to deeply understand the capability of pretrained language models in\ntext generation and conduct a diagnostic evaluation, we propose TGEA, an\nerror-annotated dataset with multiple benchmark tasks for text generation from\npretrained language models (PLMs). We use carefully selected prompt words to\nguide GPT-2 to generate candidate sentences, from which we select 47K for error\nannotation. Crowdsourced workers manually check each of these sentences and\ndetect 12k erroneous sentences. We create an error taxonomy to cover 24 types\nof errors occurring in these erroneous sentences according to the nature of\nerrors with respect to linguistics and knowledge (eg, common sense). For each\nerroneous span in PLM-generated sentences, we also detect another span that is\nclosely associated with it. Each error is hence manually labeled with\ncomprehensive annotations, including the span of the error, the associated\nspan, minimal correction to the error, the type of the error, and rationale\nbehind the error. Apart from the fully annotated dataset, we also present a\ndetailed description of the data collection procedure, statistics and analysis\nof the dataset. This is the first dataset with comprehensive annotations for\nPLM-generated texts, which facilitates the diagnostic evaluation of PLM-based\ntext generation. Furthermore, we use TGEA as a benchmark dataset and propose a\nseries of automatic diagnosis tasks, including error detection, error type\nclassification, associated span detection, error rationale generation, to\nfurther promote future study on the automatic error detection and correction on\ntexts generated by pretrained language models.\n","authors":["Jie He","Bo Peng","Yi Liao","Qun Liu","Deyi Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.04232v1.pdf","comment":"ACL 2021"},{"id":"http://arxiv.org/abs/2503.04222v1","updated":"2025-03-06T09:03:36Z","published":"2025-03-06T09:03:36Z","title":"FuseChat-3.0: Preference Optimization Meets Heterogeneous Model Fusion","summary":"  We introduce FuseChat-3.0, a suite of large language models (LLMs) developed\nby integrating the strengths of heterogeneous source LLMs into more compact\ntarget LLMs. Our source models include the powerful Gemma-2-27B-it,\nMistral-Large-Instruct-2407, Qwen-2.5-72B-Instruct, and Llama-3.1-70B-Instruct.\nFor target models, we focus on three widely-used smaller\nvariants-Llama-3.1-8B-Instruct, Gemma-2-9B-it, and Qwen-2.5-7B-Instruct-along\nwith two ultra-compact options, Llama-3.2-3B-Instruct and\nLlama-3.2-1B-Instruct. To leverage the diverse capabilities of these source\nmodels, we develop a specialized data construction protocol tailored to various\ntasks and domains. The FuseChat-3.0 training pipeline consists of two key\nstages: (1) supervised fine-tuning (SFT) to align the target and source model\ndistributions, and (2) Direct Preference Optimization (DPO) to apply\npreferences from multiple source LLMs to fine-tune the target model. The\nresulting FuseChat-3.0 models exhibit significant performance gains across\ntasks such as instruction following, general knowledge, mathematics, and\ncoding. As illustrated in Figure 1, using Llama-3.1-8B-Instruct as the target\nmodel, our fusion approach achieves an average improvement of 6.8 points across\n14 benchmarks. Moreover, it demonstrates remarkable gains of 37.1 points and\n30.1 points on the instruction-following benchmarks AlpacaEval-2 and\nArena-Hard, respectively. Our code, models, and datasets are available at\nhttps://github.com/SLIT-AI/FuseChat-3.0.\n","authors":["Ziyi Yang","Fanqi Wan","Longguang Zhong","Canbin Huang","Guosheng Liang","Xiaojun Quan"],"pdf_url":"https://arxiv.org/pdf/2503.04222v1.pdf","comment":"Technical report"},{"id":"http://arxiv.org/abs/2408.14153v3","updated":"2025-03-06T09:00:18Z","published":"2024-08-26T09:55:34Z","title":"Explaining Caption-Image Interactions in CLIP models with Second-Order\n  Attributions","summary":"  Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and predict similarities between them. Despite their\nsuccess, it is, however, not understood how these models compare their two\ninputs. Common first-order feature-attribution methods can only provide limited\ninsights into dual-encoders since their predictions depend on\nfeature-interactions rather than on individual features. In this paper, we\nfirst derive a second-order method enabling the attribution of predictions by\nany differentiable dual encoder onto feature-interactions between its inputs.\nSecond, we apply our method to CLIP models and show that they learn\nfine-grained correspondences between parts of captions and regions in images.\nThey match objects across input modes also account for mismatches. This\nvisual-linguistic grounding ability, however, varies heavily between object\nclasses and exhibits pronounced out-of-domain effects. We can identify\nindividual errors as well as systematic failure categories including object\ncoverage, unusual scenes and correlated contexts.\n","authors":["Lucas Möller","Pascal Tilli","Ngoc Thang Vu","Sebastian Padó"],"pdf_url":"https://arxiv.org/pdf/2408.14153v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02495v2","updated":"2025-03-06T08:51:47Z","published":"2025-03-04T11:01:25Z","title":"Union of Experts: Adapting Hierarchical Routing to Equivalently\n  Decomposed Transformer","summary":"  We propose Union-of-Experts (UoE), which decomposes transformer into an\nequitant group of experts, and then implement selective routing on input data\nand experts. Our approach advances MoE design with four key innovations: (1) We\nconducted equitant expert decomposition on both MLP blocks and attention blocks\nbased on matrix partition in tensor parallelism. (2) We developed two routing\nparadigms: patch-wise data selection and expert selection, to apply routing\nacross different levels. (3) We design the architecture of UoE model, including\nSelective Multi-Head Attention (SMHA) and Union-of-MLP-Experts (UoME). (4) We\ndevelop parallel implementation of UoE's routing and computation operation, and\noptimize efficiency based on the hardware processing analysis. The experiments\ndemonstrate that the UoE model surpass Full Attention, state-of-art MoEs and\nefficient transformers (including the model architecture of recently proposed\nDeepSeek-V3) in several tasks across image and natural language domains. In\nlanguage modeling tasks, we achieve an average reduction of 2.38 in perplexity\ncompared to the best-performed MoE method with an average of 76% FLOPs. In Long\nRange Arena benchmark, we recorded an average score that is at least 0.68%\nhigher than all comparison models including Full Attention, MoEs, and\ntransformer variants, with only 50% FLOPs of the best MoE method. In image\nclassification, our model yielded an average accuracy improvement of 1.75% than\nthe best model while maintaining comparable FLOPs. The source codes are\navailable at https://github.com/YujiaoYang-work/UoE.\n","authors":["Yujiao Yang","Jing Lian","Linhui Li"],"pdf_url":"https://arxiv.org/pdf/2503.02495v2.pdf","comment":"17 pages"},{"id":"http://arxiv.org/abs/2410.07009v2","updated":"2025-03-06T08:51:05Z","published":"2024-10-09T15:52:48Z","title":"Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with\n  Patent-Paper Pairs","summary":"  Dealing with long and highly complex technical text is a challenge for Large\nLanguage Models (LLMs), which still have to unfold their potential in\nsupporting expensive and timeintensive processes like patent drafting. Within\npatents, the description constitutes more than 90% of the document on average.\nYet, its automatic generation remains understudied. When drafting patent\napplications, patent attorneys typically receive invention reports (IRs), which\nare usually confidential, hindering research on LLM-supported patent drafting.\nOften, prepublication research papers serve as IRs. We leverage this duality to\nbuild PAP2PAT, an open and realistic benchmark for patent drafting consisting\nof 1.8k patent-paper pairs describing the same inventions. To address the\ncomplex longdocument patent generation task, we propose chunk-based\noutline-guided generation using the research paper as invention specification.\nOur extensive evaluation using PAP2PAT and a human case study show that LLMs\ncan effectively leverage information from the paper, but still struggle to\nprovide the necessary level of detail. Fine-tuning leads to more patent-style\nlanguage, but also to more hallucination. We release our data and code\nhttps://github.com/boschresearch/Pap2Pat.\n","authors":["Valentin Knappich","Simon Razniewski","Anna Hätty","Annemarie Friedrich"],"pdf_url":"https://arxiv.org/pdf/2410.07009v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04201v1","updated":"2025-03-06T08:28:44Z","published":"2025-03-06T08:28:44Z","title":"Knowledge-Decoupled Synergetic Learning: An MLLM based Collaborative\n  Approach to Few-shot Multimodal Dialogue Intention Recognition","summary":"  Few-shot multimodal dialogue intention recognition is a critical challenge in\nthe e-commerce domainn. Previous methods have primarily enhanced model\nclassification capabilities through post-training techniques. However, our\nanalysis reveals that training for few-shot multimodal dialogue intention\nrecognition involves two interconnected tasks, leading to a seesaw effect in\nmulti-task learning. This phenomenon is attributed to knowledge interference\nstemming from the superposition of weight matrix updates during the training\nprocess. To address these challenges, we propose Knowledge-Decoupled Synergetic\nLearning (KDSL), which mitigates these issues by utilizing smaller models to\ntransform knowledge into interpretable rules, while applying the post-training\nof larger models. By facilitating collaboration between the large and small\nmultimodal large language models for prediction, our approach demonstrates\nsignificant improvements. Notably, we achieve outstanding results on two real\nTaobao datasets, with enhancements of 6.37\\% and 6.28\\% in online weighted F1\nscores compared to the state-of-the-art method, thereby validating the efficacy\nof our framework.\n","authors":["Bin Chen","Yu Zhang","Hongfei Ye","Ziyi Huang","Hongyang Chen"],"pdf_url":"https://arxiv.org/pdf/2503.04201v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.12106v3","updated":"2025-03-06T08:18:50Z","published":"2024-09-18T16:26:22Z","title":"Measuring Human and AI Values Based on Generative Psychometrics with\n  Large Language Models","summary":"  Human values and their measurement are long-standing interdisciplinary\ninquiry. Recent advances in AI have sparked renewed interest in this area, with\nlarge language models (LLMs) emerging as both tools and subjects of value\nmeasurement. This work introduces Generative Psychometrics for Values (GPV), an\nLLM-based, data-driven value measurement paradigm, theoretically grounded in\ntext-revealed selective perceptions. The core idea is to dynamically parse\nunstructured texts into perceptions akin to static stimuli in traditional\npsychometrics, measure the value orientations they reveal, and aggregate the\nresults. Applying GPV to human-authored blogs, we demonstrate its stability,\nvalidity, and superiority over prior psychological tools. Then, extending GPV\nto LLM value measurement, we advance the current art with 1) a psychometric\nmethodology that measures LLM values based on their scalable and free-form\noutputs, enabling context-specific measurement; 2) a comparative analysis of\nmeasurement paradigms, indicating response biases of prior methods; and 3) an\nattempt to bridge LLM values and their safety, revealing the predictive power\nof different value systems and the impacts of various values on LLM safety.\nThrough interdisciplinary efforts, we aim to leverage AI for next-generation\npsychometrics and psychometrics for value-aligned AI.\n","authors":["Haoran Ye","Yuhang Xie","Yuanyi Ren","Hanjun Fang","Xin Zhang","Guojie Song"],"pdf_url":"https://arxiv.org/pdf/2409.12106v3.pdf","comment":"Accepted at AAAI 2025"},{"id":"http://arxiv.org/abs/2503.04188v1","updated":"2025-03-06T08:03:51Z","published":"2025-03-06T08:03:51Z","title":"Measuring temporal effects of agent knowledge by date-controlled tool\n  use","summary":"  Temporal progression is an integral part of knowledge accumulation and\nupdate. Web search is frequently adopted as grounding for agent knowledge, yet\nits inappropriate configuration affects the quality of agent responses. Here,\nwe construct a tool-based out-of-sample testing framework to measure the\nknowledge variability of large language model (LLM) agents from distinct\ndate-controlled tools (DCTs). We demonstrate the temporal effects of an LLM\nagent as a writing assistant, which can use web search to help complete\nscientific publication abstracts. We show that temporal effects of the search\nengine translates into tool-dependent agent performance but can be alleviated\nwith base model choice and explicit reasoning instructions such as\nchain-of-thought prompting. Our results indicate that agent evaluation should\ntake a dynamical view and account for the temporal influence of tools and the\nupdates of external resources.\n","authors":["R. Patrick Xian","Qiming Cui","Stefan Bauer","Reza Abbasi-Asl"],"pdf_url":"https://arxiv.org/pdf/2503.04188v1.pdf","comment":"comments welcome"},{"id":"http://arxiv.org/abs/2503.04184v1","updated":"2025-03-06T07:53:24Z","published":"2025-03-06T07:53:24Z","title":"Large-Scale AI in Telecom: Charting the Roadmap for Innovation,\n  Scalability, and Enhanced Digital Experiences","summary":"  This white paper discusses the role of large-scale AI in the\ntelecommunications industry, with a specific focus on the potential of\ngenerative AI to revolutionize network functions and user experiences,\nespecially in the context of 6G systems. It highlights the development and\ndeployment of Large Telecom Models (LTMs), which are tailored AI models\ndesigned to address the complex challenges faced by modern telecom networks.\nThe paper covers a wide range of topics, from the architecture and deployment\nstrategies of LTMs to their applications in network management, resource\nallocation, and optimization. It also explores the regulatory, ethical, and\nstandardization considerations for LTMs, offering insights into their future\nintegration into telecom infrastructure. The goal is to provide a comprehensive\nroadmap for the adoption of LTMs to enhance scalability, performance, and\nuser-centric innovation in telecom networks.\n","authors":["Adnan Shahid","Adrian Kliks","Ahmed Al-Tahmeesschi","Ahmed Elbakary","Alexandros Nikou","Ali Maatouk","Ali Mokh","Amirreza Kazemi","Antonio De Domenico","Athanasios Karapantelakis","Bo Cheng","Bo Yang","Bohao Wang","Carlo Fischione","Chao Zhang","Chaouki Ben Issaid","Chau Yuen","Chenghui Peng","Chongwen Huang","Christina Chaccour","Christo Kurisummoottil Thomas","Dheeraj Sharma","Dimitris Kalogiros","Dusit Niyato","Eli De Poorter","Elissa Mhanna","Emilio Calvanese Strinati","Faouzi Bader","Fathi Abdeldayem","Fei Wang","Fenghao Zhu","Gianluca Fontanesi","Giovanni Geraci","Haibo Zhou","Hakimeh Purmehdi","Hamed Ahmadi","Hang Zou","Hongyang Du","Hoon Lee","Howard H. Yang","Iacopo Poli","Igor Carron","Ilias Chatzistefanidis","Inkyu Lee","Ioannis Pitsiorlas","Jaron Fontaine","Jiajun Wu","Jie Zeng","Jinan Li","Jinane Karam","Johny Gemayel","Juan Deng","Julien Frison","Kaibin Huang","Kehai Qiu","Keith Ball","Kezhi Wang","Kun Guo","Leandros Tassiulas","Lecorve Gwenole","Liexiang Yue","Lina Bariah","Louis Powell","Marcin Dryjanski","Maria Amparo Canaveras Galdon","Marios Kountouris","Maryam Hafeez","Maxime Elkael","Mehdi Bennis","Mehdi Boudjelli","Meiling Dai","Merouane Debbah","Michele Polese","Mohamad Assaad","Mohamed Benzaghta","Mohammad Al Refai","Moussab Djerrab","Mubeen Syed","Muhammad Amir","Na Yan","Najla Alkaabi","Nan Li","Nassim Sehad","Navid Nikaein","Omar Hashash","Pawel Sroka","Qianqian Yang","Qiyang Zhao","Rasoul Nikbakht Silab","Rex Ying","Roberto Morabito","Rongpeng Li","Ryad Madi","Salah Eddine El Ayoubi","Salvatore D'Oro","Samson Lasaulce","Serveh Shalmashi","Sige Liu","Sihem Cherrared","Swarna Bindu Chetty","Swastika Dutta","Syed A. R. Zaidi","Tianjiao Chen","Timothy Murphy","Tommaso Melodia","Tony Q. S. Quek","Vishnu Ram","Walid Saad","Wassim Hamidouche","Weilong Chen","Xiaoou Liu","Xiaoxue Yu","Xijun Wang","Xingyu Shang","Xinquan Wang","Xuelin Cao","Yang Su","Yanping Liang","Yansha Deng","Yifan Yang","Yingping Cui","Yu Sun","Yuxuan Chen","Yvan Pointurier","Zeinab Nehme","Zeinab Nezami","Zhaohui Yang","Zhaoyang Zhang","Zhe Liu","Zhenyu Yang","Zhu Han","Zhuang Zhou","Zihan Chen","Zirui Chen","Zitao Shuai"],"pdf_url":"https://arxiv.org/pdf/2503.04184v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04176v1","updated":"2025-03-06T07:44:17Z","published":"2025-03-06T07:44:17Z","title":"TIMER: Temporal Instruction Modeling and Evaluation for Longitudinal\n  Clinical Records","summary":"  Large language models (LLMs) have emerged as promising tools for assisting in\nmedical tasks, yet processing Electronic Health Records (EHRs) presents unique\nchallenges due to their longitudinal nature. While LLMs' capabilities to\nperform medical tasks continue to improve, their ability to reason over\ntemporal dependencies across multiple patient visits and time frames remains\nunexplored. We introduce TIMER (Temporal Instruction Modeling and Evaluation\nfor Longitudinal Clinical Records), a framework that incorporate\ninstruction-response pairs grounding to different parts of a patient's record\nas a critical dimension in both instruction evaluation and tuning for\nlongitudinal clinical records. We develop TIMER-Bench, the first time-aware\nbenchmark that evaluates temporal reasoning capabilities over longitudinal\nEHRs, as well as TIMER-Instruct, an instruction-tuning methodology for LLMs to\nlearn reasoning over time. We demonstrate that models fine-tuned with\nTIMER-Instruct improve performance by 7.3% on human-generated benchmarks and\n9.2% on TIMER-Bench, indicating that temporal instruction-tuning improves model\nperformance for reasoning over EHR.\n","authors":["Hejie Cui","Alyssa Unell","Bowen Chen","Jason Alan Fries","Emily Alsentzer","Sanmi Koyejo","Nigam Shah"],"pdf_url":"https://arxiv.org/pdf/2503.04176v1.pdf","comment":"Preprint"},{"id":"http://arxiv.org/abs/2405.02318v3","updated":"2025-03-06T07:29:44Z","published":"2024-04-18T00:20:48Z","title":"Autoformalizing Natural Language to First-Order Logic: A Case Study in\n  Logical Fallacy Detection","summary":"  Translating natural language into formal language such as First-Order Logic\n(FOL) is a foundational challenge in NLP with wide-ranging applications in\nautomated reasoning, misinformation tracking, and knowledge validation. In this\npaper, we introduce Natural Language to First-Order Logic (NL2FOL), a framework\nto autoformalize natural language to FOL step by step using Large Language\nModels (LLMs). Our approach addresses key challenges in this translation\nprocess, including the integration of implicit background knowledge. By\nleveraging structured representations generated by NL2FOL, we use\nSatisfiability Modulo Theory (SMT) solvers to reason about the logical validity\nof natural language statements. We present logical fallacy detection as a case\nstudy to evaluate the efficacy of NL2FOL. Being neurosymbolic, our approach\nalso provides interpretable insights into the reasoning process and\ndemonstrates robustness without requiring model fine-tuning or labeled training\ndata. Our framework achieves strong performance on multiple datasets. On the\nLOGIC dataset, NL2FOL achieves an F1-score of 78%, while generalizing\neffectively to the LOGICCLIMATE dataset with an F1-score of 80%.\n","authors":["Abhinav Lalwani","Tasha Kim","Lovish Chopra","Christopher Hahn","Zhijing Jin","Mrinmaya Sachan"],"pdf_url":"https://arxiv.org/pdf/2405.02318v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13681v2","updated":"2025-03-06T07:17:09Z","published":"2025-02-19T12:51:35Z","title":"An LLM-based Agent for Reliable Docker Environment Configuration","summary":"  Environment configuration is a critical yet time-consuming step in software\ndevelopment, especially when dealing with unfamiliar code repositories. While\nLarge Language Models (LLMs) demonstrate the potential to accomplish software\nengineering tasks, existing methods for environment configuration often rely on\nmanual efforts or fragile scripts, leading to inefficiencies and unreliable\noutcomes. We introduce Repo2Run, the first LLM-based agent designed to fully\nautomate environment configuration and generate executable Dockerfiles for\narbitrary Python repositories. We address two major challenges: (1) enabling\nthe LLM agent to configure environments within isolated Docker containers, and\n(2) ensuring the successful configuration process is recorded and accurately\ntransferred to a Dockerfile without error. To achieve this, we propose atomic\nconfiguration synthesis, featuring a dual-environment architecture (internal\nand external environment) with a rollback mechanism to prevent environment\n\"pollution\" from failed commands, guaranteeing atomic execution (execute fully\nor not at all) and a Dockerfile generator to transfer successful configuration\nsteps into runnable Dockerfiles. We evaluate Repo2Run~on our proposed benchmark\nof 420 recent Python repositories with unit tests, where it achieves an 86.0%\nsuccess rate, outperforming the best baseline by 63.9%. Repo2Run is available\nat https://github.com/bytedance/Repo2Run.\n","authors":["Ruida Hu","Chao Peng","Xinchen Wang","Cuiyun Gao"],"pdf_url":"https://arxiv.org/pdf/2502.13681v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04155v1","updated":"2025-03-06T07:06:46Z","published":"2025-03-06T07:06:46Z","title":"BPQA Dataset: Evaluating How Well Language Models Leverage Blood\n  Pressures to Answer Biomedical Questions","summary":"  Clinical measurements such as blood pressures and respiration rates are\ncritical in diagnosing and monitoring patient outcomes. It is an important\ncomponent of biomedical data, which can be used to train transformer-based\nlanguage models (LMs) for improving healthcare delivery. It is, however,\nunclear whether LMs can effectively interpret and use clinical measurements. We\ninvestigate two questions: First, can LMs effectively leverage clinical\nmeasurements to answer related medical questions? Second, how to enhance an\nLM's performance on medical question-answering (QA) tasks that involve\nmeasurements? We performed a case study on blood pressure readings (BPs), a\nvital sign routinely monitored by medical professionals. We evaluated the\nperformance of four LMs: BERT, BioBERT, MedAlpaca, and GPT-3.5, on our newly\ndeveloped dataset, BPQA (Blood Pressure Question Answering). BPQA contains\n$100$ medical QA pairs that were verified by medical students and designed to\nrely on BPs . We found that GPT-3.5 and MedAlpaca (larger and medium sized LMs)\nbenefit more from the inclusion of BPs than BERT and BioBERT (small sized LMs).\nFurther, augmenting measurements with labels improves the performance of\nBioBERT and Medalpaca (domain specific LMs), suggesting that retrieval may be\nuseful for improving domain-specific LMs.\n","authors":["Chi Hang","Ruiqi Deng","Lavender Yao Jiang","Zihao Yang","Anton Alyakin","Daniel Alber","Eric Karl Oermann"],"pdf_url":"https://arxiv.org/pdf/2503.04155v1.pdf","comment":"9 pages"},{"id":"http://arxiv.org/abs/2502.18878v2","updated":"2025-03-06T07:06:40Z","published":"2025-02-26T06:45:29Z","title":"Learning to Generate Structured Output with Schema Reinforcement\n  Learning","summary":"  This study investigates the structured generation capabilities of large\nlanguage models (LLMs), focusing on producing valid JSON outputs against a\ngiven schema. Despite the widespread use of JSON in integrating language models\nwith programs, there is a lack of comprehensive analysis and benchmarking of\nthese capabilities. We explore various aspects of JSON generation, such as\nstructure understanding, escaping, and natural language description, to\ndetermine how to assess and enable LLMs to generate valid responses. Building\nupon this, we propose SchemaBench features around 40K different JSON schemas to\nobtain and assess models' abilities in generating valid JSON. We find that the\nlatest LLMs are still struggling to generate a valid JSON string. Moreover, we\ndemonstrate that incorporating reinforcement learning with a Fine-grained\nSchema Validator can further enhance models' understanding of JSON schema,\nleading to improved performance. Our models demonstrate significant improvement\nin both generating JSON outputs and downstream tasks.\n","authors":["Yaxi Lu","Haolun Li","Xin Cong","Zhong Zhang","Yesai Wu","Yankai Lin","Zhiyuan Liu","Fangming Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2502.18878v2.pdf","comment":"8 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.04150v1","updated":"2025-03-06T06:59:09Z","published":"2025-03-06T06:59:09Z","title":"Ticktack : Long Span Temporal Alignment of Large Language Models\n  Leveraging Sexagenary Cycle Time Expression","summary":"  Large language models (LLMs) suffer from temporal misalignment issues\nespecially across long span of time. The issue arises from knowing that LLMs\nare trained on large amounts of data where temporal information is rather\nsparse over long times, such as thousands of years, resulting in insufficient\nlearning or catastrophic forgetting by the LLMs. This paper proposes a\nmethodology named \"Ticktack\" for addressing the LLM's long-time span\nmisalignment in a yearly setting. Specifically, we first propose to utilize the\nsexagenary year expression instead of the Gregorian year expression employed by\nLLMs, achieving a more uniform distribution in yearly granularity. Then, we\nemploy polar coordinates to model the sexagenary cycle of 60 terms and the year\norder within each term, with additional temporal encoding to ensure LLMs\nunderstand them. Finally, we present a temporal representational alignment\napproach for post-training LLMs that effectively distinguishes time points with\nrelevant knowledge, hence improving performance on time-related tasks,\nparticularly over a long period. We also create a long time span benchmark for\nevaluation. Experimental results prove the effectiveness of our proposal.\n","authors":["Xue Han","Qian Hu","Yitong Wang","Wenchun Gao","Lianlian Zhang","Qing Wang","Lijun Mei","Chao Deng","Junlan Feng"],"pdf_url":"https://arxiv.org/pdf/2503.04150v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.06464v3","updated":"2025-03-06T06:57:34Z","published":"2024-12-09T13:09:04Z","title":"Gated Delta Networks: Improving Mamba2 with Delta Rule","summary":"  Linear Transformers have gained attention as efficient alternatives to\nstandard Transformers, but their performance in retrieval and long-context\ntasks has been limited. To address these limitations, recent work has explored\ntwo distinct mechanisms: gating for adaptive memory control and the delta\nupdate rule for precise memory modifications. We observe that these mechanisms\nare complementary: gating enables rapid memory erasure while the delta rule\nfacilitates targeted updates. Building on this insight, we introduce the gated\ndelta rule and develop a parallel training algorithm optimized for modern\nhardware. Our proposed architecture, Gated DeltaNet, consistently surpasses\nexisting models like Mamba2 and DeltaNet across multiple benchmarks, including\nlanguage modeling, common-sense reasoning, in-context retrieval, length\nextrapolation, and long-context understanding. We further enhance performance\nby developing hybrid architectures that combine Gated DeltaNet layers with\nsliding window attention or Mamba2 layers, achieving both improved training\nefficiency and superior task performance.\n","authors":["Songlin Yang","Jan Kautz","Ali Hatamizadeh"],"pdf_url":"https://arxiv.org/pdf/2412.06464v3.pdf","comment":"ICLR 2025 camera ready"},{"id":"http://arxiv.org/abs/2503.04149v1","updated":"2025-03-06T06:56:59Z","published":"2025-03-06T06:56:59Z","title":"Dynamic Benchmarking of Reasoning Capabilities in Code Large Language\n  Models Under Data Contamination","summary":"  The rapid evolution of code largelanguage models underscores the need for\neffective and transparent benchmarking of their reasoning capabilities.\nHowever, the current benchmarking approach heavily depends on publicly\navailable, human-created datasets. The widespread use of these fixed benchmark\ndatasets makes the benchmarking process to be static and thus particularly\nsusceptible to data contamination, an unavoidable consequence of the extensive\ndata collection processes used to train Code LLMs. Existing approaches that\naddress data contamination often suffer from human effort limitations and\nimbalanced problem complexity. To tackle these challenges, we propose \\tool, a\nnovel benchmarking suite for evaluating Code LLMs under potential data\ncontamination. Given a seed programming problem, \\tool employs multiple agents\nto extract and modify the context without altering the core logic, generating\nsemantically equivalent variations. We introduce a dynamic data generation\nmethods and conduct empirical studies on two seed datasets across 21 Code LLMs.\nResults show that \\tool effectively benchmarks reasoning capabilities under\ncontamination risks while generating diverse problem sets to ensure consistent\nand reliable evaluations.\n","authors":["Simin Chen","Pranav Pusarla","Baishakhi Ray"],"pdf_url":"https://arxiv.org/pdf/2503.04149v1.pdf","comment":"https://codekaleidoscope.github.io/dycodeeval.html"},{"id":"http://arxiv.org/abs/2406.01145v2","updated":"2025-03-06T06:49:04Z","published":"2024-06-03T09:38:28Z","title":"Dual Reasoning: A GNN-LLM Collaborative Framework for Knowledge Graph\n  Question Answering","summary":"  Large Language Models (LLMs) excel at intuitive, implicit reasoning. Guiding\nLLMs to construct thought chains can enhance their deliberate reasoning\nabilities, but also faces challenges such as hallucination. Knowledge Graphs\n(KGs) can provide explicit structured knowledge for LLMs to alleviate these\nissues. However, existing KG-enhanced methods often overlook explicit graph\nlearning, making it challenging to efficiently provide precise reasoning chains\nfor LLMs. Following dual-process theory, we propose Dual-Reasoning (DualR), a\nnovel framework that integrates an external system based on Graph Neural\nNetwork (GNN) for explicit reasoning on KGs, complementing the implicit\nreasoning of LLMs through externalized reasoning chains. DualR designs an\nLLM-empowered GNN module for explicit learning on KGs, efficiently extracting\nhigh-quality reasoning chains. These reasoning chains are then refined to a\nknowledge-enhanced multiple-choice prompt, guiding a frozen LLM to reason\nthoughtfully for final answer determination. Extensive experiments on three\nbenchmark KGQA datasets demonstrate that DualR achieves state-of-the-art\nperformance while maintaining high efficiency and interpretability.\n","authors":["Guangyi Liu","Yongqi Zhang","Yong Li","Quanming Yao"],"pdf_url":"https://arxiv.org/pdf/2406.01145v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12767v2","updated":"2025-03-06T06:41:40Z","published":"2025-02-18T11:31:52Z","title":"R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on\n  Knowledge Graphs","summary":"  Recent studies have combined Large Language Models (LLMs) with Knowledge\nGraphs (KGs) to enhance reasoning, improving inference accuracy without\nadditional training while mitigating hallucination. However, existing\nframeworks are often rigid, struggling to adapt to KG or task changes. They\nalso rely heavily on powerful LLMs for reliable (i.e., trustworthy) reasoning.\nTo address this, We introduce R2-KG, a plug-and-play, dual-agent framework that\nseparates reasoning into two roles: an Operator (a low-capacity LLM) that\ngathers evidence and a Supervisor (a high-capacity LLM) that makes final\njudgments. This design is cost-efficient for LLM inference while still\nmaintaining strong reasoning accuracy. Additionally, R2-KG employs an\nAbstention mechanism, generating answers only when sufficient evidence is\ncollected from KG, which significantly enhances reliability. Experiments across\nmultiple KG-based reasoning tasks show that R2-KG consistently outperforms\nbaselines in both accuracy and reliability, regardless of the inherent\ncapability of LLMs used as the Operator. Further experiments reveal that the\nsingle-agent version of R2-KG, equipped with a strict self-consistency\nstrategy, achieves significantly higher-than-baseline reliability while\nreducing inference cost. However, it also leads to a higher abstention rate in\ncomplex KGs. Our findings establish R2-KG as a flexible and cost-effective\nsolution for KG-based reasoning. It reduces reliance on high-capacity LLMs\nwhile ensuring trustworthy inference.\n","authors":["Sumin Jo","Junseong Choi","Jiho Kim","Edward Choi"],"pdf_url":"https://arxiv.org/pdf/2502.12767v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17635v2","updated":"2025-03-06T06:39:56Z","published":"2024-10-23T07:53:29Z","title":"Markov Chain of Thought for Efficient Mathematical Reasoning","summary":"  Chain of Thought (CoT) of multi-step benefits from the logical structure of\nthe reasoning steps and task-specific actions, significantly enhancing the\nmathematical reasoning capabilities of large language models. As the prevalence\nof long CoT, the number of reasoning steps exceeds manageable token limits and\nleads to higher computational demands. Inspired by the fundamental logic of\nhuman cognition, \"derive, then reduce\", we conceptualize the standard\nmulti-step CoT as a novel Markov Chain of Thought (MCoT). In this study, we\nconsider the mathematical reasoning task, defining each reasoning step as text\naccompanied by a Python code snippet. To facilitate a longer reasoning path,\nself-correction is enabled through interactions with the code interpreter. Our\nMCoT aims to compress previous reasoning steps into a simplified question,\nenabling efficient next-step inference without relying on a lengthy KV cache.\nIn our experiments, we curate the $\\texttt{MCoTInstruct}$ dataset, and the\nempirical results indicate that MCoT not only significantly enhances efficiency\nbut also maintains comparable accuracy. While much remains to be explored, this\nwork paves the way for exploring the long CoT reasoning abilities of LLMs. The\ncode is available at https://github.com/james-yw/Markov-Chain-of-Thought\n","authors":["Wen Yang","Minpeng Liao","Kai Fan"],"pdf_url":"https://arxiv.org/pdf/2410.17635v2.pdf","comment":"Camera ready version for NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2503.04141v1","updated":"2025-03-06T06:39:25Z","published":"2025-03-06T06:39:25Z","title":"HEISIR: Hierarchical Expansion of Inverted Semantic Indexing for\n  Training-free Retrieval of Conversational Data using LLMs","summary":"  The growth of conversational AI services has increased demand for effective\ninformation retrieval from dialogue data. However, existing methods often face\nchallenges in capturing semantic intent or require extensive labeling and\nfine-tuning. This paper introduces HEISIR (Hierarchical Expansion of Inverted\nSemantic Indexing for Retrieval), a novel framework that enhances semantic\nunderstanding in conversational data retrieval through optimized data\ningestion, eliminating the need for resource-intensive labeling or model\nadaptation. HEISIR implements a two-step process: (1) Hierarchical Triplets\nFormulation and (2) Adjunct Augmentation, creating semantic indices consisting\nof Subject-Verb-Object-Adjunct (SVOA) quadruplets. This structured\nrepresentation effectively captures the underlying semantic information from\ndialogue content. HEISIR achieves high retrieval performance while maintaining\nlow latency during the actual retrieval process. Our experimental results\ndemonstrate that HEISIR outperforms fine-tuned models across various embedding\ntypes and language models. Beyond improving retrieval capabilities, HEISIR also\noffers opportunities for intent and topic analysis in conversational data,\nproviding a versatile solution for dialogue systems.\n","authors":["Sangyeop Kim","Hangyeul Lee","Yohan Lee"],"pdf_url":"https://arxiv.org/pdf/2503.04141v1.pdf","comment":"Accepted by NAACL 2025 (Findings)"},{"id":"http://arxiv.org/abs/2502.14074v2","updated":"2025-03-06T06:32:54Z","published":"2025-02-19T19:59:16Z","title":"Investigating Non-Transitivity in LLM-as-a-Judge","summary":"  Automatic evaluation methods based on large language models (LLMs) are\nemerging as the standard tool for assessing the instruction-following abilities\nof LLM-based agents. The most common method in this paradigm, pairwise\ncomparisons with a baseline model, critically depends on the assumption of\ntransitive preferences. However, the validity of this assumption remains\nlargely unexplored. In this study, we investigate the presence of\nnon-transitivity within the AlpacaEval framework and analyze its effects on\nmodel rankings. We find that LLM judges exhibit non-transitive preferences,\nleading to rankings that are sensitive to the choice of the baseline model. To\nmitigate this issue, we show that round-robin tournaments combined with\nBradley-Terry models of preference can produce more reliable rankings. Notably,\nour method increases both the Spearman correlation and the Kendall correlation\nwith Chatbot Arena (95.0% -> 96.4% and 82.1% -> 86.3% respectively). To address\nthe computational cost of round-robin tournaments, we propose Swiss-Wise\nIterative Matchmaking (Swim) tournaments, using a dynamic matching strategy to\ncapture the benefits of round-robin tournaments while maintaining computational\nefficiency.\n","authors":["Yi Xu","Laura Ruis","Tim Rocktäschel","Robert Kirk"],"pdf_url":"https://arxiv.org/pdf/2502.14074v2.pdf","comment":"8 pages, 6 figures, 2 tables (30 pages, 11 figures, 8 tables\n  including references and appendices)"},{"id":"http://arxiv.org/abs/2503.04135v1","updated":"2025-03-06T06:28:36Z","published":"2025-03-06T06:28:36Z","title":"Biological Sequence with Language Model Prompting: A Survey","summary":"  Large Language models (LLMs) have emerged as powerful tools for addressing\nchallenges across diverse domains. Notably, recent studies have demonstrated\nthat large language models significantly enhance the efficiency of biomolecular\nanalysis and synthesis, attracting widespread attention from academics and\nmedicine. In this paper, we systematically investigate the application of\nprompt-based methods with LLMs to biological sequences, including DNA, RNA,\nproteins, and drug discovery tasks. Specifically, we focus on how prompt\nengineering enables LLMs to tackle domain-specific problems, such as promoter\nsequence prediction, protein structure modeling, and drug-target binding\naffinity prediction, often with limited labeled data. Furthermore, our\ndiscussion highlights the transformative potential of prompting in\nbioinformatics while addressing key challenges such as data scarcity,\nmultimodal fusion, and computational resource limitations. Our aim is for this\npaper to function both as a foundational primer for newcomers and a catalyst\nfor continued innovation within this dynamic field of study.\n","authors":["Jiyue Jiang","Zikang Wang","Yuheng Shan","Heyan Chai","Jiayi Li","Zixian Ma","Xinrui Zhang","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2503.04135v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.18104v2","updated":"2025-03-06T05:54:29Z","published":"2024-11-27T07:32:56Z","title":"Training and Evaluating Language Models with Template-based Data\n  Generation","summary":"  The rapid advancement of large language models (LLMs) such as GPT-3, PaLM,\nand Llama has significantly transformed natural language processing, showcasing\nremarkable capabilities in understanding and generating language. However,\nthese models often struggle with tasks requiring complex reasoning,\nparticularly in mathematical problem-solving, due in part to the scarcity of\nlarge-scale, high-quality, domain-specific datasets necessary for training\nsophisticated reasoning abilities. To address this limitation, we introduce\nTemplate-based Data Generation (TDG), a novel approach that leverages LLMs\n(GPT-4) to automatically generate parameterized meta-templates, which are then\nused to synthesize a vast array of high-quality problems and solutions.\nLeveraging TDG, we create TemplateMath Part I: TemplateGSM, a dataset\ncomprising over 7 million synthetically generated grade school math\nproblems--each accompanied by code-based and natural language solutions--with\nthe potential to generate an effectively unlimited number more. This dataset\nalleviates the scarcity of large-scale mathematical datasets and serves as a\nvaluable resource for pre-training, fine-tuning, and evaluating LLMs in\nmathematical reasoning. Our method not only enables the generation of virtually\ninfinite data but also elevates data augmentation to a new level by using GPT-4\nfor meta-template generation, ensuring diverse and high-quality problem\nstructures. The TemplateMath Part I: TemplateGSM dataset is publicly available\nat https://huggingface.co/datasets/math-ai/TemplateGSM. The code is available\nat https://github.com/iiis-ai/TemplateMath.\n","authors":["Yifan Zhang"],"pdf_url":"https://arxiv.org/pdf/2411.18104v2.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2409.07055v2","updated":"2025-03-06T05:48:54Z","published":"2024-09-11T07:01:08Z","title":"Legal Fact Prediction: The Missing Piece in Legal Judgment Prediction","summary":"  Legal judgment prediction (LJP), which enables litigants and their lawyers to\nforecast judgment outcomes and refine litigation strategies, has emerged as a\ncrucial legal NLP task. Existing studies typically utilize legal facts, i.e.,\nfacts that have been established by evidence and determined by the judge, to\npredict the judgment. However, legal facts are often difficult to obtain in the\nearly stages of litigation, significantly limiting the practical applicability\nof fact-based LJP. To address this limitation, we propose a novel legal NLP\ntask: \\textit{legal fact prediction} (LFP), which takes the evidence submitted\nby litigants for trial as input to predict legal facts, thereby empowering\nfact-based LJP technologies to perform prediction in the absence of\nground-truth legal facts. We also propose the first benchmark dataset,\nLFPBench, for evaluating the LFP task. Our extensive experiments on LFPBench\ndemonstrate the effectiveness of LFP-empowered LJP and highlight promising\nresearch directions for LFP. Our code and data are available at\nhttps://github.com/HPRCEST/LFPBench.\n","authors":["Junkai Liu","Yujie Tong","Hui Huang","Bowen Zheng","Yiran Hu","Peicheng Wu","Chuan Xiao","Makoto Onizuka","Muyun Yang","Shuyuan Zheng"],"pdf_url":"https://arxiv.org/pdf/2409.07055v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.02398v2","updated":"2025-03-06T05:46:40Z","published":"2024-11-04T18:59:51Z","title":"Prompting with Phonemes: Enhancing LLMs' Multilinguality for Non-Latin\n  Script Languages","summary":"  Although multilingual LLMs have achieved remarkable performance across\nbenchmarks, we find they continue to underperform on non-Latin script languages\nacross contemporary LLM families. This discrepancy arises from the fact that\nLLMs are pretrained with orthographic scripts, which are dominated by Latin\ncharacters that obscure their shared phonology with non-Latin scripts. We\npropose leveraging phonemic transcriptions as complementary signals to induce\nscript-invariant representations. Our study demonstrates that integrating\nphonemic signals improves performance across both non-Latin and Latin script\nlanguages, with a particularly significant impact on closing the performance\ngap between the two. Through detailed experiments, we show that phonemic and\northographic scripts retrieve distinct examples for in-context learning (ICL).\nThis motivates our proposed Mixed-ICL retrieval strategy, where further\naggregation from both leads to our significant performance improvements for\nboth Latin script languages (up to 12.6%) and non-Latin script languages (up to\n15.1%) compared to randomized ICL retrieval.\n","authors":["Hoang H Nguyen","Khyati Mahajan","Vikas Yadav","Julian Salazar","Philip S. Yu","Masoud Hashemi","Rishabh Maheshwary"],"pdf_url":"https://arxiv.org/pdf/2411.02398v2.pdf","comment":"Accepted for NAACL 2025 (Main Conference)"},{"id":"http://arxiv.org/abs/2503.04113v1","updated":"2025-03-06T05:43:35Z","published":"2025-03-06T05:43:35Z","title":"Uncovering Gaps in How Humans and LLMs Interpret Subjective Language","summary":"  Humans often rely on subjective natural language to direct language models\n(LLMs); for example, users might instruct the LLM to write an enthusiastic\nblogpost, while developers might train models to be helpful and harmless using\nLLM-based edits. The LLM's operational semantics of such subjective phrases --\nhow it adjusts its behavior when each phrase is included in the prompt -- thus\ndictates how aligned it is with human intent. In this work, we uncover\ninstances of misalignment between LLMs' actual operational semantics and what\nhumans expect. Our method, TED (thesaurus error detector), first constructs a\nthesaurus that captures whether two phrases have similar operational semantics\naccording to the LLM. It then elicits failures by unearthing disagreements\nbetween this thesaurus and a human-constructed reference. TED routinely\nproduces surprising instances of misalignment; for example, Mistral 7B Instruct\nproduces more harassing outputs when it edits text to be witty, and Llama 3 8B\nInstruct produces dishonest articles when instructed to make the articles\nenthusiastic. Our results demonstrate that humans can uncover unexpected LLM\nbehavior by scrutinizing relationships between abstract concepts, without\nsupervising outputs directly.\n","authors":["Erik Jones","Arjun Patrawala","Jacob Steinhardt"],"pdf_url":"https://arxiv.org/pdf/2503.04113v1.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.07272v2","updated":"2025-03-06T05:41:32Z","published":"2025-02-11T05:39:49Z","title":"GENERator: A Long-Context Generative Genomic Foundation Model","summary":"  Advancements in DNA sequencing technologies have significantly improved our\nability to decode genomic sequences. However, the prediction and interpretation\nof these sequences remain challenging due to the intricate nature of genetic\nmaterial. Large language models (LLMs) have introduced new opportunities for\nbiological sequence analysis. Recent developments in genomic language models\nhave underscored the potential of LLMs in deciphering DNA sequences.\nNonetheless, existing models often face limitations in robustness and\napplication scope, primarily due to constraints in model structure and training\ndata scale. To address these limitations, we present GENERator, a generative\ngenomic foundation model featuring a context length of 98k base pairs (bp) and\n1.2B parameters. Trained on an expansive dataset comprising 386B bp of\neukaryotic DNA, the GENERator demonstrates state-of-the-art performance across\nboth established and newly proposed benchmarks. The model adheres to the\ncentral dogma of molecular biology, accurately generating protein-coding\nsequences that translate into proteins structurally analogous to known\nfamilies. It also shows significant promise in sequence optimization,\nparticularly through the prompt-responsive generation of enhancer sequences\nwith specific activity profiles. These capabilities position the GENERator as a\npivotal tool for genomic research and biotechnological advancement, enhancing\nour ability to interpret and predict complex biological systems and enabling\nprecise genomic interventions. Implementation details and supplementary\nresources are available at https://github.com/GenerTeam/GENERator.\n","authors":["Wei Wu","Qiuyi Li","Mingyang Li","Kun Fu","Fuli Feng","Jieping Ye","Hui Xiong","Zheng Wang"],"pdf_url":"https://arxiv.org/pdf/2502.07272v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.20680v5","updated":"2025-03-06T05:34:13Z","published":"2024-05-31T08:22:49Z","title":"Unraveling and Mitigating Retriever Inconsistencies in\n  Retrieval-Augmented Large Language Models","summary":"  Although Retrieval-Augmented Large Language Models (RALMs) demonstrate their\nsuperiority in terms of factuality, they do not consistently outperform the\noriginal retrieval-free Language Models (LMs). Our experiments reveal that this\nexample-level performance inconsistency exists not only between\nretrieval-augmented and retrieval-free LM but also among different retrievers.\nTo understand this phenomenon, we investigate the degeneration behavior of\nRALMs and theoretically decompose it into four categories. Further analysis\nbased on our decomposition reveals that the innate difference in knowledge\nsources and the unpredictable degeneration of the reader model contribute most\nto the inconsistency. Drawing from our analysis, we introduce Ensemble of\nRetrievers (EoR), a trainable framework that can adaptively retrieve from\ndifferent knowledge sources and effectively decrease unpredictable reader\nerrors. Our experiments on Open Domain Question Answering show that EoR\nsubstantially improves performance over the RALM with a single retriever by\nconsiderably reducing inconsistent behaviors.\n","authors":["Mingda Li","Xinyu Li","Yifan Chen","Wenfeng Xuan","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2405.20680v5.pdf","comment":"ACL 2024 (findings)"},{"id":"http://arxiv.org/abs/2503.04104v1","updated":"2025-03-06T05:25:43Z","published":"2025-03-06T05:25:43Z","title":"LLMs Can Generate a Better Answer by Aggregating Their Own Responses","summary":"  Large Language Models (LLMs) have shown remarkable capabilities across tasks,\nyet they often require additional prompting techniques when facing complex\nproblems. While approaches like self-correction and response selection have\nemerged as popular solutions, recent studies have shown these methods perform\npoorly when relying on the LLM itself to provide feedback or selection\ncriteria. We argue this limitation stems from the fact that common LLM\npost-training procedures lack explicit supervision for discriminative judgment\ntasks. In this paper, we propose Generative Self-Aggregation (GSA), a novel\nprompting method that improves answer quality without requiring the model's\ndiscriminative capabilities. GSA first samples multiple diverse responses from\nthe LLM, then aggregates them to obtain an improved solution. Unlike previous\napproaches, our method does not require the LLM to correct errors or compare\nresponse quality; instead, it leverages the model's generative abilities to\nsynthesize a new response based on the context of multiple samples. While GSA\nshares similarities with the self-consistency (SC) approach for response\naggregation, SC requires specific verifiable tokens to enable majority voting.\nIn contrast, our approach is more general and can be applied to open-ended\ntasks. Empirical evaluation demonstrates that GSA effectively improves response\nquality across various tasks, including mathematical reasoning, knowledge-based\nproblems, and open-ended generation tasks such as code synthesis and\nconversational responses.\n","authors":["Zichong Li","Xinyu Feng","Yuheng Cai","Zixuan Zhang","Tianyi Liu","Chen Liang","Weizhu Chen","Haoyu Wang","Tuo Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.04104v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04099v1","updated":"2025-03-06T05:15:34Z","published":"2025-03-06T05:15:34Z","title":"Disparities in LLM Reasoning Accuracy and Explanations: A Case Study on\n  African American English","summary":"  Large Language Models (LLMs) have demonstrated remarkable capabilities in\nreasoning tasks, leading to their widespread deployment. However, recent\nstudies have highlighted concerning biases in these models, particularly in\ntheir handling of dialectal variations like African American English (AAE). In\nthis work, we systematically investigate dialectal disparities in LLM reasoning\ntasks. We develop an experimental framework comparing LLM performance given\nStandard American English (SAE) and AAE prompts, combining LLM-based dialect\nconversion with established linguistic analyses. We find that LLMs consistently\nproduce less accurate responses and simpler reasoning chains and explanations\nfor AAE inputs compared to equivalent SAE questions, with disparities most\npronounced in social science and humanities domains. These findings highlight\nsystematic differences in how LLMs process and reason about different language\nvarieties, raising important questions about the development and deployment of\nthese systems in our multilingual and multidialectal world. Our code repository\nis publicly available at https://github.com/Runtaozhou/dialect_bias_eval.\n","authors":["Runtao Zhou","Guangya Wan","Saadia Gabriel","Sheng Li","Alexander J Gates","Maarten Sap","Thomas Hartvigsen"],"pdf_url":"https://arxiv.org/pdf/2503.04099v1.pdf","comment":"ARR Under Review, First two authors contribute equally"},{"id":"http://arxiv.org/abs/2503.04095v1","updated":"2025-03-06T05:08:40Z","published":"2025-03-06T05:08:40Z","title":"Chart-HQA: A Benchmark for Hypothetical Question Answering in Charts","summary":"  Multimodal Large Language Models (MLLMs) have garnered significant attention\nfor their strong visual-semantic understanding. Most existing chart benchmarks\nevaluate MLLMs' ability to parse information from charts to answer\nquestions.However, they overlook the inherent output biases of MLLMs, where\nmodels rely on their parametric memory to answer questions rather than\ngenuinely understanding the chart content. To address this limitation, we\nintroduce a novel Chart Hypothetical Question Answering (HQA) task, which\nimposes assumptions on the same question to compel models to engage in\ncounterfactual reasoning based on the chart content. Furthermore, we introduce\nHAI, a human-AI interactive data synthesis approach that leverages the\nefficient text-editing capabilities of LLMs alongside human expert knowledge to\ngenerate diverse and high-quality HQA data at a low cost. Using HAI, we\nconstruct Chart-HQA, a challenging benchmark synthesized from publicly\navailable data sources. Evaluation results on 18 MLLMs of varying model sizes\nreveal that current models face significant generalization challenges and\nexhibit imbalanced reasoning performance on the HQA task.\n","authors":["Xiangnan Chen","Yuancheng Fang","Qian Xiao","Juncheng Li","Jun Lin","Siliang Tang","Yi Yang","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2503.04095v1.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2501.17399v2","updated":"2025-03-06T04:41:56Z","published":"2025-01-29T03:29:24Z","title":"MultiChallenge: A Realistic Multi-Turn Conversation Evaluation Benchmark\n  Challenging to Frontier LLMs","summary":"  We present MultiChallenge, a pioneering benchmark evaluating large language\nmodels (LLMs) on conducting multi-turn conversations with human users, a\ncrucial yet underexamined capability for their applications. MultiChallenge\nidentifies four categories of challenges in multi-turn conversations that are\nnot only common and realistic among current human-LLM interactions, but are\nalso challenging to all current frontier LLMs. All 4 challenges require\naccurate instruction-following, context allocation, and in-context reasoning at\nthe same time. We also develop LLM as judge with instance-level rubrics to\nfacilitate an automatic evaluation method with fair agreement with experienced\nhuman raters. Despite achieving near-perfect scores on existing multi-turn\nevaluation benchmarks, all frontier models have less than 50% accuracy on\nMultiChallenge, with the top-performing Claude 3.5 Sonnet (June 2024) achieving\njust a 41.4% average accuracy.\n","authors":["Ved Sirdeshmukh","Kaustubh Deshpande","Johannes Mols","Lifeng Jin","Ed-Yeremai Cardona","Dean Lee","Jeremy Kritz","Willow Primack","Summer Yue","Chen Xing"],"pdf_url":"https://arxiv.org/pdf/2501.17399v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.15683v3","updated":"2025-03-06T03:59:59Z","published":"2024-05-24T16:21:59Z","title":"Visual Description Grounding Reduces Hallucinations and Boosts Reasoning\n  in LVLMs","summary":"  Large Vision-Language Models (LVLMs) often produce responses that misalign\nwith factual information, a phenomenon known as hallucinations. While\nhallucinations are well-studied, the exact causes behind them remain\nunderexplored. In this paper, we first investigate the root causes of\nhallucinations in LVLMs. Our findings reveal that existing mitigation\ntechniques primarily reduce hallucinations for visual recognition prompts-those\nthat require simple descriptions of visual elements-but fail for cognitive\nprompts that demand deliberate reasoning. We identify the core issue as a lack\nof true visual perception in LVLMs: although they can accurately recognize\nvisual elements, they struggle to fully interpret these elements in the context\nof the input prompt and effectively link this recognition to their internal\nknowledge, which is critical for reasoning. To address this gap, we introduce\nVisual Description Grounded Decoding (VDGD), a simple, robust, and\ntraining-free method designed to enhance visual perception and improve\nreasoning capabilities in LVLMs. VDGD works by first generating a detailed\ndescription of the image and appending it as a prefix to the instruction.\nDuring response generation, tokens are sampled based on their KL divergence to\nthe description, favoring candidates with lower divergence. Experimental\nresults on multiple visual reasoning benchmarks and LVLMs demonstrate that VDGD\nconsistently outperforms existing baselines 2% - 33%. Finally, we introduce\nVaLLu, a benchmark designed for comprehensive evaluation of the cognitive\ncapabilities of LVLMs.\n","authors":["Sreyan Ghosh","Chandra Kiran Reddy Evuru","Sonal Kumar","Utkarsh Tyagi","Oriol Nieto","Zeyu Jin","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2405.15683v3.pdf","comment":"Accepted to ICLR 2025. Project: https://sreyan88.github.io/VDGD/"},{"id":"http://arxiv.org/abs/2503.04065v1","updated":"2025-03-06T03:43:21Z","published":"2025-03-06T03:43:21Z","title":"PP-DocBee: Improving Multimodal Document Understanding Through a Bag of\n  Tricks","summary":"  With the rapid advancement of digitalization, various document images are\nbeing applied more extensively in production and daily life, and there is an\nincreasingly urgent need for fast and accurate parsing of the content in\ndocument images. Therefore, this report presents PP-DocBee, a novel multimodal\nlarge language model designed for end-to-end document image understanding.\nFirst, we develop a data synthesis strategy tailored to document scenarios in\nwhich we build a diverse dataset to improve the model generalization. Then, we\napply a few training techniques, including dynamic proportional sampling, data\npreprocessing, and OCR postprocessing strategies. Extensive evaluations\ndemonstrate the superior performance of PP-DocBee, achieving state-of-the-art\nresults on English document understanding benchmarks and even outperforming\nexisting open source and commercial models in Chinese document understanding.\nThe source code and pre-trained models are publicly available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.\n","authors":["Feng Ni","Kui Huang","Yao Lu","Wenyu Lv","Guanzhong Wang","Zeyu Chen","Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2503.04065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04064v1","updated":"2025-03-06T03:41:47Z","published":"2025-03-06T03:41:47Z","title":"Uncovering inequalities in new knowledge learning by large language\n  models across different languages","summary":"  As large language models (LLMs) gradually become integral tools for problem\nsolving in daily life worldwide, understanding linguistic inequality is\nbecoming increasingly important. Existing research has primarily focused on\nstatic analyses that assess the disparities in the existing knowledge and\ncapabilities of LLMs across languages. However, LLMs are continuously evolving,\nacquiring new knowledge to generate up-to-date, domain-specific responses.\nInvestigating linguistic inequalities within this dynamic process is,\ntherefore, also essential. In this paper, we explore inequalities in new\nknowledge learning by LLMs across different languages and four key dimensions:\neffectiveness, transferability, prioritization, and robustness. Through\nextensive experiments under two settings (in-context learning and fine-tuning)\nusing both proprietary and open-source models, we demonstrate that low-resource\nlanguages consistently face disadvantages across all four dimensions. By\nshedding light on these disparities, we aim to raise awareness of linguistic\ninequalities in LLMs' new knowledge learning, fostering the development of more\ninclusive and equitable future LLMs.\n","authors":["Chenglong Wang","Haoyu Tang","Xiyuan Yang","Yueqi Xie","Jina Suh","Sunayana Sitaram","Junming Huang","Yu Xie","Zhaoya Gong","Xing Xie","Fangzhao Wu"],"pdf_url":"https://arxiv.org/pdf/2503.04064v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03459v2","updated":"2025-03-06T03:32:45Z","published":"2025-03-05T12:49:44Z","title":"Unified Mind Model: Reimagining Autonomous Agents in the LLM Era","summary":"  Large language models (LLMs) have recently demonstrated remarkable\ncapabilities across domains, tasks, and languages (e.g., ChatGPT and GPT-4),\nreviving the research of general autonomous agents with human-like cognitive\nabilities. Such human-level agents require semantic comprehension and\ninstruction-following capabilities, which exactly fall into the strengths of\nLLMs. Although there have been several initial attempts to build human-level\nagents based on LLMs, the theoretical foundation remains a challenging open\nproblem. In this paper, we propose a novel theoretical cognitive architecture,\nthe Unified Mind Model (UMM), which offers guidance to facilitate the rapid\ncreation of autonomous agents with human-level cognitive abilities.\nSpecifically, our UMM starts with the global workspace theory and further\nleverage LLMs to enable the agent with various cognitive abilities, such as\nmulti-modal perception, planning, reasoning, tool use, learning, memory,\nreflection and motivation. Building upon UMM, we then develop an agent-building\nengine, MindOS, which allows users to quickly create domain-/task-specific\nautonomous agents without any programming effort.\n","authors":["Pengbo Hu","Xiang Ying"],"pdf_url":"https://arxiv.org/pdf/2503.03459v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2503.02365v2","updated":"2025-03-06T03:29:31Z","published":"2025-03-04T07:45:45Z","title":"EchoQA: A Large Collection of Instruction Tuning Data for Echocardiogram\n  Reports","summary":"  We introduce a novel question-answering (QA) dataset using echocardiogram\nreports sourced from the Medical Information Mart for Intensive Care database.\nThis dataset is specifically designed to enhance QA systems in cardiology,\nconsisting of 771,244 QA pairs addressing a wide array of cardiac abnormalities\nand their severity. We compare large language models (LLMs), including\nopen-source and biomedical-specific models for zero-shot evaluation, and\nclosed-source models for zero-shot and three-shot evaluation. Our results show\nthat fine-tuning LLMs improves performance across various QA metrics,\nvalidating the value of our dataset. Clinicians also qualitatively evaluate the\nbest-performing model to assess the LLM responses for correctness. Further, we\nconduct fine-grained fairness audits to assess the bias-performance trade-off\nof LLMs across various social determinants of health. Our objective is to\npropel the field forward by establishing a benchmark for LLM AI agents aimed at\nsupporting clinicians with cardiac differential diagnoses, thereby reducing the\ndocumentation burden that contributes to clinician burnout and enabling\nhealthcare professionals to focus more on patient care.\n","authors":["Lama Moukheiber","Mira Moukheiber","Dana Moukheiiber","Jae-Woo Ju","Hyung-Chul Lee"],"pdf_url":"https://arxiv.org/pdf/2503.02365v2.pdf","comment":"NeurIPS SafeGenAI 2024"},{"id":"http://arxiv.org/abs/2401.08392v4","updated":"2025-03-06T03:27:02Z","published":"2024-01-16T14:33:09Z","title":"DoraemonGPT: Toward Understanding Dynamic Scenes with Large Language\n  Models (Exemplified as A Video Agent)","summary":"  Recent LLM-driven visual agents mainly focus on solving image-based tasks,\nwhich limits their ability to understand dynamic scenes, making it far from\nreal-life applications like guiding students in laboratory experiments and\nidentifying their mistakes. Hence, this paper explores DoraemonGPT, a\ncomprehensive and conceptually elegant system driven by LLMs to understand\ndynamic scenes. Considering the video modality better reflects the\never-changing nature of real-world scenarios, we exemplify DoraemonGPT as a\nvideo agent. Given a video with a question/task, DoraemonGPT begins by\nconverting the input video into a symbolic memory that stores task-related\nattributes. This structured representation allows for spatial-temporal querying\nand reasoning by well-designed sub-task tools, resulting in concise\nintermediate results. Recognizing that LLMs have limited internal knowledge\nwhen it comes to specialized domains (e.g., analyzing the scientific principles\nunderlying experiments), we incorporate plug-and-play tools to assess external\nknowledge and address tasks across different domains. Moreover, a novel\nLLM-driven planner based on Monte Carlo Tree Search is introduced to explore\nthe large planning space for scheduling various tools. The planner iteratively\nfinds feasible solutions by backpropagating the result's reward, and multiple\nsolutions can be summarized into an improved final answer. We extensively\nevaluate DoraemonGPT's effectiveness on three benchmarks and several\nin-the-wild scenarios. The code will be released at\nhttps://github.com/z-x-yang/DoraemonGPT.\n","authors":["Zongxin Yang","Guikun Chen","Xiaodi Li","Wenguan Wang","Yi Yang"],"pdf_url":"https://arxiv.org/pdf/2401.08392v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.06916v2","updated":"2025-03-06T03:04:44Z","published":"2024-10-09T14:15:30Z","title":"SWIFT: On-the-Fly Self-Speculative Decoding for LLM Inference\n  Acceleration","summary":"  Speculative decoding (SD) has emerged as a widely used paradigm to accelerate\nLLM inference without compromising quality. It works by first employing a\ncompact model to draft multiple tokens efficiently and then using the target\nLLM to verify them in parallel. While this technique has achieved notable\nspeedups, most existing approaches necessitate either additional parameters or\nextensive training to construct effective draft models, thereby restricting\ntheir applicability across different LLMs and tasks. To address this\nlimitation, we explore a novel plug-and-play SD solution with layer-skipping,\nwhich skips intermediate layers of the target LLM as the compact draft model.\nOur analysis reveals that LLMs exhibit great potential for self-acceleration\nthrough layer sparsity and the task-specific nature of this sparsity. Building\non these insights, we introduce SWIFT, an on-the-fly self-speculative decoding\nalgorithm that adaptively selects intermediate layers of LLMs to skip during\ninference. SWIFT does not require auxiliary models or additional training,\nmaking it a plug-and-play solution for accelerating LLM inference across\ndiverse input data streams. Our extensive experiments across a wide range of\nmodels and downstream tasks demonstrate that SWIFT can achieve over a 1.3x-1.6x\nspeedup while preserving the original distribution of the generated text. We\nrelease our code in https://github.com/hemingkx/SWIFT.\n","authors":["Heming Xia","Yongqi Li","Jun Zhang","Cunxiao Du","Wenjie Li"],"pdf_url":"https://arxiv.org/pdf/2410.06916v2.pdf","comment":"ICLR 2025, camera-ready version"},{"id":"http://arxiv.org/abs/2408.15176v2","updated":"2025-03-06T02:45:08Z","published":"2024-08-27T16:18:51Z","title":"Unifying Multitrack Music Arrangement via Reconstruction Fine-Tuning and\n  Efficient Tokenization","summary":"  Automatic music arrangement streamlines the creation of musical variants for\ncomposers and arrangers, reducing reliance on extensive music expertise.\nHowever, existing methods suffer from inefficient tokenization,\nunderutilization of pre-trained music language models (LMs), and suboptimal\nfidelity and coherence in generated arrangements. This paper introduces an\nefficient multitrack music tokenizer for unconditional and conditional symbolic\nmusic generation, along with a unified sequence-to-sequence reconstruction\nfine-tuning objective for pre-trained music LMs that balances task-specific\nneeds with coherence constraints. Our approach achieves state-of-the-art\nresults on band arrangement, piano reduction, and drum arrangement, surpassing\ntask-specific models in both objective metrics and perceptual quality.\nAdditionally, we demonstrate that generative pretraining significantly\ncontributes to the performance across these arrangement tasks, especially when\nhandling long segments with complex alignment.\n","authors":["Longshen Ou","Jingwei Zhao","Ziyu Wang","Gus Xia","Ye Wang"],"pdf_url":"https://arxiv.org/pdf/2408.15176v2.pdf","comment":"Submitted to IJCAI 2025"},{"id":"http://arxiv.org/abs/2406.10292v3","updated":"2025-03-06T02:41:55Z","published":"2024-06-13T04:23:35Z","title":"Automatically Labeling Clinical Trial Outcomes: A Large-Scale Benchmark\n  for Drug Development","summary":"  Background The cost of drug discovery and development is substantial, with\nclinical trial outcomes playing a critical role in regulatory approval and\npatient care. However, access to large-scale, high-quality clinical trial\noutcome data remains limited, hindering advancements in predictive modeling and\nevidence-based decision-making.\n  Methods We present the Clinical Trial Outcome (CTO) benchmark, a fully\nreproducible, large-scale repository encompassing approximately 125,000 drug\nand biologics trials. CTO integrates large language model (LLM) interpretations\nof publications, trial phase progression tracking, sentiment analysis from news\nsources, stock price movements of trial sponsors, and additional trial-related\nmetrics. Furthermore, we manually annotated a dataset of clinical trials\nconducted between 2020 and 2024 to enhance the quality and reliability of\noutcome labels.\n  Results The trial outcome labels in the CTO benchmark agree strongly with\nexpert annotations, achieving an F1 score of 94 for Phase 3 trials and 91\nacross all phases. Additionally, benchmarking standard machine learning models\non our manually annotated dataset revealed distribution shifts in recent\ntrials, underscoring the necessity of continuously updated labeling approaches.\n  Conclusions By analyzing CTO's performance on recent clinical trials, we\ndemonstrate the ongoing need for high-quality, up-to-date trial outcome labels.\nWe publicly release the CTO knowledge base and annotated labels at\nhttps://chufangao.github.io/CTOD, with regular updates to support research on\nclinical trial outcomes and inform data-driven improvements in drug\ndevelopment.\n","authors":["Chufan Gao","Jathurshan Pradeepkumar","Trisha Das","Shivashankar Thati","Jimeng Sun"],"pdf_url":"https://arxiv.org/pdf/2406.10292v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04036v1","updated":"2025-03-06T02:40:51Z","published":"2025-03-06T02:40:51Z","title":"Robust Data Watermarking in Language Models by Injecting Fictitious\n  Knowledge","summary":"  Data watermarking in language models injects traceable signals, such as\nspecific token sequences or stylistic patterns, into copyrighted text, allowing\ncopyright holders to track and verify training data ownership. Previous data\nwatermarking techniques primarily focus on effective memorization after\npretraining, while overlooking challenges that arise in other stages of the LLM\npipeline, such as the risk of watermark filtering during data preprocessing, or\npotential forgetting through post-training, or verification difficulties due to\nAPI-only access. We propose a novel data watermarking approach that injects\ncoherent and plausible yet fictitious knowledge into training data using\ngenerated passages describing a fictitious entity and its associated\nattributes. Our watermarks are designed to be memorized by the LLM through\nseamlessly integrating in its training data, making them harder to detect\nlexically during preprocessing.We demonstrate that our watermarks can be\neffectively memorized by LLMs, and that increasing our watermarks' density,\nlength, and diversity of attributes strengthens their memorization. We further\nshow that our watermarks remain robust throughout LLM development, maintaining\ntheir effectiveness after continual pretraining and supervised finetuning.\nFinally, we show that our data watermarks can be evaluated even under API-only\naccess via question answering.\n","authors":["Xinyue Cui","Johnny Tian-Zheng Wei","Swabha Swayamdipta","Robin Jia"],"pdf_url":"https://arxiv.org/pdf/2503.04036v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04013v1","updated":"2025-03-06T02:01:59Z","published":"2025-03-06T02:01:59Z","title":"Benchmarking Large Language Models on Multiple Tasks in Bioinformatics\n  NLP with Prompting","summary":"  Large language models (LLMs) have become important tools in solving\nbiological problems, offering improvements in accuracy and adaptability over\nconventional methods. Several benchmarks have been proposed to evaluate the\nperformance of these LLMs. However, current benchmarks can hardly evaluate the\nperformance of these models across diverse tasks effectively. In this paper, we\nintroduce a comprehensive prompting-based benchmarking framework, termed\nBio-benchmark, which includes 30 key bioinformatics tasks covering areas such\nas proteins, RNA, drugs, electronic health records, and traditional Chinese\nmedicine. Using this benchmark, we evaluate six mainstream LLMs, including\nGPT-4o and Llama-3.1-70b, etc., using 0-shot and few-shot Chain-of-Thought\n(CoT) settings without fine-tuning to reveal their intrinsic capabilities. To\nimprove the efficiency of our evaluations, we demonstrate BioFinder, a new tool\nfor extracting answers from LLM responses, which increases extraction accuracy\nby round 30% compared to existing methods. Our benchmark results show the\nbiological tasks suitable for current LLMs and identify specific areas\nrequiring enhancement. Furthermore, we propose targeted prompt engineering\nstrategies for optimizing LLM performance in these contexts. Based on these\nfindings, we provide recommendations for the development of more robust LLMs\ntailored for various biological applications. This work offers a comprehensive\nevaluation framework and robust tools to support the application of LLMs in\nbioinformatics.\n","authors":["Jiyue Jiang","Pengan Chen","Jiuming Wang","Dongchen He","Ziqin Wei","Liang Hong","Licheng Zong","Sheng Wang","Qinze Yu","Zixian Ma","Yanyu Chen","Yimin Fan","Xiangyu Shi","Jiawei Sun","Chuan Wu","Yu Li"],"pdf_url":"https://arxiv.org/pdf/2503.04013v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20490v2","updated":"2025-03-06T00:59:40Z","published":"2025-02-27T19:54:16Z","title":"EgoNormia: Benchmarking Physical Social Norm Understanding","summary":"  Human activity is moderated by norms. However, machines are often trained\nwithout explicit supervision on norm understanding and reasoning, especially\nwhen the norms are grounded in a physical and social context. To improve and\nevaluate the normative reasoning capability of vision-language models (VLMs),\nwe present EgoNormia $\\|\\epsilon\\|$, consisting of 1,853 ego-centric videos of\nhuman interactions, each of which has two related questions evaluating both the\nprediction and justification of normative actions. The normative actions\nencompass seven categories: safety, privacy, proxemics, politeness,\ncooperation, coordination/proactivity, and communication/legibility. To compile\nthis dataset at scale, we propose a novel pipeline leveraging video sampling,\nautomatic answer generation, filtering, and human validation. Our work\ndemonstrates that current state-of-the-art vision-language models lack robust\nnorm understanding, scoring a maximum of 45% on EgoNormia (versus a human bench\nof 92%). Our analysis of performance in each dimension highlights the\nsignificant risks of safety, privacy, and the lack of collaboration and\ncommunication capability when applied to real-world agents. We additionally\nshow that through a retrieval-based generation method, it is possible to use\nEgoNormia to enhance normative reasoning in VLMs.\n","authors":["MohammadHossein Rezaei","Yicheng Fu","Phil Cuvin","Caleb Ziems","Yanzhe Zhang","Hao Zhu","Diyi Yang"],"pdf_url":"https://arxiv.org/pdf/2502.20490v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02163v2","updated":"2025-03-06T00:58:55Z","published":"2024-10-03T03:06:42Z","title":"Adversarial Decoding: Generating Readable Documents for Adversarial\n  Objectives","summary":"  We design, implement, and evaluate adversarial decoding, a new, generic text\ngeneration technique that produces readable documents for different adversarial\nobjectives. Prior methods either produce easily detectable gibberish, or cannot\nhandle objectives that include embedding similarity. In particular, they only\nwork for direct attacks (such as jailbreaking) and cannot produce adversarial\ntext for realistic indirect injection, e.g., documents that (1) are retrieved\nin RAG systems in response to broad classes of queries, and also (2)\nadversarially influence subsequent generation. We also show that fluency (low\nperplexity) is not sufficient to evade filtering. We measure the effectiveness\nof adversarial decoding for different objectives, including RAG poisoning,\njailbreaking, and evasion of defensive filters, and demonstrate that it\noutperforms existing methods while producing readable adversarial documents.\n","authors":["Collin Zhang","Tingwei Zhang","Vitaly Shmatikov"],"pdf_url":"https://arxiv.org/pdf/2410.02163v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17651v3","updated":"2025-03-06T00:45:00Z","published":"2025-02-24T21:01:39Z","title":"METAL: A Multi-Agent Framework for Chart Generation with Test-Time\n  Scaling","summary":"  Chart generation aims to generate code to produce charts satisfying the\ndesired visual properties, e.g., texts, layout, color, and type. It has great\npotential to empower the automatic professional report generation in financial\nanalysis, research presentation, education, and healthcare. In this work, we\nbuild a vision-language model (VLM) based multi-agent framework for effective\nautomatic chart generation. Generating high-quality charts requires both strong\nvisual design skills and precise coding capabilities that embed the desired\nvisual properties into code. Such a complex multi-modal reasoning process is\ndifficult for direct prompting of VLMs. To resolve these challenges, we propose\nMETAL, a multi-agent framework that decomposes the task of chart generation\ninto the iterative collaboration among specialized agents. METAL achieves 5.2%\nimprovement over the current best result in the chart generation task. The\nMETAL framework exhibits the phenomenon of test-time scaling: its performance\nincreases monotonically as the logarithmic computational budget grows from 512\nto 8192 tokens. In addition, we find that separating different modalities\nduring the critique process of METAL boosts the self-correction capability of\nVLMs in the multimodal context.\n","authors":["Bingxuan Li","Yiwei Wang","Jiuxiang Gu","Kai-Wei Chang","Nanyun Peng"],"pdf_url":"https://arxiv.org/pdf/2502.17651v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16457v2","updated":"2025-03-06T00:40:18Z","published":"2025-02-23T06:16:23Z","title":"Towards Fully-Automated Materials Discovery via Large-Scale Synthesis\n  Dataset and Expert-Level LLM-as-a-Judge","summary":"  Materials synthesis is vital for innovations such as energy storage,\ncatalysis, electronics, and biomedical devices. Yet, the process relies heavily\non empirical, trial-and-error methods guided by expert intuition. Our work aims\nto support the materials science community by providing a practical,\ndata-driven resource. We have curated a comprehensive dataset of 17K\nexpert-verified synthesis recipes from open-access literature, which forms the\nbasis of our newly developed benchmark, AlchemyBench. AlchemyBench offers an\nend-to-end framework that supports research in large language models applied to\nsynthesis prediction. It encompasses key tasks, including raw materials and\nequipment prediction, synthesis procedure generation, and characterization\noutcome forecasting. We propose an LLM-as-a-Judge framework that leverages\nlarge language models for automated evaluation, demonstrating strong\nstatistical agreement with expert assessments. Overall, our contributions offer\na supportive foundation for exploring the capabilities of LLMs in predicting\nand guiding materials synthesis, ultimately paving the way for more efficient\nexperimental design and accelerated innovation in materials science.\n","authors":["Heegyu Kim","Taeyang Jeon","Seungtaek Choi","Ji Hoon Hong","Dong Won Jeon","Sung Beom Cho","Ga-Yeon Baek","Kyung-Won Kwak","Dong-Hee Lee","Sun-Jin Choi","Jisu Bae","Chihoon Lee","Yunseo Kim","Jinsung Park","Hyunsouk Cho"],"pdf_url":"https://arxiv.org/pdf/2502.16457v2.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2503.03987v1","updated":"2025-03-06T00:19:54Z","published":"2025-03-06T00:19:54Z","title":"RetinalGPT: A Retinal Clinical Preference Conversational Assistant\n  Powered by Large Vision-Language Models","summary":"  Recently, Multimodal Large Language Models (MLLMs) have gained significant\nattention for their remarkable ability to process and analyze non-textual data,\nsuch as images, videos, and audio. Notably, several adaptations of\ngeneral-domain MLLMs to the medical field have been explored, including\nLLaVA-Med. However, these medical adaptations remain insufficiently advanced in\nunderstanding and interpreting retinal images. In contrast, medical experts\nemphasize the importance of quantitative analyses for disease detection and\ninterpretation. This underscores a gap between general-domain and\nmedical-domain MLLMs: while general-domain MLLMs excel in broad applications,\nthey lack the specialized knowledge necessary for precise diagnostic and\ninterpretative tasks in the medical field. To address these challenges, we\nintroduce \\textit{RetinalGPT}, a multimodal conversational assistant for\nclinically preferred quantitative analysis of retinal images. Specifically, we\nachieve this by compiling a large retinal image dataset, developing a novel\ndata pipeline, and employing customized visual instruction tuning to enhance\nboth retinal analysis and enrich medical knowledge. In particular, RetinalGPT\noutperforms MLLM in the generic domain by a large margin in the diagnosis of\nretinal diseases in 8 benchmark retinal datasets. Beyond disease diagnosis,\nRetinalGPT features quantitative analyses and lesion localization, representing\na pioneering step in leveraging LLMs for an interpretable and end-to-end\nclinical research framework. The code is available at\nhttps://github.com/Retinal-Research/RetinalGPT\n","authors":["Wenhui Zhu","Xin Li","Xiwen Chen","Peijie Qiu","Vamsi Krishna Vasa","Xuanzhao Dong","Yanxi Chen","Natasha Lepore","Oana Dumitrascu","Yi Su","Yalin Wang"],"pdf_url":"https://arxiv.org/pdf/2503.03987v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03983v1","updated":"2025-03-06T00:10:26Z","published":"2025-03-06T00:10:26Z","title":"Audio Flamingo 2: An Audio-Language Model with Long-Audio Understanding\n  and Expert Reasoning Abilities","summary":"  Understanding and reasoning over non-speech sounds and music are crucial for\nboth humans and AI agents to interact effectively with their environments. In\nthis paper, we introduce Audio Flamingo 2 (AF2), an Audio-Language Model (ALM)\nwith advanced audio understanding and reasoning capabilities. AF2 leverages (i)\na custom CLAP model, (ii) synthetic Audio QA data for fine-grained audio\nreasoning, and (iii) a multi-stage curriculum learning strategy. AF2 achieves\nstate-of-the-art performance with only a 3B parameter small language model,\nsurpassing large open-source and proprietary models across over 20 benchmarks.\nNext, for the first time, we extend audio understanding to long audio segments\n(30 secs to 5 mins) and propose LongAudio, a large and novel dataset for\ntraining ALMs on long audio captioning and question-answering tasks.\nFine-tuning AF2 on LongAudio leads to exceptional performance on our proposed\nLongAudioBench, an expert annotated benchmark for evaluating ALMs on long audio\nunderstanding capabilities. We conduct extensive ablation studies to confirm\nthe efficacy of our approach. Project Website:\nhttps://research.nvidia.com/labs/adlr/AF2/.\n","authors":["Sreyan Ghosh","Zhifeng Kong","Sonal Kumar","S Sakshi","Jaehyeon Kim","Wei Ping","Rafael Valle","Dinesh Manocha","Bryan Catanzaro"],"pdf_url":"https://arxiv.org/pdf/2503.03983v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03979v1","updated":"2025-03-06T00:03:55Z","published":"2025-03-06T00:03:55Z","title":"ReasonGraph: Visualisation of Reasoning Paths","summary":"  Large Language Models (LLMs) reasoning processes are challenging to analyze\ndue to their complexity and the lack of organized visualization tools. We\npresent ReasonGraph, a web-based platform for visualizing and analyzing LLM\nreasoning processes. It supports both sequential and tree-based reasoning\nmethods while integrating with major LLM providers and over fifty\nstate-of-the-art models. ReasonGraph incorporates an intuitive UI with meta\nreasoning method selection, configurable visualization parameters, and a\nmodular framework that facilitates efficient extension. Our evaluation shows\nhigh parsing reliability, efficient processing, and strong usability across\nvarious downstream applications. By providing a unified visualization\nframework, ReasonGraph reduces cognitive load in analyzing complex reasoning\npaths, improves error detection in logical processes, and enables more\neffective development of LLM-based applications. The platform is open-source,\npromoting accessibility and reproducibility in LLM reasoning analysis.\n","authors":["Zongqian Li","Ehsan Shareghi","Nigel Collier"],"pdf_url":"https://arxiv.org/pdf/2503.03979v1.pdf","comment":null}],"Computer Vision and Pattern Recognition":[{"id":"http://arxiv.org/abs/2503.04720v1","updated":"2025-03-06T18:59:06Z","published":"2025-03-06T18:59:06Z","title":"FluidNexus: 3D Fluid Reconstruction and Prediction from a Single Video","summary":"  We study reconstructing and predicting 3D fluid appearance and velocity from\na single video. Current methods require multi-view videos for fluid\nreconstruction. We present FluidNexus, a novel framework that bridges video\ngeneration and physics simulation to tackle this task. Our key insight is to\nsynthesize multiple novel-view videos as references for reconstruction.\nFluidNexus consists of two key components: (1) a novel-view video synthesizer\nthat combines frame-wise view synthesis with video diffusion refinement for\ngenerating realistic videos, and (2) a physics-integrated particle\nrepresentation coupling differentiable simulation and rendering to\nsimultaneously facilitate 3D fluid reconstruction and prediction. To evaluate\nour approach, we collect two new real-world fluid datasets featuring textured\nbackgrounds and object interactions. Our method enables dynamic novel view\nsynthesis, future prediction, and interaction simulation from a single fluid\nvideo. Project website: https://yuegao.me/FluidNexus.\n","authors":["Yue Gao","Hong-Xing Yu","Bo Zhu","Jiajun Wu"],"pdf_url":"https://arxiv.org/pdf/2503.04720v1.pdf","comment":"CVPR 2025. Project website: https://yuegao.me/FluidNexus"},{"id":"http://arxiv.org/abs/2503.04718v1","updated":"2025-03-06T18:58:45Z","published":"2025-03-06T18:58:45Z","title":"Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation","summary":"  Scene flow estimation is a foundational task for many robotic applications,\nincluding robust dynamic object detection, automatic labeling, and sensor\nsynchronization. Two types of approaches to the problem have evolved: 1)\nSupervised and 2) optimization-based methods. Supervised methods are fast\nduring inference and achieve high-quality results, however, they are limited by\nthe need for large amounts of labeled training data and are susceptible to\ndomain gaps. In contrast, unsupervised test-time optimization methods do not\nface the problem of domain gaps but usually suffer from substantial runtime,\nexhibit artifacts, or fail to converge to the right solution. In this work, we\nmitigate several limitations of existing optimization-based methods. To this\nend, we 1) introduce a simple voxel grid-based model that improves over the\nstandard MLP-based formulation in multiple dimensions and 2) introduce a new\nmultiframe loss formulation. 3) We combine both contributions in our new\nmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only\nby EulerFlow among unsupervised methods while achieving comparable performance\nat a fraction of the computational cost. Floxels achieves a massive speedup of\nmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10\nminutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels\nachieves a speedup of ~14x.\n","authors":["David T. Hoffmann","Syed Haseeb Raza","Hanqiu Jiang","Denis Tananaev","Steffen Klingenhoefer","Martin Meinke"],"pdf_url":"https://arxiv.org/pdf/2503.04718v1.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2503.04707v1","updated":"2025-03-06T18:55:21Z","published":"2025-03-06T18:55:21Z","title":"Iris Style Transfer: Enhancing Iris Recognition with Style Features and\n  Privacy Preservation through Neural Style Transfer","summary":"  Iris texture is widely regarded as a gold standard biometric modality for\nauthentication and identification. The demand for robust iris recognition\nmethods, coupled with growing security and privacy concerns regarding iris\nattacks, has escalated recently. Inspired by neural style transfer, an advanced\ntechnique that leverages neural networks to separate content and style\nfeatures, we hypothesize that iris texture's style features provide a reliable\nfoundation for recognition and are more resilient to variations like rotation\nand perspective shifts than traditional approaches. Our experimental results\nsupport this hypothesis, showing a significantly higher classification accuracy\ncompared to conventional features. Further, we propose using neural style\ntransfer to mask identifiable iris style features, ensuring the protection of\nsensitive biometric information while maintaining the utility of eye images for\ntasks like eye segmentation and gaze estimation. This work opens new avenues\nfor iris-oriented, secure, and privacy-aware biometric systems.\n","authors":["Mengdi Wang","Efe Bozkir","Enkelejda Kasneci"],"pdf_url":"https://arxiv.org/pdf/2503.04707v1.pdf","comment":"14 pages main paper, 4 pages appendix"},{"id":"http://arxiv.org/abs/2503.04698v1","updated":"2025-03-06T18:46:10Z","published":"2025-03-06T18:46:10Z","title":"DEAL-YOLO: Drone-based Efficient Animal Localization using YOLO","summary":"  Although advances in deep learning and aerial surveillance technology are\nimproving wildlife conservation efforts, complex and erratic environmental\nconditions still pose a problem, requiring innovative solutions for\ncost-effective small animal detection. This work introduces DEAL-YOLO, a novel\napproach that improves small object detection in Unmanned Aerial Vehicle (UAV)\nimages by using multi-objective loss functions like Wise IoU (WIoU) and\nNormalized Wasserstein Distance (NWD), which prioritize pixels near the centre\nof the bounding box, ensuring smoother localization and reducing abrupt\ndeviations. Additionally, the model is optimized through efficient feature\nextraction with Linear Deformable (LD) convolutions, enhancing accuracy while\nmaintaining computational efficiency. The Scaled Sequence Feature Fusion (SSFF)\nmodule enhances object detection by effectively capturing inter-scale\nrelationships, improving feature representation, and boosting metrics through\noptimized multiscale fusion. Comparison with baseline models reveals high\nefficacy with up to 69.5\\% fewer parameters compared to vanilla Yolov8-N,\nhighlighting the robustness of the proposed modifications. Through this\napproach, our paper aims to facilitate the detection of endangered species,\nanimal population analysis, habitat monitoring, biodiversity research, and\nvarious other applications that enrich wildlife conservation efforts. DEAL-YOLO\nemploys a two-stage inference paradigm for object detection, refining selected\nregions to improve localization and confidence. This approach enhances\nperformance, especially for small instances with low objectness scores.\n","authors":["Aditya Prashant Naidu","Hem Gosalia","Ishaan Gakhar","Shaurya Singh Rathore","Krish Didwania","Ujjwal Verma"],"pdf_url":"https://arxiv.org/pdf/2503.04698v1.pdf","comment":"Accepted as a Poster at the ML4RS Workshop at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04688v1","updated":"2025-03-06T18:31:41Z","published":"2025-03-06T18:31:41Z","title":"Teach YOLO to Remember: A Self-Distillation Approach for Continual\n  Object Detection","summary":"  Real-time object detectors like YOLO achieve exceptional performance when\ntrained on large datasets for multiple epochs. However, in real-world scenarios\nwhere data arrives incrementally, neural networks suffer from catastrophic\nforgetting, leading to a loss of previously learned knowledge. To address this,\nprior research has explored strategies for Class Incremental Learning (CIL) in\nContinual Learning for Object Detection (CLOD), with most approaches focusing\non two-stage object detectors. However, existing work suggests that Learning\nwithout Forgetting (LwF) may be ineffective for one-stage anchor-free detectors\nlike YOLO due to noisy regression outputs, which risk transferring corrupted\nknowledge. In this work, we introduce YOLO LwF, a self-distillation approach\ntailored for YOLO-based continual object detection. We demonstrate that when\ncoupled with a replay memory, YOLO LwF significantly mitigates forgetting.\nCompared to previous approaches, it achieves state-of-the-art performance,\nimproving mAP by +2.1% and +2.9% on the VOC and COCO benchmarks, respectively.\n","authors":["Riccardo De Monte","Davide Dalle Pezze","Gian Antonio Susto"],"pdf_url":"https://arxiv.org/pdf/2503.04688v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.12360v2","updated":"2025-03-06T18:07:00Z","published":"2025-02-17T22:50:45Z","title":"Detecting Systematic Weaknesses in Vision Models along Predefined\n  Human-Understandable Dimensions","summary":"  Slice discovery methods (SDMs) are prominent algorithms for finding\nsystematic weaknesses in DNNs. They identify top-k semantically coherent\nslices/subsets of data where a DNN-under-test has low performance. For being\ndirectly useful, slices should be aligned with human-understandable and\nrelevant dimensions, which, for example, are defined by safety and domain\nexperts as part of the operational design domain (ODD). While SDMs can be\napplied effectively on structured data, their application on image data is\ncomplicated by the lack of semantic metadata. To address these issues, we\npresent an algorithm that combines foundation models for zero-shot image\nclassification to generate semantic metadata with methods for combinatorial\nsearch to find systematic weaknesses in images. In contrast to existing\napproaches, ours identifies weak slices that are in line with pre-defined\nhuman-understandable dimensions. As the algorithm includes foundation models,\nits intermediate and final results may not always be exact. Therefore, we\ninclude an approach to address the impact of noisy metadata. We validate our\nalgorithm on both synthetic and real-world datasets, demonstrating its ability\nto recover human-understandable systematic weaknesses. Furthermore, using our\napproach, we identify systematic weaknesses of multiple pre-trained and\npublicly available state-of-the-art computer vision DNNs.\n","authors":["Sujan Sai Gannamaneni","Rohil Prakash Rao","Michael Mock","Maram Akila","Stefan Wrobel"],"pdf_url":"https://arxiv.org/pdf/2502.12360v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04666v1","updated":"2025-03-06T17:59:29Z","published":"2025-03-06T17:59:29Z","title":"What Are You Doing? A Closer Look at Controllable Human Video Generation","summary":"  High-quality benchmarks are crucial for driving progress in machine learning\nresearch. However, despite the growing interest in video generation, there is\nno comprehensive dataset to evaluate human generation. Humans can perform a\nwide variety of actions and interactions, but existing datasets, like TikTok\nand TED-Talks, lack the diversity and complexity to fully capture the\ncapabilities of video generation models. We close this gap by introducing `What\nAre You Doing?' (WYD): a new benchmark for fine-grained evaluation of\ncontrollable image-to-video generation of humans. WYD consists of 1{,}544\ncaptioned videos that have been meticulously collected and annotated with 56\nfine-grained categories. These allow us to systematically measure performance\nacross 9 aspects of human generation, including actions, interactions and\nmotion. We also propose and validate automatic metrics that leverage our\nannotations and better capture human evaluations. Equipped with our dataset and\nmetrics, we perform in-depth analyses of seven state-of-the-art models in\ncontrollable image-to-video generation, showing how WYD provides novel insights\nabout the capabilities of these models. We release our data and code to drive\nforward progress in human video generation modeling at\nhttps://github.com/google-deepmind/wyd-benchmark.\n","authors":["Emanuele Bugliarello","Anurag Arnab","Roni Paiss","Pieter-Jan Kindermans","Cordelia Schmid"],"pdf_url":"https://arxiv.org/pdf/2503.04666v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04665v1","updated":"2025-03-06T17:58:55Z","published":"2025-03-06T17:58:55Z","title":"Implicit Neural Representation for Video and Image Super-Resolution","summary":"  We present a novel approach for super-resolution that utilizes implicit\nneural representation (INR) to effectively reconstruct and enhance\nlow-resolution videos and images. By leveraging the capacity of neural networks\nto implicitly encode spatial and temporal features, our method facilitates\nhigh-resolution reconstruction using only low-resolution inputs and a 3D\nhigh-resolution grid. This results in an efficient solution for both image and\nvideo super-resolution. Our proposed method, SR-INR, maintains consistent\ndetails across frames and images, achieving impressive temporal stability\nwithout relying on the computationally intensive optical flow or motion\nestimation typically used in other video super-resolution techniques. The\nsimplicity of our approach contrasts with the complexity of many existing\nmethods, making it both effective and efficient. Experimental evaluations show\nthat SR-INR delivers results on par with or superior to state-of-the-art\nsuper-resolution methods, while maintaining a more straightforward structure\nand reduced computational demands. These findings highlight the potential of\nimplicit neural representations as a powerful tool for reconstructing\nhigh-quality, temporally consistent video and image signals from low-resolution\ndata.\n","authors":["Mary Aiyetigbo","Wanqi Yuan","Feng Luo","Nianyi Li"],"pdf_url":"https://arxiv.org/pdf/2503.04665v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09696v2","updated":"2025-03-06T17:45:33Z","published":"2025-02-13T18:59:11Z","title":"ZeroBench: An Impossible Visual Benchmark for Contemporary Large\n  Multimodal Models","summary":"  Large Multimodal Models (LMMs) exhibit major shortfalls when interpreting\nimages and, by some measures, have poorer spatial cognition than small children\nor animals. Despite this, they attain high scores on many popular visual\nbenchmarks, with headroom rapidly eroded by an ongoing surge of model progress.\nTo address this, there is a pressing need for difficult benchmarks that remain\nrelevant for longer. We take this idea to its limit by introducing ZeroBench-a\nlightweight visual reasoning benchmark that is entirely impossible for\ncontemporary frontier LMMs. Our benchmark consists of 100 manually curated\nquestions and 334 less difficult subquestions. We evaluate 20 LMMs on\nZeroBench, all of which score 0.0%, and rigorously analyse the errors. To\nencourage progress in visual understanding, we publicly release ZeroBench.\n","authors":["Jonathan Roberts","Mohammad Reza Taesiri","Ansh Sharma","Akash Gupta","Samuel Roberts","Ioana Croitoru","Simion-Vlad Bogolin","Jialu Tang","Florian Langer","Vyas Raina","Vatsal Raina","Hanyi Xiong","Vishaal Udandarao","Jingyi Lu","Shiyang Chen","Sam Purkis","Tianshuo Yan","Wenye Lin","Gyungin Shin","Qiaochu Yang","Anh Totti Nguyen","David I. Atkinson","Aaditya Baranwal","Alexandru Coca","Mikah Dang","Sebastian Dziadzio","Jakob D. Kunz","Kaiqu Liang","Alexander Lo","Brian Pulfer","Steven Walton","Charig Yang","Kai Han","Samuel Albanie"],"pdf_url":"https://arxiv.org/pdf/2502.09696v2.pdf","comment":"20 pages, 13 figures"},{"id":"http://arxiv.org/abs/2503.04653v1","updated":"2025-03-06T17:43:03Z","published":"2025-03-06T17:43:03Z","title":"RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval\n  via Radiology Report Mining","summary":"  Developing advanced medical imaging retrieval systems is challenging due to\nthe varying definitions of `similar images' across different medical contexts.\nThis challenge is compounded by the lack of large-scale, high-quality medical\nimaging retrieval datasets and benchmarks. In this paper, we propose a novel\nmethodology that leverages dense radiology reports to define image-wise\nsimilarity ordering at multiple granularities in a scalable and fully automatic\nmanner. Using this approach, we construct two comprehensive medical imaging\nretrieval datasets: MIMIC-IR for Chest X-rays and CTRATE-IR for CT scans,\nproviding detailed image-image ranking annotations conditioned on diverse\nanatomical structures. Furthermore, we develop two retrieval systems, RadIR-CXR\nand model-ChestCT, which demonstrate superior performance in traditional\nimage-image and image-report retrieval tasks. These systems also enable\nflexible, effective image retrieval conditioned on specific anatomical\nstructures described in text, achieving state-of-the-art results on 77 out of\n78 metrics.\n","authors":["Tengfei Zhang","Ziheng Zhao","Chaoyi Wu","Xiao Zhou","Ya Zhang","Yangfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2503.04653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04649v1","updated":"2025-03-06T17:35:37Z","published":"2025-03-06T17:35:37Z","title":"Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators","summary":"  We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We also\nrelease a package of the codes and weights for using our pre-trained GNPs for\nprocessing point cloud representations. This allows for incorporating our\npre-trained GNPs as components for reuse within existing and new data\nprocessing pipelines. The GNPs also can be used as part of numerical solvers\ninvolving geometry or as part of methods for performing inference and other\ngeometric tasks.\n","authors":["Blaine Quackenbush","Paul J. Atzberger"],"pdf_url":"https://arxiv.org/pdf/2503.04649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04873v2","updated":"2025-03-06T17:35:19Z","published":"2025-01-08T23:07:10Z","title":"Back Home: A Machine Learning Approach to Seashell Classification and\n  Ecosystem Restoration","summary":"  In Costa Rica, an average of 5 tons of seashells are extracted from\necosystems annually. Confiscated seashells, cannot be returned to their\necosystems due to the lack of origin recognition. To address this issue, we\ndeveloped a convolutional neural network (CNN) specifically for seashell\nidentification. We built a dataset from scratch, consisting of approximately\n19000 images from the Pacific and Caribbean coasts. Using this dataset, the\nmodel achieved a classification accuracy exceeding 85%. The model has been\nintegrated into a user-friendly application, which has classified over 36,000\nseashells to date, delivering real-time results within 3 seconds per image. To\nfurther enhance the system's accuracy, an anomaly detection mechanism was\nincorporated to filter out irrelevant or anomalous inputs, ensuring only valid\nseashell images are processed.\n","authors":["Alexander Valverde","Luis Solano"],"pdf_url":"https://arxiv.org/pdf/2501.04873v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04643v1","updated":"2025-03-06T17:32:15Z","published":"2025-03-06T17:32:15Z","title":"Adaptive Prototype Learning for Multimodal Cancer Survival Analysis","summary":"  Leveraging multimodal data, particularly the integration of whole-slide\nhistology images (WSIs) and transcriptomic profiles, holds great promise for\nimproving cancer survival prediction. However, excessive redundancy in\nmultimodal data can degrade model performance. In this paper, we propose\nAdaptive Prototype Learning (APL), a novel and effective approach for\nmultimodal cancer survival analysis. APL adaptively learns representative\nprototypes in a data-driven manner, reducing redundancy while preserving\ncritical information. Our method employs two sets of learnable query vectors\nthat serve as a bridge between high-dimensional representations and survival\nprediction, capturing task-relevant features. Additionally, we introduce a\nmultimodal mixed self-attention mechanism to enable cross-modal interactions,\nfurther enhancing information fusion. Extensive experiments on five benchmark\ncancer datasets demonstrate the superiority of our approach over existing\nmethods. The code is available at https://github.com/HongLiuuuuu/APL.\n","authors":["Hong Liu","Haosen Yang","Federica Eduati","Josien P. W. Pluim","Mitko Veta"],"pdf_url":"https://arxiv.org/pdf/2503.04643v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.04641v1","updated":"2025-03-06T17:31:43Z","published":"2025-03-06T17:31:43Z","title":"Simulating the Real World: A Unified Survey of Multimodal Generative\n  Models","summary":"  Understanding and replicating the real world is a critical challenge in\nArtificial General Intelligence (AGI) research. To achieve this, many existing\napproaches, such as world models, aim to capture the fundamental principles\ngoverning the physical world, enabling more accurate simulations and meaningful\ninteractions. However, current methods often treat different modalities,\nincluding 2D (images), videos, 3D, and 4D representations, as independent\ndomains, overlooking their interdependencies. Additionally, these methods\ntypically focus on isolated dimensions of reality without systematically\nintegrating their connections. In this survey, we present a unified survey for\nmultimodal generative models that investigate the progression of data\ndimensionality in real-world simulation. Specifically, this survey starts from\n2D generation (appearance), then moves to video (appearance+dynamics) and 3D\ngeneration (appearance+geometry), and finally culminates in 4D generation that\nintegrate all dimensions. To the best of our knowledge, this is the first\nattempt to systematically unify the study of 2D, video, 3D and 4D generation\nwithin a single framework. To guide future research, we provide a comprehensive\nreview of datasets, evaluation metrics and future directions, and fostering\ninsights for newcomers. This survey serves as a bridge to advance the study of\nmultimodal generative models and real-world simulation within a unified\nframework.\n","authors":["Yuqi Hu","Longguang Wang","Xian Liu","Ling-Hao Chen","Yuwei Guo","Yukai Shi","Ce Liu","Anyi Rao","Zeyu Wang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.04641v1.pdf","comment":"Repository for the related papers at\n  https://github.com/ALEEEHU/World-Simulator"},{"id":"http://arxiv.org/abs/2503.04639v1","updated":"2025-03-06T17:28:48Z","published":"2025-03-06T17:28:48Z","title":"Enhancing SAM with Efficient Prompting and Preference Optimization for\n  Semi-supervised Medical Image Segmentation","summary":"  Foundational models such as the Segment Anything Model (SAM) are gaining\ntraction in medical imaging segmentation, supporting multiple downstream tasks.\nHowever, such models are supervised in nature, still relying on large annotated\ndatasets or prompts supplied by experts. Conventional techniques such as active\nlearning to alleviate such limitations are limited in scope and still\nnecessitate continuous human involvement and complex domain knowledge for label\nrefinement or establishing reward ground truth. To address these challenges, we\npropose an enhanced Segment Anything Model (SAM) framework that utilizes\nannotation-efficient prompts generated in a fully unsupervised fashion, while\nstill capturing essential semantic, location, and shape information through\ncontrastive language-image pretraining and visual question answering. We adopt\nthe direct preference optimization technique to design an optimal policy that\nenables the model to generate high-fidelity segmentations with simple ratings\nor rankings provided by a virtual annotator simulating the human annotation\nprocess. State-of-the-art performance of our framework in tasks such as lung\nsegmentation, breast tumor segmentation, and organ segmentation across various\nmodalities, including X-ray, ultrasound, and abdominal CT, justifies its\neffectiveness in low-annotation data scenarios.\n","authors":["Aishik Konwer","Zhijian Yang","Erhan Bas","Cao Xiao","Prateek Prasanna","Parminder Bhatia","Taha Kass-Hout"],"pdf_url":"https://arxiv.org/pdf/2503.04639v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.04635v1","updated":"2025-03-06T17:23:55Z","published":"2025-03-06T17:23:55Z","title":"3HANDS Dataset: Learning from Humans for Generating Naturalistic\n  Handovers with Supernumerary Robotic Limbs","summary":"  Supernumerary robotic limbs (SRLs) are robotic structures integrated closely\nwith the user's body, which augment human physical capabilities and necessitate\nseamless, naturalistic human-machine interaction. For effective assistance in\nphysical tasks, enabling SRLs to hand over objects to humans is crucial. Yet,\ndesigning heuristic-based policies for robots is time-consuming, difficult to\ngeneralize across tasks, and results in less human-like motion. When trained\nwith proper datasets, generative models are powerful alternatives for creating\nnaturalistic handover motions. We introduce 3HANDS, a novel dataset of object\nhandover interactions between a participant performing a daily activity and\nanother participant enacting a hip-mounted SRL in a naturalistic manner. 3HANDS\ncaptures the unique characteristics of SRL interactions: operating in intimate\npersonal space with asymmetric object origins, implicit motion synchronization,\nand the user's engagement in a primary task during the handover. To demonstrate\nthe effectiveness of our dataset, we present three models: one that generates\nnaturalistic handover trajectories, another that determines the appropriate\nhandover endpoints, and a third that predicts the moment to initiate a\nhandover. In a user study (N=10), we compare the handover interaction performed\nwith our method compared to a baseline. The findings show that our method was\nperceived as significantly more natural, less physically demanding, and more\ncomfortable.\n","authors":["Artin Saberpour Abadian","Yi-Chi Liao","Ata Otaran","Rishabh Dabral","Marie Muehlhaus","Christian Theobalt","Martin Schmitz","Jürgen Steimle"],"pdf_url":"https://arxiv.org/pdf/2503.04635v1.pdf","comment":"CHI '25"},{"id":"http://arxiv.org/abs/2503.04634v1","updated":"2025-03-06T17:21:12Z","published":"2025-03-06T17:21:12Z","title":"PathoPainter: Augmenting Histopathology Segmentation via Tumor-aware\n  Inpainting","summary":"  Tumor segmentation plays a critical role in histopathology, but it requires\ncostly, fine-grained image-mask pairs annotated by pathologists. Thus,\nsynthesizing histopathology data to expand the dataset is highly desirable.\nPrevious works suffer from inaccuracies and limited diversity in image-mask\npairs, both of which affect training segmentation, particularly in small-scale\ndatasets and the inherently complex nature of histopathology images. To address\nthis challenge, we propose PathoPainter, which reformulates image-mask pair\ngeneration as a tumor inpainting task. Specifically, our approach preserves the\nbackground while inpainting the tumor region, ensuring precise alignment\nbetween the generated image and its corresponding mask. To enhance dataset\ndiversity while maintaining biological plausibility, we incorporate a sampling\nmechanism that conditions tumor inpainting on regional embeddings from a\ndifferent image. Additionally, we introduce a filtering strategy to exclude\nuncertain synthetic regions, further improving the quality of the generated\ndata. Our comprehensive evaluation spans multiple datasets featuring diverse\ntumor types and various training data scales. As a result, segmentation\nimproved significantly with our synthetic data, surpassing existing\nsegmentation data synthesis approaches, e.g., 75.69% -> 77.69% on CAMELYON16.\nThe code is available at https://github.com/HongLiuuuuu/PathoPainter.\n","authors":["Hong Liu","Haosen Yang","Evi M. C. Huijben","Mark Schuiveling","Ruisheng Su","Josien P. W. Pluim","Mitko Veta"],"pdf_url":"https://arxiv.org/pdf/2503.04634v1.pdf","comment":"10 pages, 3 figures"},{"id":"http://arxiv.org/abs/2503.00897v3","updated":"2025-03-06T17:19:22Z","published":"2025-03-02T13:43:53Z","title":"A Simple and Effective Reinforcement Learning Method for Text-to-Image\n  Diffusion Fine-tuning","summary":"  Reinforcement learning (RL)-based fine-tuning has emerged as a powerful\napproach for aligning diffusion models with black-box objectives. Proximal\npolicy optimization (PPO) is the most popular choice of method for policy\noptimization. While effective in terms of performance, PPO is highly sensitive\nto hyper-parameters and involves substantial computational overhead. REINFORCE,\non the other hand, mitigates some computational complexities such as high\nmemory overhead and sensitive hyper-parameter tuning, but has suboptimal\nperformance due to high-variance and sample inefficiency. While the variance of\nthe REINFORCE can be reduced by sampling multiple actions per input prompt and\nusing a baseline correction term, it still suffers from sample inefficiency. To\naddress these challenges, we systematically analyze the\nefficiency-effectiveness trade-off between REINFORCE and PPO, and propose\nleave-one-out PPO (LOOP), a novel RL for diffusion fine-tuning method. LOOP\ncombines variance reduction techniques from REINFORCE, such as sampling\nmultiple actions per input prompt and a baseline correction term, with the\nrobustness and sample efficiency of PPO via clipping and importance sampling.\nOur results demonstrate that LOOP effectively improves diffusion models on\nvarious black-box objectives, and achieves a better balance between\ncomputational efficiency and performance.\n","authors":["Shashank Gupta","Chaitanya Ahuja","Tsung-Yu Lin","Sreya Dutta Roy","Harrie Oosterhuis","Maarten de Rijke","Satya Narayan Shukla"],"pdf_url":"https://arxiv.org/pdf/2503.00897v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.12833v2","updated":"2025-03-06T17:18:49Z","published":"2024-05-21T14:37:35Z","title":"A Survey of Deep Learning-based Radiology Report Generation Using\n  Multimodal Data","summary":"  Automatic radiology report generation can alleviate the workload for\nphysicians and minimize regional disparities in medical resources, therefore\nbecoming an important topic in the medical image analysis field. It is a\nchallenging task, as the computational model needs to mimic physicians to\nobtain information from multi-modal input data (i.e., medical images, clinical\ninformation, medical knowledge, etc.), and produce comprehensive and accurate\nreports. Recently, numerous works have emerged to address this issue using\ndeep-learning-based methods, such as transformers, contrastive learning, and\nknowledge-base construction. This survey summarizes the key techniques\ndeveloped in the most recent works and proposes a general workflow for\ndeep-learning-based report generation with five main components, including\nmulti-modality data acquisition, data preparation, feature learning, feature\nfusion and interaction, and report generation. The state-of-the-art methods for\neach of these components are highlighted. Additionally, we summarize the latest\ndevelopments in large model-based methods and model explainability, along with\npublic datasets, evaluation methods, current challenges, and future directions\nin this field. We have also conducted a quantitative comparison between\ndifferent methods in the same experimental setting. This is the most up-to-date\nsurvey that focuses on multi-modality inputs and data fusion for radiology\nreport generation. The aim is to provide comprehensive and rich information for\nresearchers interested in automatic clinical report generation and medical\nimage analysis, especially when using multimodal inputs, and to assist them in\ndeveloping new algorithms to advance the field.\n","authors":["Xinyi Wang","Grazziela Figueredo","Ruizhe Li","Wei Emma Zhang","Weitong Chen","Xin Chen"],"pdf_url":"https://arxiv.org/pdf/2405.12833v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11919v3","updated":"2025-03-06T17:12:48Z","published":"2024-09-18T12:32:25Z","title":"LLM-wrapper: Black-Box Semantic-Aware Adaptation of Vision-Language\n  Models for Referring Expression Comprehension","summary":"  Vision Language Models (VLMs) have demonstrated remarkable capabilities in\nvarious open-vocabulary tasks, yet their zero-shot performance lags behind\ntask-specific fine-tuned models, particularly in complex tasks like Referring\nExpression Comprehension (REC). Fine-tuning usually requires 'white-box' access\nto the model's architecture and weights, which is not always feasible due to\nproprietary or privacy concerns. In this work, we propose LLM-wrapper, a method\nfor 'black-box' adaptation of VLMs for the REC task using Large Language Models\n(LLMs). LLM-wrapper capitalizes on the reasoning abilities of LLMs, improved\nwith a light fine-tuning, to select the most relevant bounding box matching the\nreferring expression, from candidates generated by a zero-shot black-box VLM.\nOur approach offers several advantages: it enables the adaptation of\nclosed-source models without needing access to their internal workings, it is\nversatile as it works with any VLM, it transfers to new VLMs and datasets, and\nit allows for the adaptation of an ensemble of VLMs. We evaluate LLM-wrapper on\nmultiple datasets using different VLMs and LLMs, demonstrating significant\nperformance improvements and highlighting the versatility of our method. While\nLLM-wrapper is not meant to directly compete with standard white-box\nfine-tuning, it offers a practical and effective alternative for black-box VLM\nadaptation. Code and checkpoints are available at\nhttps://github.com/valeoai/LLM_wrapper .\n","authors":["Amaia Cardiel","Eloi Zablocki","Elias Ramzi","Oriane Siméoni","Matthieu Cord"],"pdf_url":"https://arxiv.org/pdf/2409.11919v3.pdf","comment":"LLM-wrapper (v3) is published as a conference paper at ICLR 2025. (v1\n  was presented at EVAL-FoMo workshop, ECCV 2024.)"},{"id":"http://arxiv.org/abs/2410.05116v2","updated":"2025-03-06T17:11:55Z","published":"2024-10-07T15:12:01Z","title":"Human-Feedback Efficient Reinforcement Learning for Online Diffusion\n  Model Finetuning","summary":"  Controllable generation through Stable Diffusion (SD) fine-tuning aims to\nimprove fidelity, safety, and alignment with human guidance. Existing\nreinforcement learning from human feedback methods usually rely on predefined\nheuristic reward functions or pretrained reward models built on large-scale\ndatasets, limiting their applicability to scenarios where collecting such data\nis costly or difficult. To effectively and efficiently utilize human feedback,\nwe develop a framework, HERO, which leverages online human feedback collected\non the fly during model learning. Specifically, HERO features two key\nmechanisms: (1) Feedback-Aligned Representation Learning, an online training\nmethod that captures human feedback and provides informative learning signals\nfor fine-tuning, and (2) Feedback-Guided Image Generation, which involves\ngenerating images from SD's refined initialization samples, enabling faster\nconvergence towards the evaluator's intent. We demonstrate that HERO is 4x more\nefficient in online feedback for body part anomaly correction compared to the\nbest existing method. Additionally, experiments show that HERO can effectively\nhandle tasks like reasoning, counting, personalization, and reducing NSFW\ncontent with only 0.5K online feedback.\n","authors":["Ayano Hiranaka","Shang-Fu Chen","Chieh-Hsin Lai","Dongjun Kim","Naoki Murata","Takashi Shibuya","Wei-Hsiang Liao","Shao-Hua Sun","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.05116v2.pdf","comment":"Published in International Conference on Learning Representations\n  (ICLR) 2025"},{"id":"http://arxiv.org/abs/2503.02394v3","updated":"2025-03-06T17:10:24Z","published":"2025-03-04T08:35:01Z","title":"BHViT: Binarized Hybrid Vision Transformer","summary":"  Model binarization has made significant progress in enabling real-time and\nenergy-efficient computation for convolutional neural networks (CNN), offering\na potential solution to the deployment challenges faced by Vision Transformers\n(ViTs) on edge devices. However, due to the structural differences between CNN\nand Transformer architectures, simply applying binary CNN strategies to the ViT\nmodels will lead to a significant performance drop. To tackle this challenge,\nwe propose BHViT, a binarization-friendly hybrid ViT architecture and its full\nbinarization model with the guidance of three important observations.\nInitially, BHViT utilizes the local information interaction and hierarchical\nfeature aggregation technique from coarse to fine levels to address redundant\ncomputations stemming from excessive tokens. Then, a novel module based on\nshift operations is proposed to enhance the performance of the binary\nMultilayer Perceptron (MLP) module without significantly increasing\ncomputational overhead. In addition, an innovative attention matrix\nbinarization method based on quantization decomposition is proposed to evaluate\nthe token's importance in the binarized attention matrix. Finally, we propose a\nregularization loss to address the inadequate optimization caused by the\nincompatibility between the weight oscillation in the binary layers and the\nAdam Optimizer. Extensive experimental results demonstrate that our proposed\nalgorithm achieves SOTA performance among binary ViT methods.\n","authors":["Tian Gao","Zhiyuan Zhang","Yu Zhang","Huajun Liu","Kaijie Yin","Chengzhong Xu","Hui Kong"],"pdf_url":"https://arxiv.org/pdf/2503.02394v3.pdf","comment":"Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2407.18125v3","updated":"2025-03-06T17:03:35Z","published":"2024-07-25T15:32:59Z","title":"Self-supervised pre-training with diffusion model for few-shot landmark\n  detection in x-ray images","summary":"  Deep neural networks have been extensively applied in the medical domain for\nvarious tasks, including image classification, segmentation, and landmark\ndetection. However, their application is often hindered by data scarcity, both\nin terms of available annotations and images. This study introduces a novel\napplication of denoising diffusion probabilistic models (DDPMs) to the landmark\ndetection task, specifically addressing the challenge of limited annotated data\nin x-ray imaging. Our key innovation lies in leveraging DDPMs for\nself-supervised pre-training in landmark detection, a previously unexplored\napproach in this domain. This method enables accurate landmark detection with\nminimal annotated training data (as few as 50 images), surpassing both ImageNet\nsupervised pre-training and traditional self-supervised techniques across three\npopular x-ray benchmark datasets. To our knowledge, this work represents the\nfirst application of diffusion models for self-supervised learning in landmark\ndetection, which may offer a valuable pre-training approach in few-shot\nregimes, for mitigating data scarcity.\n","authors":["Roberto Di Via","Francesca Odone","Vito Paolo Pastore"],"pdf_url":"https://arxiv.org/pdf/2407.18125v3.pdf","comment":"Accepted at WACV 2025"},{"id":"http://arxiv.org/abs/2503.04606v1","updated":"2025-03-06T16:53:14Z","published":"2025-03-06T16:53:14Z","title":"The Best of Both Worlds: Integrating Language Models and Diffusion\n  Models for Video Generation","summary":"  Recent advancements in text-to-video (T2V) generation have been driven by two\ncompeting paradigms: autoregressive language models and diffusion models.\nHowever, each paradigm has intrinsic limitations: language models struggle with\nvisual quality and error accumulation, while diffusion models lack semantic\nunderstanding and causal modeling. In this work, we propose LanDiff, a hybrid\nframework that synergizes the strengths of both paradigms through\ncoarse-to-fine generation. Our architecture introduces three key innovations:\n(1) a semantic tokenizer that compresses 3D visual features into compact 1D\ndiscrete representations through efficient semantic compression, achieving a\n$\\sim$14,000$\\times$ compression ratio; (2) a language model that generates\nsemantic tokens with high-level semantic relationships; (3) a streaming\ndiffusion model that refines coarse semantics into high-fidelity videos.\nExperiments show that LanDiff, a 5B model, achieves a score of 85.43 on the\nVBench T2V benchmark, surpassing the state-of-the-art open-source models\nHunyuan Video (13B) and other commercial models such as Sora, Keling, and\nHailuo. Furthermore, our model also achieves state-of-the-art performance in\nlong video generation, surpassing other open-source models in this field. Our\ndemo can be viewed at https://landiff.github.io/.\n","authors":["Aoxiong Yin","Kai Shen","Yichong Leng","Xu Tan","Xinyu Zhou","Juncheng Li","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2503.04606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.17494v4","updated":"2025-03-06T16:43:10Z","published":"2024-10-23T01:25:25Z","title":"Enhancing Multimodal Medical Image Classification using Cross-Graph\n  Modal Contrastive Learning","summary":"  The classification of medical images is a pivotal aspect of disease\ndiagnosis, often enhanced by deep learning techniques. However, traditional\napproaches typically focus on unimodal medical image data, neglecting the\nintegration of diverse non-image patient data. This paper proposes a novel\nCross-Graph Modal Contrastive Learning (CGMCL) framework for multimodal\nstructured data from different data domains to improve medical image\nclassification. The model effectively integrates both image and non-image data\nby constructing cross-modality graphs and leveraging contrastive learning to\nalign multimodal features in a shared latent space. An inter-modality feature\nscaling module further optimizes the representation learning process by\nreducing the gap between heterogeneous modalities. The proposed approach is\nevaluated on two datasets: a Parkinson's disease (PD) dataset and a public\nmelanoma dataset. Results demonstrate that CGMCL outperforms conventional\nunimodal methods in accuracy, interpretability, and early disease prediction.\nAdditionally, the method shows superior performance in multi-class melanoma\nclassification. The CGMCL framework provides valuable insights into medical\nimage classification while offering improved disease interpretability and\npredictive capabilities.\n","authors":["Jun-En Ding","Chien-Chin Hsu","Chi-Hsiang Chu","Shuqiang Wang","Feng Liu"],"pdf_url":"https://arxiv.org/pdf/2410.17494v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04592v1","updated":"2025-03-06T16:31:34Z","published":"2025-03-06T16:31:34Z","title":"A Benchmark for Multi-Lingual Vision-Language Learning in Remote Sensing\n  Image Captioning","summary":"  Remote Sensing Image Captioning (RSIC) is a cross-modal field bridging vision\nand language, aimed at automatically generating natural language descriptions\nof features and scenes in remote sensing imagery. Despite significant advances\nin developing sophisticated methods and large-scale datasets for training\nvision-language models (VLMs), two critical challenges persist: the scarcity of\nnon-English descriptive datasets and the lack of multilingual capability\nevaluation for models. These limitations fundamentally impede the progress and\npractical deployment of RSIC, particularly in the era of large VLMs. To address\nthese challenges, this paper presents several significant contributions to the\nfield. First, we introduce and analyze BRSIC (Bilingual Remote Sensing Image\nCaptioning), a comprehensive bilingual dataset that enriches three established\nEnglish RSIC datasets with Chinese descriptions, encompassing 13,634 images\npaired with 68,170 bilingual captions. Building upon this foundation, we\ndevelop a systematic evaluation framework that addresses the prevalent\ninconsistency in evaluation protocols, enabling rigorous assessment of model\nperformance through standardized retraining procedures on BRSIC. Furthermore,\nwe present an extensive empirical study of eight state-of-the-art large\nvision-language models (LVLMs), examining their capabilities across multiple\nparadigms including zero-shot inference, supervised fine-tuning, and\nmulti-lingual training. This comprehensive evaluation provides crucial insights\ninto the strengths and limitations of current LVLMs in handling multilingual\nremote sensing tasks. Additionally, our cross-dataset transfer experiments\nreveal interesting findings. The code and data will be available at\nhttps://github.com/mrazhou/BRSIC.\n","authors":["Qing Zhou","Tao Yang","Junyu Gao","Weiping Ni","Junzheng Wu","Qi Wang"],"pdf_url":"https://arxiv.org/pdf/2503.04592v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03663v2","updated":"2025-03-06T16:25:37Z","published":"2025-03-05T16:52:34Z","title":"LION-FS: Fast & Slow Video-Language Thinker as Online Video Assistant","summary":"  First-person video assistants are highly anticipated to enhance our daily\nlives through online video dialogue. However, existing online video assistants\noften sacrifice assistant efficacy for real-time efficiency by processing\nlow-frame-rate videos with coarse-grained visual features.To overcome the\ntrade-off between efficacy and efficiency, we propose \"Fast & Slow\nVideo-Language Thinker\" as an onLIne videO assistaNt, LION-FS, achieving\nreal-time, proactive, temporally accurate, and contextually precise responses.\nLION-FS adopts a two-stage optimization strategy: 1)Fast Path: Routing-Based\nResponse Determination evaluates frame-by-frame whether an immediate response\nis necessary. To enhance response determination accuracy and handle higher\nframe-rate inputs efficiently, we employ Token Aggregation Routing to\ndynamically fuse spatiotemporal features without increasing token numbers,\nwhile utilizing Token Dropping Routing to eliminate redundant features. 2)Slow\nPath: Multi-granularity Keyframe Augmentation optimizes keyframes during\nresponse generation. To provide comprehensive and detailed responses beyond\natomic actions constrained by training data, fine-grained spatial features and\nhuman-environment interaction features are extracted through multi-granular\npooling. These features are further integrated into a meticulously designed\nmultimodal Thinking Template to guide more precise response generation.\nComprehensive evaluations on online video tasks demonstrate that LION-FS\nachieves state-of-the-art efficacy and efficiency.\n","authors":["Wei Li","Bing Hu","Rui Shao","Leyang Shen","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2503.03663v2.pdf","comment":"Accept to CVPR 2025, Project page:\n  https://github.com/JiuTian-VL/LION-FS"},{"id":"http://arxiv.org/abs/2503.04565v1","updated":"2025-03-06T15:53:42Z","published":"2025-03-06T15:53:42Z","title":"Omnidirectional Multi-Object Tracking","summary":"  Panoramic imagery, with its 360{\\deg} field of view, offers comprehensive\ninformation to support Multi-Object Tracking (MOT) in capturing spatial and\ntemporal relationships of surrounding objects. However, most MOT algorithms are\ntailored for pinhole images with limited views, impairing their effectiveness\nin panoramic settings. Additionally, panoramic image distortions, such as\nresolution loss, geometric deformation, and uneven lighting, hinder direct\nadaptation of existing MOT methods, leading to significant performance\ndegradation. To address these challenges, we propose OmniTrack, an\nomnidirectional MOT framework that incorporates Tracklet Management to\nintroduce temporal cues, FlexiTrack Instances for object localization and\nassociation, and the CircularStatE Module to alleviate image and geometric\ndistortions. This integration enables tracking in large field-of-view\nscenarios, even under rapid sensor motion. To mitigate the lack of panoramic\nMOT datasets, we introduce the QuadTrack dataset--a comprehensive panoramic\ndataset collected by a quadruped robot, featuring diverse challenges such as\nwide fields of view, intense motion, and complex environments. Extensive\nexperiments on the public JRDB dataset and the newly introduced QuadTrack\nbenchmark demonstrate the state-of-the-art performance of the proposed\nframework. OmniTrack achieves a HOTA score of 26.92% on JRDB, representing an\nimprovement of 3.43%, and further achieves 23.45% on QuadTrack, surpassing the\nbaseline by 6.81%. The dataset and code will be made publicly available at\nhttps://github.com/xifen523/OmniTrack.\n","authors":["Kai Luo","Hao Shi","Sheng Wu","Fei Teng","Mengfei Duan","Chang Huang","Yuhang Wang","Kaiwei Wang","Kailun Yang"],"pdf_url":"https://arxiv.org/pdf/2503.04565v1.pdf","comment":"Accepted to CVPR 2025. The dataset and code will be made publicly\n  available at https://github.com/xifen523/OmniTrack"},{"id":"http://arxiv.org/abs/2502.09990v2","updated":"2025-03-06T15:38:31Z","published":"2025-02-14T08:22:51Z","title":"X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability","summary":"  Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.\n","authors":["Xiaoya Lu","Dongrui Liu","Yi Yu","Luxin Xu","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2502.09990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04545v1","updated":"2025-03-06T15:33:19Z","published":"2025-03-06T15:33:19Z","title":"ViT-VS: On the Applicability of Pretrained Vision Transformer Features\n  for Generalizable Visual Servoing","summary":"  Visual servoing enables robots to precisely position their end-effector\nrelative to a target object. While classical methods rely on hand-crafted\nfeatures and thus are universally applicable without task-specific training,\nthey often struggle with occlusions and environmental variations, whereas\nlearning-based approaches improve robustness but typically require extensive\ntraining. We present a visual servoing approach that leverages pretrained\nvision transformers for semantic feature extraction, combining the advantages\nof both paradigms while also being able to generalize beyond the provided\nsample. Our approach achieves full convergence in unperturbed scenarios and\nsurpasses classical image-based visual servoing by up to 31.2\\% relative\nimprovement in perturbed scenarios. Even the convergence rates of\nlearning-based methods are matched despite requiring no task- or\nobject-specific training. Real-world evaluations confirm robust performance in\nend-effector positioning, industrial box manipulation, and grasping of unseen\nobjects using only a reference from the same category. Our code and simulation\nenvironment are available at: https://alessandroscherl.github.io/ViT-VS/\n","authors":["Alessandro Scherl","Stefan Thalhammer","Bernhard Neuberger","Wilfried Wöber","José Gracía-Rodríguez"],"pdf_url":"https://arxiv.org/pdf/2503.04545v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00299v2","updated":"2025-03-06T15:32:33Z","published":"2024-10-01T00:43:45Z","title":"GSPR: Multimodal Place Recognition Using 3D Gaussian Splatting for\n  Autonomous Driving","summary":"  Place recognition is a crucial component that enables autonomous vehicles to\nobtain localization results in GPS-denied environments. In recent years,\nmultimodal place recognition methods have gained increasing attention. They\novercome the weaknesses of unimodal sensor systems by leveraging complementary\ninformation from different modalities. However, most existing methods explore\ncross-modality correlations through feature-level or descriptor-level fusion,\nsuffering from a lack of interpretability. Conversely, the recently proposed 3D\nGaussian Splatting provides a new perspective on multimodal fusion by\nharmonizing different modalities into an explicit scene representation. In this\npaper, we propose a 3D Gaussian Splatting-based multimodal place recognition\nnetwork dubbed GSPR. It explicitly combines multi-view RGB images and LiDAR\npoint clouds into a spatio-temporally unified scene representation with the\nproposed Multimodal Gaussian Splatting. A network composed of 3D graph\nconvolution and transformer is designed to extract spatio-temporal features and\nglobal descriptors from the Gaussian scenes for place recognition. Extensive\nevaluations on three datasets demonstrate that our method can effectively\nleverage complementary strengths of both multi-view cameras and LiDAR,\nachieving SOTA place recognition performance while maintaining solid\ngeneralization ability. Our open-source code will be released at\nhttps://github.com/QiZS-BIT/GSPR.\n","authors":["Zhangshuo Qi","Junyi Ma","Jingyi Xu","Zijie Zhou","Luqi Cheng","Guangming Xiong"],"pdf_url":"https://arxiv.org/pdf/2410.00299v2.pdf","comment":"8 pages, 6 figures"},{"id":"http://arxiv.org/abs/2412.07775v2","updated":"2025-03-06T15:15:58Z","published":"2024-12-10T18:59:58Z","title":"Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed\n  GFlowNets","summary":"  While one commonly trains large diffusion models by collecting datasets on\ntarget downstream tasks, it is often desired to align and finetune pretrained\ndiffusion models with some reward functions that are either designed by experts\nor learned from small-scale datasets. Existing post-training methods for reward\nfinetuning of diffusion models typically suffer from lack of diversity in\ngenerated samples, lack of prior preservation, and/or slow convergence in\nfinetuning. Inspired by recent successes in generative flow networks\n(GFlowNets), a class of probabilistic models that sample with the unnormalized\ndensity of a reward function, we propose a novel GFlowNet method dubbed\nNabla-GFlowNet (abbreviated as \\methodname), the first GFlowNet method that\nleverages the rich signal in reward gradients, together with an objective\ncalled \\graddb plus its variant \\resgraddb designed for prior-preserving\ndiffusion finetuning. We show that our proposed method achieves fast yet\ndiversity- and prior-preserving finetuning of Stable Diffusion, a large-scale\ntext-conditioned image diffusion model, on different realistic reward\nfunctions.\n","authors":["Zhen Liu","Tim Z. Xiao","Weiyang Liu","Yoshua Bengio","Dinghuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.07775v2.pdf","comment":"Technical Report (35 pages, 31 figures), Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04522v1","updated":"2025-03-06T15:08:34Z","published":"2025-03-06T15:08:34Z","title":"In-Context Reverse Classification Accuracy: Efficient Estimation of\n  Segmentation Quality without Ground-Truth","summary":"  Assessing the quality of automatic image segmentation is crucial in clinical\npractice, but often very challenging due to the limited availability of ground\ntruth annotations. In this paper, we introduce In-Context Reverse\nClassification Accuracy (In-Context RCA), a novel framework for automatically\nestimating segmentation quality in the absence of ground-truth annotations. By\nleveraging recent in-context learning segmentation models and incorporating\nretrieval-augmentation techniques to select the most relevant reference images,\nour approach enables efficient quality estimation with minimal reference data.\nValidated across diverse medical imaging modalities, our method demonstrates\nrobust performance and computational efficiency, offering a promising solution\nfor automated quality control in clinical workflows, where fast and reliable\nsegmentation assessment is essential. The code is available at\nhttps://github.com/mcosarinsky/In-Context-RCA.\n","authors":["Matias Cosarinsky","Ramiro Billot","Lucas Mansilla","Gabriel Gimenez","Nicolas Gaggión","Guanghui Fu","Enzo Ferrante"],"pdf_url":"https://arxiv.org/pdf/2503.04522v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05874v2","updated":"2025-03-06T15:02:33Z","published":"2025-02-09T12:23:40Z","title":"MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor\n  Scene Generation","summary":"  Controllable 3D scene generation has extensive applications in virtual\nreality and interior design, where the generated scenes should exhibit high\nlevels of realism and controllability in terms of geometry. Scene graphs\nprovide a suitable data representation that facilitates these applications.\nHowever, current graph-based methods for scene generation are constrained to\ntext-based inputs and exhibit insufficient adaptability to flexible user\ninputs, hindering the ability to precisely control object geometry. To address\nthis issue, we propose MMGDreamer, a dual-branch diffusion model for scene\ngeneration that incorporates a novel Mixed-Modality Graph, visual enhancement\nmodule, and relation predictor. The mixed-modality graph allows object nodes to\nintegrate textual and visual modalities, with optional relationships between\nnodes. It enhances adaptability to flexible user inputs and enables meticulous\ncontrol over the geometry of objects in the generated scenes. The visual\nenhancement module enriches the visual fidelity of text-only nodes by\nconstructing visual representations using text embeddings. Furthermore, our\nrelation predictor leverages node representations to infer absent relationships\nbetween nodes, resulting in more coherent scene layouts. Extensive experimental\nresults demonstrate that MMGDreamer exhibits superior control of object\ngeometry, achieving state-of-the-art scene generation performance. Project\npage: https://yangzhifeio.github.io/project/MMGDreamer.\n","authors":["Zhifei Yang","Keyang Lu","Chao Zhang","Jiaxing Qi","Hanqi Jiang","Ruifei Ma","Shenglin Yin","Yifan Xu","Mingzhe Xing","Zhen Xiao","Jieyi Long","Xiangde Liu","Guangyao Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.05874v2.pdf","comment":"Accepted by AAAI 2025 Main Track"},{"id":"http://arxiv.org/abs/2503.04513v1","updated":"2025-03-06T14:59:38Z","published":"2025-03-06T14:59:38Z","title":"A Novel Solution for Drone Photogrammetry with Low-overlap Aerial Images\n  using Monocular Depth Estimation","summary":"  Low-overlap aerial imagery poses significant challenges to traditional\nphotogrammetric methods, which rely heavily on high image overlap to produce\naccurate and complete mapping products. In this study, we propose a novel\nworkflow based on monocular depth estimation to address the limitations of\nconventional techniques. Our method leverages tie points obtained from aerial\ntriangulation to establish a relationship between monocular depth and metric\ndepth, thus transforming the original depth map into a metric depth map,\nenabling the generation of dense depth information and the comprehensive\nreconstruction of the scene. For the experiments, a high-overlap drone dataset\ncontaining 296 images is processed using Metashape to generate depth maps and\nDSMs as ground truth. Subsequently, we create a low-overlap dataset by\nselecting 20 images for experimental evaluation. Results demonstrate that while\nthe recovered depth maps and resulting DSMs achieve meter-level accuracy, they\nprovide significantly better completeness compared to traditional methods,\nparticularly in regions covered by single images. This study showcases the\npotential of monocular depth estimation in low-overlap aerial photogrammetry.\n","authors":["Jiageng Zhong","Qi Zhou","Ming Li","Armin Gruen","Xuan Liao"],"pdf_url":"https://arxiv.org/pdf/2503.04513v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04504v1","updated":"2025-03-06T14:52:34Z","published":"2025-03-06T14:52:34Z","title":"AnyAnomaly: Zero-Shot Customizable Video Anomaly Detection with LVLM","summary":"  Video anomaly detection (VAD) is crucial for video analysis and surveillance\nin computer vision. However, existing VAD models rely on learned normal\npatterns, which makes them difficult to apply to diverse environments.\nConsequently, users should retrain models or develop separate AI models for new\nenvironments, which requires expertise in machine learning, high-performance\nhardware, and extensive data collection, limiting the practical usability of\nVAD. To address these challenges, this study proposes customizable video\nanomaly detection (C-VAD) technique and the AnyAnomaly model. C-VAD considers\nuser-defined text as an abnormal event and detects frames containing a\nspecified event in a video. We effectively implemented AnyAnomaly using a\ncontext-aware visual question answering without fine-tuning the large vision\nlanguage model. To validate the effectiveness of the proposed model, we\nconstructed C-VAD datasets and demonstrated the superiority of AnyAnomaly.\nFurthermore, our approach showed competitive performance on VAD benchmark\ndatasets, achieving state-of-the-art results on the UBnormal dataset and\noutperforming other methods in generalization across all datasets. Our code is\navailable online at github.com/SkiddieAhn/Paper-AnyAnomaly.\n","authors":["Sunghyun Ahn","Youngwan Jo","Kijung Lee","Sein Kwon","Inpyo Hong","Sanghyun Park"],"pdf_url":"https://arxiv.org/pdf/2503.04504v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.01262v2","updated":"2025-03-06T14:50:58Z","published":"2025-02-03T11:36:01Z","title":"FSPGD: Rethinking Black-box Attacks on Semantic Segmentation","summary":"  Transferability, the ability of adversarial examples crafted for one model to\ndeceive other models, is crucial for black-box attacks. Despite advancements in\nattack methods for semantic segmentation, transferability remains limited,\nreducing their effectiveness in real-world applications. To address this, we\nintroduce the Feature Similarity Projected Gradient Descent (FSPGD) attack, a\nnovel black-box approach that enhances both attack performance and\ntransferability. Unlike conventional segmentation attacks that rely on output\npredictions for gradient calculation, FSPGD computes gradients from\nintermediate layer features. Specifically, our method introduces a loss\nfunction that targets local information by comparing features between clean\nimages and adversarial examples, while also disrupting contextual information\nby accounting for spatial relationships between objects. Experiments on Pascal\nVOC 2012 and Cityscapes datasets demonstrate that FSPGD achieves superior\ntransferability and attack performance, establishing a new state-of-the-art\nbenchmark. Code is available at https://github.com/KU-AIVS/FSPGD.\n","authors":["Eun-Sol Park","MiSo Park","Seung Park","Yong-Goo Shin"],"pdf_url":"https://arxiv.org/pdf/2502.01262v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04501v1","updated":"2025-03-06T14:50:17Z","published":"2025-03-06T14:50:17Z","title":"IMFine: 3D Inpainting via Geometry-guided Multi-view Refinement","summary":"  Current 3D inpainting and object removal methods are largely limited to\nfront-facing scenes, facing substantial challenges when applied to diverse,\n\"unconstrained\" scenes where the camera orientation and trajectory are\nunrestricted. To bridge this gap, we introduce a novel approach that produces\ninpainted 3D scenes with consistent visual quality and coherent underlying\ngeometry across both front-facing and unconstrained scenes. Specifically, we\npropose a robust 3D inpainting pipeline that incorporates geometric priors and\na multi-view refinement network trained via test-time adaptation, building on a\npre-trained image inpainting model. Additionally, we develop a novel inpainting\nmask detection technique to derive targeted inpainting masks from object masks,\nboosting the performance in handling unconstrained scenes. To validate the\nefficacy of our approach, we create a challenging and diverse benchmark that\nspans a wide range of scenes. Comprehensive experiments demonstrate that our\nproposed method substantially outperforms existing state-of-the-art approaches.\n","authors":["Zhihao Shi","Dong Huo","Yuhongze Zhou","Kejia Yin","Yan Min","Juwei Lu","Xinxin Zuo"],"pdf_url":"https://arxiv.org/pdf/2503.04501v1.pdf","comment":"Accepted at CVPR 2025,\n  \\href{https://xinxinzuo2353.github.io/imfine/}{Project Page}"},{"id":"http://arxiv.org/abs/2503.04500v1","updated":"2025-03-06T14:49:28Z","published":"2025-03-06T14:49:28Z","title":"ReynoldsFlow: Exquisite Flow Estimation via Reynolds Transport Theorem","summary":"  Optical flow is a fundamental technique for motion estimation, widely applied\nin video stabilization, interpolation, and object tracking. Recent advancements\nin artificial intelligence (AI) have enabled deep learning models to leverage\noptical flow as an important feature for motion analysis. However, traditional\noptical flow methods rely on restrictive assumptions, such as brightness\nconstancy and slow motion constraints, limiting their effectiveness in complex\nscenes. Deep learning-based approaches require extensive training on large\ndomain-specific datasets, making them computationally demanding. Furthermore,\noptical flow is typically visualized in the HSV color space, which introduces\nnonlinear distortions when converted to RGB and is highly sensitive to noise,\ndegrading motion representation accuracy. These limitations inherently\nconstrain the performance of downstream models, potentially hindering object\ntracking and motion analysis tasks. To address these challenges, we propose\nReynolds flow, a novel training-free flow estimation inspired by the Reynolds\ntransport theorem, offering a principled approach to modeling complex motion\ndynamics. Beyond the conventional HSV-based visualization, denoted\nReynoldsFlow, we introduce an alternative representation, ReynoldsFlow+,\ndesigned to improve flow visualization. We evaluate ReynoldsFlow and\nReynoldsFlow+ across three video-based benchmarks: tiny object detection on\nUAVDB, infrared object detection on Anti-UAV, and pose estimation on GolfDB.\nExperimental results demonstrate that networks trained with ReynoldsFlow+\nachieve state-of-the-art (SOTA) performance, exhibiting improved robustness and\nefficiency across all tasks.\n","authors":["Yu-Hsi Chen","Chin-Tien Wu"],"pdf_url":"https://arxiv.org/pdf/2503.04500v1.pdf","comment":"10 pages, 3 figures, 3 tables"},{"id":"http://arxiv.org/abs/2503.04499v1","updated":"2025-03-06T14:48:25Z","published":"2025-03-06T14:48:25Z","title":"Spatial regularisation for improved accuracy and interpretability in\n  keypoint-based registration","summary":"  Unsupervised registration strategies bypass requirements in ground truth\ntransforms or segmentations by optimising similarity metrics between fixed and\nmoved volumes. Among these methods, a recent subclass of approaches based on\nunsupervised keypoint detection stand out as very promising for\ninterpretability. Specifically, these methods train a network to predict\nfeature maps for fixed and moving images, from which explainable centres of\nmass are computed to obtain point clouds, that are then aligned in closed-form.\nHowever, the features returned by the network often yield spatially diffuse\npatterns that are hard to interpret, thus undermining the purpose of\nkeypoint-based registration. Here, we propose a three-fold loss to regularise\nthe spatial distribution of the features. First, we use the KL divergence to\nmodel features as point spread functions that we interpret as probabilistic\nkeypoints. Then, we sharpen the spatial distributions of these features to\nincrease the precision of the detected landmarks. Finally, we introduce a new\nrepulsive loss across keypoints to encourage spatial diversity. Overall, our\nloss considerably improves the interpretability of the features, which now\ncorrespond to precise and anatomically meaningful landmarks. We demonstrate our\nthree-fold loss in foetal rigid motion tracking and brain MRI affine\nregistration tasks, where it not only outperforms state-of-the-art unsupervised\nstrategies, but also bridges the gap with state-of-the-art supervised methods.\nOur code is available at https://github.com/BenBillot/spatial_regularisation.\n","authors":["Benjamin Billot","Ramya Muthukrishnan","Esra Abaci-Turk","Ellen P. Grant","Nicholas Ayache","Hervé Delingette","Polina Golland"],"pdf_url":"https://arxiv.org/pdf/2503.04499v1.pdf","comment":"under review"},{"id":"http://arxiv.org/abs/2503.04496v1","updated":"2025-03-06T14:44:25Z","published":"2025-03-06T14:44:25Z","title":"Learning Object Placement Programs for Indoor Scene Synthesis with\n  Iterative Self Training","summary":"  Data driven and autoregressive indoor scene synthesis systems generate indoor\nscenes automatically by suggesting and then placing objects one at a time.\nEmpirical observations show that current systems tend to produce incomplete\nnext object location distributions. We introduce a system which addresses this\nproblem. We design a Domain Specific Language (DSL) that specifies functional\nconstraints. Programs from our language take as input a partial scene and\nobject to place. Upon execution they predict possible object placements. We\ndesign a generative model which writes these programs automatically. Available\n3D scene datasets do not contain programs to train on, so we build upon\nprevious work in unsupervised program induction to introduce a new program\nbootstrapping algorithm. In order to quantify our empirical observations we\nintroduce a new evaluation procedure which captures how well a system models\nper-object location distributions. We ask human annotators to label all the\npossible places an object can go in a scene and show that our system produces\nper-object location distributions more consistent with human annotators. Our\nsystem also generates indoor scenes of comparable quality to previous systems\nand while previous systems degrade in performance when training data is sparse,\nour system does not degrade to the same degree.\n","authors":["Adrian Chang","Kai Wang","Yuanbo Li","Manolis Savva","Angel X. Chang","Daniel Ritchie"],"pdf_url":"https://arxiv.org/pdf/2503.04496v1.pdf","comment":"21 pages, 20 figures Subjects: Graphics (cs.GR), Computer Vision and\n  Pattern Recognition (cs.CV), Machine Learning (cs.LG)"},{"id":"http://arxiv.org/abs/2412.04842v3","updated":"2025-03-06T14:40:15Z","published":"2024-12-06T08:27:53Z","title":"UniMLVG: Unified Framework for Multi-view Long Video Generation with\n  Comprehensive Control Capabilities for Autonomous Driving","summary":"  The creation of diverse and realistic driving scenarios has become essential\nto enhance perception and planning capabilities of the autonomous driving\nsystem. However, generating long-duration, surround-view consistent driving\nvideos remains a significant challenge. To address this, we present UniMLVG, a\nunified framework designed to generate extended street multi-perspective videos\nunder precise control. By integrating single- and multi-view driving videos\ninto the training data, our approach updates a DiT-based diffusion model\nequipped with cross-frame and cross-view modules across three stages with multi\ntraining objectives, substantially boosting the diversity and quality of\ngenerated visual content. Importantly, we propose an innovative explicit\nviewpoint modeling approach for multi-view video generation to effectively\nimprove motion transition consistency. Capable of handling various input\nreference formats (e.g., text, images, or video), our UniMLVG generates\nhigh-quality multi-view videos according to the corresponding condition\nconstraints such as 3D bounding boxes or frame-level text descriptions.\nCompared to the best models with similar capabilities, our framework achieves\nimprovements of 48.2% in FID and 35.2% in FVD.\n","authors":["Rui Chen","Zehuan Wu","Yichen Liu","Yuxin Guo","Jingcheng Ni","Haifeng Xia","Siyu Xia"],"pdf_url":"https://arxiv.org/pdf/2412.04842v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03222v2","updated":"2025-03-06T14:32:49Z","published":"2025-03-05T06:32:49Z","title":"Mocap-2-to-3: Lifting 2D Diffusion-Based Pretrained Models for 3D Motion\n  Capture","summary":"  Recovering absolute poses in the world coordinate system from monocular views\npresents significant challenges. Two primary issues arise in this context.\nFirstly, existing methods rely on 3D motion data for training, which requires\ncollection in limited environments. Acquiring such 3D labels for new actions in\na timely manner is impractical, severely restricting the model's generalization\ncapabilities. In contrast, 2D poses are far more accessible and easier to\nobtain. Secondly, estimating a person's absolute position in metric space from\na single viewpoint is inherently more complex. To address these challenges, we\nintroduce Mocap-2-to-3, a novel framework that decomposes intricate 3D motions\ninto 2D poses, leveraging 2D data to enhance 3D motion reconstruction in\ndiverse scenarios and accurately predict absolute positions in the world\ncoordinate system. We initially pretrain a single-view diffusion model with\nextensive 2D data, followed by fine-tuning a multi-view diffusion model for\nview consistency using publicly available 3D data. This strategy facilitates\nthe effective use of large-scale 2D data. Additionally, we propose an\ninnovative human motion representation that decouples local actions from global\nmovements and encodes geometric priors of the ground, ensuring the generative\nmodel learns accurate motion priors from 2D data. During inference, this allows\nfor the gradual recovery of global movements, resulting in more plausible\npositioning. We evaluate our model's performance on real-world datasets,\ndemonstrating superior accuracy in motion and absolute human positioning\ncompared to state-of-the-art methods, along with enhanced generalization and\nscalability. Our code will be made publicly available.\n","authors":["Zhumei Wang","Zechen Hu","Ruoxi Guo","Huaijin Pi","Ziyong Feng","Sida Peng","Xiaowei Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.03222v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04478v1","updated":"2025-03-06T14:28:17Z","published":"2025-03-06T14:28:17Z","title":"Semantic Alignment of Unimodal Medical Text and Vision Representations","summary":"  General-purpose AI models, particularly those designed for text and vision,\ndemonstrate impressive versatility across a wide range of deep-learning tasks.\nHowever, they often underperform in specialised domains like medical imaging,\nwhere domain-specific solutions or alternative knowledge transfer approaches\nare typically required. Recent studies have noted that general-purpose models\ncan exhibit similar latent spaces when processing semantically related data,\nalthough this alignment does not occur naturally. Building on this insight, it\nhas been shown that applying a simple transformation - at most affine -\nestimated from a subset of semantically corresponding samples, known as\nanchors, enables model stitching across diverse training paradigms,\narchitectures, and modalities. In this paper, we explore how semantic alignment\n- estimating transformations between anchors - can bridge general-purpose AI\nwith specialised medical knowledge. Using multiple public chest X-ray datasets,\nwe demonstrate that model stitching across model architectures allows general\nmodels to integrate domain-specific knowledge without additional training,\nleading to improved performance on medical tasks. Furthermore, we introduce a\nnovel zero-shot classification approach for unimodal vision encoders that\nleverages semantic alignment across modalities. Our results show that our\nmethod not only outperforms general multimodal models but also approaches the\nperformance levels of fully trained, medical-specific multimodal solutions\n","authors":["Maxime Di Folco","Emily Chan","Marta Hasny","Cosmin I. Bercea","Julia A. Schnabel"],"pdf_url":"https://arxiv.org/pdf/2503.04478v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13524v4","updated":"2025-03-06T14:27:12Z","published":"2025-02-19T08:21:59Z","title":"MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D\n  Medical Image Analysis","summary":"  Efficient evaluation of three-dimensional (3D) medical images is crucial for\ndiagnostic and therapeutic practices in healthcare. Recent years have seen a\nsubstantial uptake in applying deep learning and computer vision to analyse and\ninterpret medical images. Traditional approaches, such as convolutional neural\nnetworks (CNNs) and vision transformers (ViTs), face significant computational\nchallenges, prompting the need for architectural advancements. Recent efforts\nhave led to the introduction of novel architectures like the ``Mamba'' model as\nalternative solutions to traditional CNNs or ViTs. The Mamba model excels in\nthe linear processing of one-dimensional data with low computational demands.\nHowever, Mamba's potential for 3D medical image analysis remains underexplored\nand could face significant computational challenges as the dimension increases.\nThis manuscript presents MobileViM, a streamlined architecture for efficient\nsegmentation of 3D medical images. In the MobileViM network, we invent a new\ndimension-independent mechanism and a dual-direction traversing approach to\nincorporate with a vision-Mamba-based framework. MobileViM also features a\ncross-scale bridging technique to improve efficiency and accuracy across\nvarious medical imaging modalities. With these enhancements, MobileViM achieves\nsegmentation speeds exceeding 90 frames per second (FPS) on a single graphics\nprocessing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster\nthan the state-of-the-art deep learning models for processing 3D images with\nthe same computational resources. In addition, experimental evaluations\ndemonstrate that MobileViM delivers superior performance, with Dice similarity\nscores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024,\nATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses\nexisting models.\n","authors":["Wei Dai","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.13524v4.pdf","comment":"The corresponding author disagrees with the manuscript submitted to\n  arXiv"},{"id":"http://arxiv.org/abs/2503.04475v1","updated":"2025-03-06T14:24:22Z","published":"2025-03-06T14:24:22Z","title":"ForestLPR: LiDAR Place Recognition in Forests Attentioning Multiple BEV\n  Density Images","summary":"  Place recognition is essential to maintain global consistency in large-scale\nlocalization systems. While research in urban environments has progressed\nsignificantly using LiDARs or cameras, applications in natural forest-like\nenvironments remain largely under-explored. Furthermore, forests present\nparticular challenges due to high self-similarity and substantial variations in\nvegetation growth over time. In this work, we propose a robust LiDAR-based\nplace recognition method for natural forests, ForestLPR. We hypothesize that a\nset of cross-sectional images of the forest's geometry at different heights\ncontains the information needed to recognize revisiting a place. The\ncross-sectional images are represented by \\ac{bev} density images of horizontal\nslices of the point cloud at different heights. Our approach utilizes a visual\ntransformer as the shared backbone to produce sets of local descriptors and\nintroduces a multi-BEV interaction module to attend to information at different\nheights adaptively. It is followed by an aggregation layer that produces a\nrotation-invariant place descriptor. We evaluated the efficacy of our method\nextensively on real-world data from public benchmarks as well as robotic\ndatasets and compared it against the state-of-the-art (SOTA) methods. The\nresults indicate that ForestLPR has consistently good performance on all\nevaluations and achieves an average increase of 7.38\\% and 9.11\\% on Recall@1\nover the closest competitor on intra-sequence loop closure detection and\ninter-sequence re-localization, respectively, validating our hypothesis\n","authors":["Yanqing Shen","Turcan Tuna","Marco Hutter","Cesar Cadena","Nanning Zheng"],"pdf_url":"https://arxiv.org/pdf/2503.04475v1.pdf","comment":"accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2503.04470v1","updated":"2025-03-06T14:21:43Z","published":"2025-03-06T14:21:43Z","title":"Gate-Shift-Pose: Enhancing Action Recognition in Sports with Skeleton\n  Information","summary":"  This paper introduces Gate-Shift-Pose, an enhanced version of Gate-Shift-Fuse\nnetworks, designed for athlete fall classification in figure skating by\nintegrating skeleton pose data alongside RGB frames. We evaluate two fusion\nstrategies: early-fusion, which combines RGB frames with Gaussian heatmaps of\npose keypoints at the input stage, and late-fusion, which employs a\nmulti-stream architecture with attention mechanisms to combine RGB and pose\nfeatures. Experiments on the FR-FS dataset demonstrate that Gate-Shift-Pose\nsignificantly outperforms the RGB-only baseline, improving accuracy by up to\n40% with ResNet18 and 20% with ResNet50. Early-fusion achieves the highest\naccuracy (98.08%) with ResNet50, leveraging the model's capacity for effective\nmultimodal integration, while late-fusion is better suited for lighter\nbackbones like ResNet18. These results highlight the potential of multimodal\narchitectures for sports action recognition and the critical role of skeleton\npose information in capturing complex motion patterns.\n","authors":["Edoardo Bianchi","Oswald Lanz"],"pdf_url":"https://arxiv.org/pdf/2503.04470v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04459v1","updated":"2025-03-06T14:11:46Z","published":"2025-03-06T14:11:46Z","title":"Question-Aware Gaussian Experts for Audio-Visual Question Answering","summary":"  Audio-Visual Question Answering (AVQA) requires not only question-based\nmultimodal reasoning but also precise temporal grounding to capture subtle\ndynamics for accurate prediction. However, existing methods mainly use question\ninformation implicitly, limiting focus on question-specific details.\nFurthermore, most studies rely on uniform frame sampling, which can miss key\nquestion-relevant frames. Although recent Top-K frame selection methods aim to\naddress this, their discrete nature still overlooks fine-grained temporal\ndetails. This paper proposes \\textbf{QA-TIGER}, a novel framework that\nexplicitly incorporates question information and models continuous temporal\ndynamics. Our key idea is to use Gaussian-based modeling to adaptively focus on\nboth consecutive and non-consecutive frames based on the question, while\nexplicitly injecting question information and applying progressive refinement.\nWe leverage a Mixture of Experts (MoE) to flexibly implement multiple Gaussian\nmodels, activating temporal experts specifically tailored to the question.\nExtensive experiments on multiple AVQA benchmarks show that QA-TIGER\nconsistently achieves state-of-the-art performance. Code is available at\nhttps://github.com/AIM-SKKU/QA-TIGER\n","authors":["Hongyeob Kim","Inyoung Jung","Dayoon Suh","Youjia Zhang","Sangmin Lee","Sungeun Hong"],"pdf_url":"https://arxiv.org/pdf/2503.04459v1.pdf","comment":"CVPR 2025. Project page at https://aim-skku.github.io/QA-TIGER/"},{"id":"http://arxiv.org/abs/2503.04457v1","updated":"2025-03-06T14:11:00Z","published":"2025-03-06T14:11:00Z","title":"TPC: Cross-Temporal Prediction Connection for Vision-Language Model\n  Hallucination Reduction","summary":"  Vision-language models (VLMs) have achieved remarkable advancements,\ncapitalizing on the impressive capabilities of large language models (LLMs)\nacross diverse tasks. Despite this, a critical challenge known as hallucination\noccurs when models overconfidently describe objects or attributes absent from\nthe image, a problem exacerbated by the tendency of VLMs to rely on linguistic\npriors. This limitation reduces model reliability in high-stakes applications.\nIn this work, we have observed the characteristic of logits' continuity\nconsistency enhancement and introduced a straightforward and efficient method,\nCross-Temporal Prediction Connection (TPC), designed to enhance the semantic\nconsistency of logits by connecting them temporally across timesteps. TPC\namplifies information flow and improves coherence, effectively reducing\nhallucination. Extensive experiments show that TPC surpasses existing\nrepresentatives, delivering superior performance in both accuracy and\nefficiency while maintaining robustness in open-ended text generation tasks.\n","authors":["Chao Wang","Weiwei Fu","Yang Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.04457v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04452v1","updated":"2025-03-06T14:06:35Z","published":"2025-03-06T14:06:35Z","title":"A lightweight model FDM-YOLO for small target improvement based on\n  YOLOv8","summary":"  Small targets are particularly difficult to detect due to their low pixel\ncount, complex backgrounds, and varying shooting angles, which make it hard for\nmodels to extract effective features. While some large-scale models offer high\naccuracy, their long inference times make them unsuitable for real-time\ndeployment on edge devices. On the other hand, models designed for low\ncomputational power often suffer from poor detection accuracy. This paper\nfocuses on small target detection and explores methods for object detection\nunder low computational constraints. Building on the YOLOv8 model, we propose a\nnew network architecture called FDM-YOLO. Our research includes the following\nkey contributions: We introduce FDM-YOLO by analyzing the output of the YOLOv8\ndetection head. We add a highresolution layer and remove the large target\ndetection layer to better handle small targets. Based on PConv, we propose a\nlightweight network structure called Fast-C2f, which is integrated into the PAN\nmodule of the model. To mitigate the accuracy loss caused by model\nlightweighting, we employ dynamic upsampling (Dysample) and a lightweight EMA\nattention mechanism.The FDM-YOLO model was validated on the Visdrone dataset,\nachieving a 38% reduction in parameter count and improving the Map0.5 score\nfrom 38.4% to 42.5%, all while maintaining nearly the same inference speed.\nThis demonstrates the effectiveness of our approach in balancing accuracy and\nefficiency for edge device deployment.\n","authors":["Xuerui Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04452v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.04484v3","updated":"2025-03-06T14:06:24Z","published":"2023-12-07T17:59:53Z","title":"FRNet: Frustum-Range Networks for Scalable LiDAR Segmentation","summary":"  LiDAR segmentation has become a crucial component of advanced autonomous\ndriving systems. Recent range-view LiDAR segmentation approaches show promise\nfor real-time processing. However, they inevitably suffer from corrupted\ncontextual information and rely heavily on post-processing techniques for\nprediction refinement. In this work, we propose FRNet, a simple yet powerful\nmethod aimed at restoring the contextual information of range image pixels\nusing corresponding frustum LiDAR points. First, a frustum feature encoder\nmodule is used to extract per-point features within the frustum region, which\npreserves scene consistency and is critical for point-level predictions. Next,\na frustum-point fusion module is introduced to update per-point features\nhierarchically, enabling each point to extract more surrounding information\nthrough the frustum features. Finally, a head fusion module is used to fuse\nfeatures at different levels for final semantic predictions. Extensive\nexperiments conducted on four popular LiDAR segmentation benchmarks under\nvarious task setups demonstrate the superiority of FRNet. Notably, FRNet\nachieves 73.3% and 82.5% mIoU scores on the testing sets of SemanticKITTI and\nnuScenes. While achieving competitive performance, FRNet operates 5 times\nfaster than state-of-the-art approaches. Such high efficiency opens up new\npossibilities for more scalable LiDAR segmentation. The code has been made\npublicly available at https://github.com/Xiangxu-0103/FRNet.\n","authors":["Xiang Xu","Lingdong Kong","Hui Shuai","Qingshan Liu"],"pdf_url":"https://arxiv.org/pdf/2312.04484v3.pdf","comment":"TIP 2025; 18 pages, 11 figures, 14 tables; Code at\n  https://github.com/Xiangxu-0103/FRNet"},{"id":"http://arxiv.org/abs/2407.13304v3","updated":"2025-03-06T14:06:01Z","published":"2024-07-18T09:07:23Z","title":"A Dataset and Benchmark for Shape Completion of Fruits for Agricultural\n  Robotics","summary":"  As the world population is expected to reach 10 billion by 2050, our\nagricultural production system needs to double its productivity despite a\ndecline of human workforce in the agricultural sector. Autonomous robotic\nsystems are one promising pathway to increase productivity by taking over\nlabor-intensive manual tasks like fruit picking. To be effective, such systems\nneed to monitor and interact with plants and fruits precisely, which is\nchallenging due to the cluttered nature of agricultural environments causing,\nfor example, strong occlusions. Thus, being able to estimate the complete 3D\nshapes of objects in presence of occlusions is crucial for automating\noperations such as fruit harvesting. In this paper, we propose the first\npublicly available 3D shape completion dataset for agricultural vision systems.\nWe provide an RGB-D dataset for estimating the 3D shape of fruits.\nSpecifically, our dataset contains RGB-D frames of single sweet peppers in lab\nconditions but also in a commercial greenhouse. For each fruit, we additionally\ncollected high-precision point clouds that we use as ground truth. For\nacquiring the ground truth shape, we developed a measuring process that allows\nus to record data of real sweet pepper plants, both in the lab and in the\ngreenhouse with high precision, and determine the shape of the sensed fruits.\nWe release our dataset, consisting of almost 7,000 RGB-D frames belonging to\nmore than 100 different fruits. We provide segmented RGB-D frames, with camera\nintrinsics to easily obtain colored point clouds, together with the\ncorresponding high-precision, occlusion-free point clouds obtained with a\nhigh-precision laser scanner. We additionally enable evaluation of shape\ncompletion approaches on a hidden test set through a public challenge on a\nbenchmark server.\n","authors":["Federico Magistri","Thomas Läbe","Elias Marks","Sumanth Nagulavancha","Yue Pan","Claus Smitt","Lasse Klingbeil","Michael Halstead","Heiner Kuhlmann","Chris McCool","Jens Behley","Cyrill Stachniss"],"pdf_url":"https://arxiv.org/pdf/2407.13304v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04444v1","updated":"2025-03-06T14:00:59Z","published":"2025-03-06T14:00:59Z","title":"ToFu: Visual Tokens Reduction via Fusion for Multi-modal, Multi-patch,\n  Multi-image Task","summary":"  Large Multimodal Models (LMMs) are powerful tools that are capable of\nreasoning and understanding multimodal information beyond text and language.\nDespite their entrenched impact, the development of LMMs is hindered by the\nhigher computational requirements compared to their unimodal counterparts. One\nof the main causes of this is the large amount of tokens needed to encode the\nvisual input, which is especially evident for multi-image multimodal tasks.\nRecent approaches to reduce visual tokens depend on the visual encoder\narchitecture, require fine-tuning the LLM to maintain the performance, and only\nconsider single-image scenarios. To address these limitations, we propose ToFu,\na visual encoder-agnostic, training-free Token Fusion strategy that combines\nredundant visual tokens of LMMs for high-resolution, multi-image, tasks. The\ncore intuition behind our method is straightforward yet effective: preserve\ndistinctive tokens while combining similar ones. We achieve this by\nsequentially examining visual tokens and deciding whether to merge them with\nothers or keep them as separate entities. We validate our approach on the\nwell-established LLaVA-Interleave Bench, which covers challenging multi-image\ntasks. In addition, we push to the extreme our method by testing it on a\nnewly-created benchmark, ComPairs, focused on multi-image comparisons where a\nlarger amount of images and visual tokens are inputted to the LMMs. Our\nextensive analysis, considering several LMM architectures, demonstrates the\nbenefits of our approach both in terms of efficiency and performance gain.\n","authors":["Vittorio Pippi","Matthieu Guillaumin","Silvia Cascianelli","Rita Cucchiara","Maximilian Jaritz","Loris Bazzani"],"pdf_url":"https://arxiv.org/pdf/2503.04444v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04441v1","updated":"2025-03-06T13:56:48Z","published":"2025-03-06T13:56:48Z","title":"EvidMTL: Evidential Multi-Task Learning for Uncertainty-Aware Semantic\n  Surface Mapping from Monocular RGB Images","summary":"  For scene understanding in unstructured environments, an accurate and\nuncertainty-aware metric-semantic mapping is required to enable informed action\nselection by autonomous systems.Existing mapping methods often suffer from\noverconfident semantic predictions, and sparse and noisy depth sensing, leading\nto inconsistent map representations. In this paper, we therefore introduce\nEvidMTL, a multi-task learning framework that uses evidential heads for depth\nestimation and semantic segmentation, enabling uncertainty-aware inference from\nmonocular RGB images. To enable uncertainty-calibrated evidential multi-task\nlearning, we propose a novel evidential depth loss function that jointly\noptimizes the belief strength of the depth prediction in conjunction with\nevidential segmentation loss. Building on this, we present EvidKimera, an\nuncertainty-aware semantic surface mapping framework, which uses evidential\ndepth and semantics prediction for improved 3D metric-semantic consistency. We\ntrain and evaluate EvidMTL on the NYUDepthV2 and assess its zero-shot\nperformance on ScanNetV2, demonstrating superior uncertainty estimation\ncompared to conventional approaches while maintaining comparable depth\nestimation and semantic segmentation. In zero-shot mapping tests on ScanNetV2,\nEvidKimera outperforms Kimera in semantic surface mapping accuracy and\nconsistency, highlighting the benefits of uncertainty-aware mapping and\nunderscoring its potential for real-world robotic applications.\n","authors":["Rohit Menon","Nils Dengler","Sicong Pan","Gokul Krishna Chenchani","Maren Bennewitz"],"pdf_url":"https://arxiv.org/pdf/2503.04441v1.pdf","comment":"Submitted to IROS 2025 Conference"},{"id":"http://arxiv.org/abs/2503.03272v2","updated":"2025-03-06T13:49:46Z","published":"2025-03-05T08:52:55Z","title":"Towards Effective and Sparse Adversarial Attack on Spiking Neural\n  Networks via Breaking Invisible Surrogate Gradients","summary":"  Spiking neural networks (SNNs) have shown their competence in handling\nspatial-temporal event-based data with low energy consumption. Similar to\nconventional artificial neural networks (ANNs), SNNs are also vulnerable to\ngradient-based adversarial attacks, wherein gradients are calculated by\nspatial-temporal back-propagation (STBP) and surrogate gradients (SGs).\nHowever, the SGs may be invisible for an inference-only model as they do not\ninfluence the inference results, and current gradient-based attacks are\nineffective for binary dynamic images captured by the dynamic vision sensor\n(DVS). While some approaches addressed the issue of invisible SGs through\nuniversal SGs, their SGs lack a correlation with the victim model, resulting in\nsub-optimal performance. Moreover, the imperceptibility of existing SNN-based\nbinary attacks is still insufficient. In this paper, we introduce an innovative\npotential-dependent surrogate gradient (PDSG) method to establish a robust\nconnection between the SG and the model, thereby enhancing the adaptability of\nadversarial attacks across various models with invisible SGs. Additionally, we\npropose the sparse dynamic attack (SDA) to effectively attack binary dynamic\nimages. Utilizing a generation-reduction paradigm, SDA can fully optimize the\nsparsity of adversarial perturbations. Experimental results demonstrate that\nour PDSG and SDA outperform state-of-the-art SNN-based attacks across various\nmodels and datasets. Specifically, our PDSG achieves 100% attack success rate\non ImageNet, and our SDA obtains 82% attack success rate by modifying only\n0.24% of the pixels on CIFAR10DVS. The code is available at\nhttps://github.com/ryime/PDSG-SDA .\n","authors":["Li Lun","Kunyu Feng","Qinglong Ni","Ling Liang","Yuan Wang","Ying Li","Dunshan Yu","Xiaoxin Cui"],"pdf_url":"https://arxiv.org/pdf/2503.03272v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.04420v1","updated":"2025-03-06T13:23:03Z","published":"2025-03-06T13:23:03Z","title":"PointsToWood: A deep learning framework for complete canopy leaf-wood\n  segmentation of TLS data across diverse European forests","summary":"  Point clouds from Terrestrial Laser Scanning (TLS) are an increasingly\npopular source of data for studying plant structure and function but typically\nrequire extensive manual processing to extract ecologically important\ninformation. One key task is the accurate semantic segmentation of different\nplant material within point clouds, particularly wood and leaves, which is\nrequired to understand plant productivity, architecture and physiology.\nExisting automated semantic segmentation methods are primarily developed for\nsingle ecosystem types, and whilst they show good accuracy for biomass\nassessment from the trunk and large branches, often perform less well within\nthe crown. In this study, we demonstrate a new framework that uses a deep\nlearning architecture newly developed from PointNet and pointNEXT for\nprocessing 3D point clouds to provide a reliable semantic segmentation of wood\nand leaf in TLS point clouds from the tree base to branch tips, trained on data\nfrom diverse mature European forests. Our model uses meticulously labelled data\ncombined with voxel-based sampling, neighbourhood rescaling, and a novel gated\nreflectance integration module embedded throughout the feature extraction\nlayers. We evaluate its performance across open datasets from boreal,\ntemperate, Mediterranean and tropical regions, encompassing diverse ecosystem\ntypes and sensor characteristics. Our results show consistent outperformance\nagainst the most widely used PointNet based approach for leaf/wood segmentation\non our high-density TLS dataset collected across diverse mixed forest plots\nacross all major biomes in Europe. We also find consistently strong performance\ntested on others open data from China, Eastern Cameroon, Germany and Finland,\ncollected using both time-of-flight and phase-shift sensors, showcasing the\ntransferability of our model to a wide range of ecosystems and sensors.\n","authors":["Harry J. F. Owen","Matthew J. A. Allen","Stuart W. D. Grieve","Phill Wilkes","Emily R. Lines"],"pdf_url":"https://arxiv.org/pdf/2503.04420v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08388v2","updated":"2025-03-06T13:19:58Z","published":"2024-09-12T20:34:34Z","title":"Continual Learning in 3D Point Clouds: Employing Spectral Techniques for\n  Exemplar Selection","summary":"  We introduce a novel framework for Continual Learning in 3D object\nclassification. Our approach, CL3D, is based on the selection of prototypes\nfrom each class using spectral clustering. For non-Euclidean data such as point\nclouds, spectral clustering can be employed as long as one can define a\ndistance measure between pairs of samples. Choosing the appropriate distance\nmeasure enables us to leverage 3D geometric characteristics to identify\nrepresentative prototypes for each class. We explore the effectiveness of\nclustering in the input space (3D points), local feature space\n(1024-dimensional points), and global feature space. We conduct experiments on\nthe ModelNet40, ShapeNet, and ScanNet datasets, achieving state-of-the-art\naccuracy exclusively through the use of input space features. By leveraging the\ncombined input, local, and global features, we have improved the\nstate-of-the-art on ModelNet and ShapeNet, utilizing nearly half the memory\nused by competing approaches. For the challenging ScanNet dataset, our method\nenhances accuracy by 4.1% while consuming just 28% of the memory used by our\ncompetitors, demonstrating the scalability of our approach.\n","authors":["Hossein Resani","Behrooz Nasihatkon","Mohammadreza Alimoradi Jazi"],"pdf_url":"https://arxiv.org/pdf/2409.08388v2.pdf","comment":"Accepted to WACV 2025, Tucson, Arizona, USA"},{"id":"http://arxiv.org/abs/2503.04416v1","updated":"2025-03-06T13:18:37Z","published":"2025-03-06T13:18:37Z","title":"Learning Transformer-based World Models with Contrastive Predictive\n  Coding","summary":"  The DreamerV3 algorithm recently obtained remarkable performance across\ndiverse environment domains by learning an accurate world model based on\nRecurrent Neural Networks (RNNs). Following the success of model-based\nreinforcement learning algorithms and the rapid adoption of the Transformer\narchitecture for its superior training efficiency and favorable scaling\nproperties, recent works such as STORM have proposed replacing RNN-based world\nmodels with Transformer-based world models using masked self-attention.\nHowever, despite the improved training efficiency of these methods, their\nimpact on performance remains limited compared to the Dreamer algorithm,\nstruggling to learn competitive Transformer-based world models. In this work,\nwe show that the next state prediction objective adopted in previous approaches\nis insufficient to fully exploit the representation capabilities of\nTransformers. We propose to extend world model predictions to longer time\nhorizons by introducing TWISTER (Transformer-based World model wIth contraSTivE\nRepresentations), a world model using action-conditioned Contrastive Predictive\nCoding to learn high-level temporal feature representations and improve the\nagent performance. TWISTER achieves a human-normalized mean score of 162% on\nthe Atari 100k benchmark, setting a new record among state-of-the-art methods\nthat do not employ look-ahead search.\n","authors":["Maxime Burchi","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2503.04416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.03420v2","updated":"2025-03-06T12:52:29Z","published":"2024-05-06T12:40:15Z","title":"Implantable Adaptive Cells: A Novel Enhancement for Pre-Trained U-Nets\n  in Medical Image Segmentation","summary":"  This paper introduces a novel approach to enhance the performance of\npre-trained neural networks in medical image segmentation using gradient-based\nNeural Architecture Search (NAS) methods. We present the concept of Implantable\nAdaptive Cell (IAC), small modules identified through Partially-Connected DARTS\nbased approach, designed to be injected into the skip connections of an\nexisting and already trained U-shaped model. Unlike traditional NAS methods,\nour approach refines existing architectures without full retraining.\nExperiments on four medical datasets with MRI and CT images show consistent\naccuracy improvements on various U-Net configurations, with segmentation\naccuracy gain by approximately 5 percentage points across all validation\ndatasets, with improvements reaching up to 11\\%pt in the best-performing cases.\nThe findings of this study not only offer a cost-effective alternative to the\ncomplete overhaul of complex models for performance upgrades but also indicate\nthe potential applicability of our method to other architectures and problem\ndomains.\n","authors":["Emil Benedykciuk","Marcin Denkowski","Grzegorz Wójcik"],"pdf_url":"https://arxiv.org/pdf/2405.03420v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20742v2","updated":"2025-03-06T12:50:44Z","published":"2025-02-28T05:47:34Z","title":"Structured Preference Optimization for Vision-Language Long-Horizon Task\n  Planning","summary":"  Existing methods for vision-language task planning excel in short-horizon\ntasks but often fall short in complex, long-horizon planning within dynamic\nenvironments. These challenges primarily arise from the difficulty of\neffectively training models to produce high-quality reasoning processes for\nlong-horizon tasks. To address this, we propose Structured Preference\nOptimization (SPO), which aims to enhance reasoning and action selection in\nlong-horizon task planning through structured preference evaluation and\noptimized training strategies. Specifically, SPO introduces: 1)\nPreference-Based Scoring and Optimization, which systematically evaluates\nreasoning chains based on task relevance, visual grounding, and historical\nconsistency; and 2) Curriculum-Guided Training, where the model progressively\nadapts from simple to complex tasks, improving its generalization ability in\nlong-horizon scenarios and enhancing reasoning robustness. To advance research\nin vision-language long-horizon task planning, we introduce ExtendaBench, a\ncomprehensive benchmark covering 1,509 tasks across VirtualHome and Habitat\n2.0, categorized into ultra-short, short, medium, and long tasks. Experimental\nresults demonstrate that SPO significantly improves reasoning quality and final\ndecision accuracy, outperforming prior methods on long-horizon tasks and\nunderscoring the effectiveness of preference-driven optimization in\nvision-language task planning. Specifically, SPO achieves a +5.98% GCR and\n+4.68% SR improvement in VirtualHome and a +3.30% GCR and +2.11% SR improvement\nin Habitat over the best-performing baselines.\n","authors":["Xiwen Liang","Min Lin","Weiqi Ruan","Rongtao Xu","Yuecheng Liu","Jiaqi Chen","Bingqian Lin","Yuzheng Zhuang","Xiaodan Liang"],"pdf_url":"https://arxiv.org/pdf/2502.20742v2.pdf","comment":"18 pages"},{"id":"http://arxiv.org/abs/2503.03285v2","updated":"2025-03-06T12:42:37Z","published":"2025-03-05T09:12:16Z","title":"Enhancing Vietnamese VQA through Curriculum Learning on Raw and\n  Augmented Text Representations","summary":"  Visual Question Answering (VQA) is a multimodal task requiring reasoning\nacross textual and visual inputs, which becomes particularly challenging in\nlow-resource languages like Vietnamese due to linguistic variability and the\nlack of high-quality datasets. Traditional methods often rely heavily on\nextensive annotated datasets, computationally expensive pipelines, and large\npre-trained models, specifically in the domain of Vietnamese VQA, limiting\ntheir applicability in such scenarios. To address these limitations, we propose\na training framework that combines a paraphrase-based feature augmentation\nmodule with a dynamic curriculum learning strategy. Explicitly, augmented\nsamples are considered \"easy\" while raw samples are regarded as \"hard\". The\nframework then utilizes a mechanism that dynamically adjusts the ratio of easy\nto hard samples during training, progressively modifying the same dataset to\nincrease its difficulty level. By enabling gradual adaptation to task\ncomplexity, this approach helps the Vietnamese VQA model generalize well, thus\nimproving overall performance. Experimental results show consistent\nimprovements on the OpenViVQA dataset and mixed outcomes on the ViVQA dataset,\nhighlighting both the potential and challenges of our approach in advancing VQA\nfor Vietnamese language.\n","authors":["Khoi Anh Nguyen","Linh Yen Vu","Thang Dinh Duong","Thuan Nguyen Duong","Huy Thanh Nguyen","Vinh Quang Dinh"],"pdf_url":"https://arxiv.org/pdf/2503.03285v2.pdf","comment":"10 pages, 3 figures, AAAI-25 Workshop on Document Understanding and\n  Intelligence"},{"id":"http://arxiv.org/abs/2503.04385v1","updated":"2025-03-06T12:36:35Z","published":"2025-03-06T12:36:35Z","title":"Scale-Invariant Adversarial Attack against Arbitrary-scale\n  Super-resolution","summary":"  The advent of local continuous image function (LIIF) has garnered significant\nattention for arbitrary-scale super-resolution (SR) techniques. However, while\nthe vulnerabilities of fixed-scale SR have been assessed, the robustness of\ncontinuous representation-based arbitrary-scale SR against adversarial attacks\nremains an area warranting further exploration. The elaborately designed\nadversarial attacks for fixed-scale SR are scale-dependent, which will cause\ntime-consuming and memory-consuming problems when applied to arbitrary-scale\nSR. To address this concern, we propose a simple yet effective\n``scale-invariant'' SR adversarial attack method with good transferability,\ntermed SIAGT. Specifically, we propose to construct resource-saving attacks by\nexploiting finite discrete points of continuous representation. In addition, we\nformulate a coordinate-dependent loss to enhance the cross-model\ntransferability of the attack. The attack can significantly deteriorate the SR\nimages while introducing imperceptible distortion to the targeted\nlow-resolution (LR) images. Experiments carried out on three popular LIIF-based\nSR approaches and four classical SR datasets show remarkable attack performance\nand transferability of SIAGT.\n","authors":["Yihao Huang","Xin Luo","Qing Guo","Felix Juefei-Xu","Xiaojun Jia","Weikai Miao","Geguang Pu","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.04385v1.pdf","comment":"15 pages, accepted by TIFS 2025"},{"id":"http://arxiv.org/abs/2503.04376v1","updated":"2025-03-06T12:27:58Z","published":"2025-03-06T12:27:58Z","title":"MIDAS: Modeling Ground-Truth Distributions with Dark Knowledge for\n  Domain Generalized Stereo Matching","summary":"  Despite the significant advances in domain generalized stereo matching,\nexisting methods still exhibit domain-specific preferences when transferring\nfrom synthetic to real domains, hindering their practical applications in\ncomplex and diverse scenarios. The probability distributions predicted by the\nstereo network naturally encode rich similarity and uncertainty information.\nInspired by this observation, we propose to extract these two types of dark\nknowledge from the pre-trained network to model intuitive multi-modal\nground-truth distributions for both edge and non-edge regions. To mitigate the\ninherent domain preferences of a single network, we adopt network ensemble and\nfurther distinguish between objective and biased knowledge in the Laplace\nparameter space. Finally, the objective knowledge and the original disparity\nlabels are jointly modeled as a mixture of Laplacians to provide fine-grained\nsupervision for the stereo network training. Extensive experiments demonstrate\nthat: 1) Our method is generic and effectively improves the generalization of\nexisting networks. 2) PCWNet with our method achieves the state-of-the-art\ngeneralization performance on both KITTI 2015 and 2012 datasets. 3) Our method\noutperforms existing methods in comprehensive ranking across four popular\nreal-world datasets.\n","authors":["Peng Xu","Zhiyu Xiang","Jingyun Fu","Tianyu Pu","Hanzhi Zhong","Eryun Liu"],"pdf_url":"https://arxiv.org/pdf/2503.04376v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.08824v3","updated":"2025-03-06T12:26:08Z","published":"2024-09-13T13:37:33Z","title":"Pathfinder for Low-altitude Aircraft with Binary Neural Network","summary":"  A prior global topological map (e.g., the OpenStreetMap, OSM) can boost the\nperformance of autonomous mapping by a ground mobile robot. However, the prior\nmap is usually incomplete due to lacking labeling in partial paths. To solve\nthis problem, this paper proposes an OSM maker using airborne sensors carried\nby low-altitude aircraft, where the core of the OSM maker is a novel efficient\npathfinder approach based on LiDAR and camera data, i.e., a binary dual-stream\nroad segmentation model. Specifically, a multi-scale feature extraction based\non the UNet architecture is implemented for images and point clouds. To reduce\nthe effect caused by the sparsity of point cloud, an attention-guided gated\nblock is designed to integrate image and point-cloud features. To optimize the\nmodel for edge deployment that significantly reduces storage footprint and\ncomputational demands, we propose a binarization streamline to each model\ncomponent, including a variant of vision transformer (ViT) architecture as the\nencoder of the image branch, and new focal and perception losses to optimize\nthe model training. The experimental results on two datasets demonstrate that\nour pathfinder method achieves SOTA accuracy with high efficiency in finding\npaths from the low-level airborne sensors, and we can create complete OSM prior\nmaps based on the segmented road skeletons. Code and data are available at:\n\\href{https://github.com/IMRL/Pathfinder}{https://github.com/IMRL/Pathfinder}.\n","authors":["Kaijie Yin","Tian Gao","Hui Kong"],"pdf_url":"https://arxiv.org/pdf/2409.08824v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16532v2","updated":"2025-03-06T12:19:59Z","published":"2025-02-23T10:48:11Z","title":"Deep unrolling for learning optimal spatially varying regularisation\n  parameters for Total Generalised Variation","summary":"  We extend a recently introduced deep unrolling framework for learning\nspatially varying regularisation parameters in inverse imaging problems to the\ncase of Total Generalised Variation (TGV). The framework combines a deep\nconvolutional neural network (CNN) inferring the two spatially varying TGV\nparameters with an unrolled algorithmic scheme that solves the corresponding\nvariational problem. The two subnetworks are jointly trained end-to-end in a\nsupervised fashion and as such the CNN learns to compute those parameters that\ndrive the reconstructed images as close to the ground truth as possible.\nNumerical results in image denoising and MRI reconstruction show a significant\nqualitative and quantitative improvement compared to the best TGV scalar\nparameter case as well as to other approaches employing spatially varying\nparameters computed by unsupervised methods. We also observe that the inferred\nspatially varying parameter maps have a consistent structure near the image\nedges, asking for further theoretical investigations. In particular, the\nparameter that weighs the first-order TGV term has a triple-edge structure with\nalternating high-low-high values whereas the one that weighs the second-order\nterm attains small values in a large neighbourhood around the edges.\n","authors":["Thanh Trung Vu","Andreas Kofler","Kostas Papafitsoros"],"pdf_url":"https://arxiv.org/pdf/2502.16532v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.10329v2","updated":"2025-03-06T12:16:09Z","published":"2024-09-16T14:39:15Z","title":"InfoDisent: Explainability of Image Classification Models by Information\n  Disentanglement","summary":"  In this work, we introduce InfoDisent, a hybrid approach to explainability\nbased on the information bottleneck principle. InfoDisent enables the\ndisentanglement of information in the final layer of any pretrained model into\natomic concepts, which can be interpreted as prototypical parts. This approach\nmerges the flexibility of post-hoc methods with the concept-level modeling\ncapabilities of self-explainable neural networks, such as ProtoPNets. We\ndemonstrate the effectiveness of InfoDisent through computational experiments\nand user studies across various datasets using modern backbones such as ViTs\nand convolutional networks. Notably, InfoDisent generalizes the prototypical\nparts approach to novel domains (ImageNet).\n","authors":["Łukasz Struski","Dawid Rymarczyk","Jacek Tabor"],"pdf_url":"https://arxiv.org/pdf/2409.10329v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.01334v3","updated":"2025-03-06T11:59:11Z","published":"2024-08-02T15:32:42Z","title":"A Backbone for Long-Horizon Robot Task Understanding","summary":"  End-to-end robot learning, particularly for long-horizon tasks, often results\nin unpredictable outcomes and poor generalization. To address these challenges,\nwe propose a novel Therblig-Based Backbone Framework (TBBF) as a fundamental\nstructure to enhance interpretability, data efficiency, and generalization in\nrobotic systems. TBBF utilizes expert demonstrations to enable therblig-level\ntask decomposition, facilitate efficient action-object mapping, and generate\nadaptive trajectories for new scenarios. The approach consists of two stages:\noffline training and online testing. During the offline training stage, we\ndeveloped the Meta-RGate SynerFusion (MGSF) network for accurate therblig\nsegmentation across various tasks. In the online testing stage, after a\none-shot demonstration of a new task is collected, our MGSF network extracts\nhigh-level knowledge, which is then encoded into the image using Action\nRegistration (ActionREG). Additionally, Large Language Model (LLM)-Alignment\nPolicy for Visual Correction (LAP-VC) is employed to ensure precise action\nregistration, facilitating trajectory transfer in novel robot scenarios.\nExperimental results validate these methods, achieving 94.37% recall in\ntherblig segmentation and success rates of 94.4% and 80% in real-world online\nrobot testing for simple and complex scenarios, respectively. Supplementary\nmaterial is available at:\nhttps://sites.google.com/view/therbligsbasedbackbone/home\n","authors":["Xiaoshuai Chen","Wei Chen","Dongmyoung Lee","Yukun Ge","Nicolas Rojas","Petar Kormushev"],"pdf_url":"https://arxiv.org/pdf/2408.01334v3.pdf","comment":"8 pages, 8 figures. This work has been published by IEEE Robotics and\n  Automation Letters (RA-L)"},{"id":"http://arxiv.org/abs/2503.04353v1","updated":"2025-03-06T11:55:44Z","published":"2025-03-06T11:55:44Z","title":"ObjMST: An Object-Focused Multimodal Style Transfer Framework","summary":"  We propose ObjMST, an object-focused multimodal style transfer framework that\nprovides separate style supervision for salient objects and surrounding\nelements while addressing alignment issues in multimodal representation\nlearning. Existing image-text multimodal style transfer methods face the\nfollowing challenges: (1) generating non-aligned and inconsistent multimodal\nstyle representations; and (2) content mismatch, where identical style patterns\nare applied to both salient objects and their surrounding elements. Our\napproach mitigates these issues by: (1) introducing a Style-Specific Masked\nDirectional CLIP Loss, which ensures consistent and aligned style\nrepresentations for both salient objects and their surroundings; and (2)\nincorporating a salient-to-key mapping mechanism for stylizing salient objects,\nfollowed by image harmonization to seamlessly blend the stylized objects with\ntheir environment. We validate the effectiveness of ObjMST through experiments,\nusing both quantitative metrics and qualitative visual evaluations of the\nstylized outputs. Our code is available at:\nhttps://github.com/chandagrover/ObjMST.\n","authors":["Chanda Grover Kamra","Indra Deep Mastan","Debayan Gupta"],"pdf_url":"https://arxiv.org/pdf/2503.04353v1.pdf","comment":"8 pages, 8 Figures, 3 Tables"},{"id":"http://arxiv.org/abs/2503.04351v1","updated":"2025-03-06T11:49:43Z","published":"2025-03-06T11:49:43Z","title":"PLMP -- Point-Line Minimal Problems for Projective SfM","summary":"  We completely classify all minimal problems for Structure-from-Motion (SfM)\nwhere arrangements of points and lines are fully observed by multiple\nuncalibrated pinhole cameras. We find 291 minimal problems, 73 of which have\nunique solutions and can thus be solved linearly. Two of the linear problems\nallow an arbitrary number of views, while all other minimal problems have at\nmost 9 cameras. All minimal problems have at most 7 points and at most 12\nlines. We compute the number of solutions of each minimal problem, as this\ngives a measurement of the problem's intrinsic difficulty, and find that these\nnumber are relatively low (e.g., when comparing with minimal problems for\ncalibrated cameras). Finally, by exploring stabilizer subgroups of\nsubarrangements, we develop a geometric and systematic way to 1) factorize\nminimal problems into smaller problems, 2) identify minimal problems in\nunderconstrained problems, and 3) formally prove non-minimality.\n","authors":["Kim Kiehn","Albin Ahlbäck","Kathlén Kohn"],"pdf_url":"https://arxiv.org/pdf/2503.04351v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04344v1","updated":"2025-03-06T11:41:36Z","published":"2025-03-06T11:41:36Z","title":"LEDiT: Your Length-Extrapolatable Diffusion Transformer without\n  Positional Encoding","summary":"  Diffusion transformers(DiTs) struggle to generate images at resolutions\nhigher than their training resolutions. The primary obstacle is that the\nexplicit positional encodings(PE), such as RoPE, need extrapolation which\ndegrades performance when the inference resolution differs from training. In\nthis paper, we propose a Length-Extrapolatable Diffusion Transformer(LEDiT), a\nsimple yet powerful architecture to overcome this limitation. LEDiT needs no\nexplicit PEs, thereby avoiding extrapolation. The key innovations of LEDiT are\nintroducing causal attention to implicitly impart global positional information\nto tokens, while enhancing locality to precisely distinguish adjacent tokens.\nExperiments on 256x256 and 512x512 ImageNet show that LEDiT can scale the\ninference resolution to 512x512 and 1024x1024, respectively, while achieving\nbetter image quality compared to current state-of-the-art length extrapolation\nmethods(NTK-aware, YaRN). Moreover, LEDiT achieves strong extrapolation\nperformance with just 100K steps of fine-tuning on a pretrained DiT,\ndemonstrating its potential for integration into existing text-to-image DiTs.\n","authors":["Shen Zhang","Yaning Tan","Siyuan Liang","Linze Li","Ge Wu","Yuhao Chen","Shuheng Li","Zhenyu Zhao","Caihua Chen","Jiajun Liang","Yao Tang"],"pdf_url":"https://arxiv.org/pdf/2503.04344v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03644v2","updated":"2025-03-06T11:36:33Z","published":"2025-03-05T16:20:53Z","title":"DongbaMIE: A Multimodal Information Extraction Dataset for Evaluating\n  Semantic Understanding of Dongba Pictograms","summary":"  Dongba pictographs are the only pictographs still in use in the world. They\nhave pictorial ideographic features, and their symbols carry rich cultural and\ncontextual information. Due to the lack of relevant datasets, existing research\nhas difficulty in advancing the study of semantic understanding of Dongba\npictographs. To this end, we propose DongbaMIE, the first multimodal dataset\nfor semantic understanding and extraction of Dongba pictographs. The dataset\nconsists of Dongba pictograph images and their corresponding Chinese semantic\nannotations. It contains 23,530 sentence-level and 2,539 paragraph-level\nimages, covering four semantic dimensions: objects, actions, relations, and\nattributes. We systematically evaluate the GPT-4o, Gemini-2.0, and Qwen2-VL\nmodels. Experimental results show that the F1 scores of GPT-4o and Gemini in\nthe best object extraction are only 3.16 and 3.11 respectively. The F1 score of\nQwen2-VL after supervised fine-tuning is only 11.49. These results suggest that\ncurrent large multimodal models still face significant challenges in accurately\nrecognizing the diverse semantic information in Dongba pictographs. The dataset\ncan be obtained from this URL.\n","authors":["Xiaojun Bi","Shuo Li","Ziyue Wang","Fuwen Luo","Weizheng Qiao","Lu Han","Ziwei Sun","Peng Li","Yang Liu"],"pdf_url":"https://arxiv.org/pdf/2503.03644v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04333v1","updated":"2025-03-06T11:31:08Z","published":"2025-03-06T11:31:08Z","title":"GaussianVideo: Efficient Video Representation and Compression by\n  Gaussian Splatting","summary":"  Implicit Neural Representation for Videos (NeRV) has introduced a novel\nparadigm for video representation and compression, outperforming traditional\ncodecs. As model size grows, however, slow encoding and decoding speed and high\nmemory consumption hinder its application in practice. To address these\nlimitations, we propose a new video representation and compression method based\non 2D Gaussian Splatting to efficiently handle video data. Our proposed\ndeformable 2D Gaussian Splatting dynamically adapts the transformation of 2D\nGaussians at each frame, significantly reducing memory cost. Equipped with a\nmulti-plane-based spatiotemporal encoder and a lightweight decoder, it predicts\nchanges in color, coordinates, and shape of initialized Gaussians, given the\ntime step. By leveraging temporal gradients, our model effectively captures\ntemporal redundancy at negligible cost, significantly enhancing video\nrepresentation efficiency. Our method reduces GPU memory usage by up to 78.4%,\nand significantly expedites video processing, achieving 5.5x faster training\nand 12.5x faster decoding compared to the state-of-the-art NeRV methods.\n","authors":["Inseo Lee","Youngyoon Choi","Joonseok Lee"],"pdf_url":"https://arxiv.org/pdf/2503.04333v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04325v1","updated":"2025-03-06T11:18:22Z","published":"2025-03-06T11:18:22Z","title":"GBT-SAM: A Parameter-Efficient Depth-Aware Model for Generalizable Brain\n  tumour Segmentation on mp-MRI","summary":"  Gliomas are brain tumours that stand out for their highly lethal and\naggressive nature, which demands a precise approach in their diagnosis. Medical\nimage segmentation plays a crucial role in the evaluation and follow-up of\nthese tumours, allowing specialists to analyse their morphology. However,\nexisting methods for automatic glioma segmentation often lack generalization\ncapability across other brain tumour domains, require extensive computational\nresources, or fail to fully utilize the multi-parametric MRI (mp-MRI) data used\nto delineate them. In this work, we introduce GBT-SAM, a novel Generalizable\nBrain Tumour (GBT) framework that extends the Segment Anything Model (SAM) to\nbrain tumour segmentation tasks. Our method employs a two-step training\nprotocol: first, fine-tuning the patch embedding layer to process the entire\nmp-MRI modalities, and second, incorporating parameter-efficient LoRA blocks\nand a Depth-Condition block into the Vision Transformer (ViT) to capture\ninter-slice correlations. GBT-SAM achieves state-of-the-art performance on the\nAdult Glioma dataset (Dice Score of $93.54$) while demonstrating robust\ngeneralization across Meningioma, Pediatric Glioma, and Sub-Saharan Glioma\ndatasets. Furthermore, GBT-SAM uses less than 6.5M trainable parameters, thus\noffering an efficient solution for brain tumour segmentation. \\\\ Our code and\nmodels are available at https://github.com/vpulab/med-sam-brain .\n","authors":["Cecilia Diana-Albelda","Roberto Alcover-Couso","Álvaro García-Martín","Jesus Bescos","Marcos Escudero-Viñolo"],"pdf_url":"https://arxiv.org/pdf/2503.04325v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.17634v2","updated":"2025-03-06T11:17:31Z","published":"2025-01-29T13:11:21Z","title":"Federated Learning With Individualized Privacy Through Client Sampling","summary":"  With growing concerns about user data collection, individualized privacy has\nemerged as a promising solution to balance protection and utility by accounting\nfor diverse user privacy preferences. Instead of enforcing a uniform level of\nanonymization for all users, this approach allows individuals to choose privacy\nsettings that align with their comfort levels. Building on this idea, we\npropose an adapted method for enabling Individualized Differential Privacy\n(IDP) in Federated Learning (FL) by handling clients according to their\npersonal privacy preferences. By extending the SAMPLE algorithm from\ncentralized settings to FL, we calculate client-specific sampling rates based\non their heterogeneous privacy budgets and integrate them into a modified\nIDP-FedAvg algorithm. We test this method under realistic privacy distributions\nand multiple datasets. The experimental results demonstrate that our approach\nachieves clear improvements over uniform DP baselines, reducing the trade-off\nbetween privacy and utility. Compared to the alternative SCALE method in\nrelated work, which assigns differing noise scales to clients, our method\nperforms notably better. However, challenges remain for complex tasks with\nnon-i.i.d. data, primarily stemming from the constraints of the decentralized\nsetting.\n","authors":["Lucas Lange","Ole Borchardt","Erhard Rahm"],"pdf_url":"https://arxiv.org/pdf/2501.17634v2.pdf","comment":"Accepted at 10th International Conference on Machine Learning\n  Technologies (ICMLT 2025)"},{"id":"http://arxiv.org/abs/2503.04322v1","updated":"2025-03-06T11:14:59Z","published":"2025-03-06T11:14:59Z","title":"A Modular Pipeline for 3D Object Tracking Using RGB Cameras","summary":"  Object tracking is a key challenge of computer vision with various\napplications that all require different architectures. Most tracking systems\nhave limitations such as constraining all movement to a 2D plane and they often\ntrack only one object. In this paper, we present a new modular pipeline that\ncalculates 3D trajectories of multiple objects. It is adaptable to various\nsettings where multiple time-synced and stationary cameras record moving\nobjects, using off the shelf webcams. Our pipeline was tested on the Table\nSetting Dataset, where participants are recorded with various sensors as they\nset a table with tableware objects. We need to track these manipulated objects,\nusing 6 rgb webcams. Challenges include: Detecting small objects in 9.874.699\ncamera frames, determining camera poses, discriminating between nearby and\noverlapping objects, temporary occlusions, and finally calculating a 3D\ntrajectory using the right subset of an average of 11.12.456 pixel coordinates\nper 3-minute trial. We implement a robust pipeline that results in accurate\ntrajectories with covariance of x,y,z-position as a confidence metric. It deals\ndynamically with appearing and disappearing objects, instantiating new Extended\nKalman Filters. It scales to hundreds of table-setting trials with very little\nhuman annotation input, even with the camera poses of each trial unknown. The\ncode is available at https://github.com/LarsBredereke/object_tracking\n","authors":["Lars Bredereke","Yale Hartmann","Tanja Schultz"],"pdf_url":"https://arxiv.org/pdf/2503.04322v1.pdf","comment":"9 pages, 11 figures, original paper not to be published anywhere else"},{"id":"http://arxiv.org/abs/2501.16981v3","updated":"2025-03-06T11:08:38Z","published":"2025-01-28T14:28:55Z","title":"Modulating CNN Features with Pre-Trained ViT Representations for\n  Open-Vocabulary Object Detection","summary":"  Owing to large-scale image-text contrastive training, pre-trained vision\nlanguage model (VLM) like CLIP shows superior open-vocabulary recognition\nability. Most existing open-vocabulary object detectors attempt to utilize the\npre-trained VLMs to attain generalized representation. F-ViT uses the\npre-trained visual encoder as the backbone network and freezes it during\ntraining. However, its frozen backbone doesn't benefit from the labeled data to\nstrengthen the representation for detection. Therefore, we propose a novel\ntwo-branch backbone network, named as \\textbf{V}iT-Feature-\\textbf{M}odulated\nMulti-Scale \\textbf{C}onvolutional Network (VMCNet), which consists of a\ntrainable convolutional branch, a frozen pre-trained ViT branch and a VMC\nmodule. The trainable CNN branch could be optimized with labeled data while the\nfrozen pre-trained ViT branch could keep the representation ability derived\nfrom large-scale pre-training. Then, the proposed VMC module could modulate the\nmulti-scale CNN features with the representations from ViT branch. With this\nproposed mixed structure, the detector is more likely to discover objects of\nnovel categories. Evaluated on two popular benchmarks, our method boosts the\ndetection performance on novel category and outperforms state-of-the-art\nmethods. On OV-COCO, the proposed method achieves 44.3\nAP$_{50}^{\\mathrm{novel}}$ with ViT-B/16 and 48.5 AP$_{50}^{\\mathrm{novel}}$\nwith ViT-L/14. On OV-LVIS, VMCNet with ViT-B/16 and ViT-L/14 reaches 27.8 and\n38.4 mAP$_{r}$.\n","authors":["Xiangyu Gao","Yu Dai","Benliu Qiu","Lanxiao Wang","Heqian Qiu","Hongliang Li"],"pdf_url":"https://arxiv.org/pdf/2501.16981v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.01879v3","updated":"2025-03-06T11:05:33Z","published":"2024-02-02T20:08:11Z","title":"$σ$-zero: Gradient-based Optimization of $\\ell_0$-norm Adversarial\n  Examples","summary":"  Evaluating the adversarial robustness of deep networks to gradient-based\nattacks is challenging. While most attacks consider $\\ell_2$- and\n$\\ell_\\infty$-norm constraints to craft input perturbations, only a few\ninvestigate sparse $\\ell_1$- and $\\ell_0$-norm attacks. In particular,\n$\\ell_0$-norm attacks remain the least studied due to the inherent complexity\nof optimizing over a non-convex and non-differentiable constraint. However,\nevaluating adversarial robustness under these attacks could reveal weaknesses\notherwise left untested with more conventional $\\ell_2$- and $\\ell_\\infty$-norm\nattacks. In this work, we propose a novel $\\ell_0$-norm attack, called\n$\\sigma$-zero, which leverages a differentiable approximation of the $\\ell_0$\nnorm to facilitate gradient-based optimization, and an adaptive projection\noperator to dynamically adjust the trade-off between loss minimization and\nperturbation sparsity. Extensive evaluations using MNIST, CIFAR10, and ImageNet\ndatasets, involving robust and non-robust models, show that\n$\\sigma$\\texttt{-zero} finds minimum $\\ell_0$-norm adversarial examples without\nrequiring any time-consuming hyperparameter tuning, and that it outperforms all\ncompeting sparse attacks in terms of success rate, perturbation size, and\nefficiency.\n","authors":["Antonio Emanuele Cinà","Francesco Villani","Maura Pintor","Lea Schönherr","Battista Biggio","Marcello Pelillo"],"pdf_url":"https://arxiv.org/pdf/2402.01879v3.pdf","comment":"Paper accepted at International Conference on Learning\n  Representations (ICLR 2025). Code available at\n  https://github.com/sigma0-advx/sigma-zero"},{"id":"http://arxiv.org/abs/2412.00156v3","updated":"2025-03-06T11:05:32Z","published":"2024-11-29T08:10:49Z","title":"VISION-XL: High Definition Video Inverse Problem Solver using Latent\n  Image Diffusion Models","summary":"  In this paper, we propose a novel framework for solving high-definition video\ninverse problems using latent image diffusion models. Building on recent\nadvancements in spatio-temporal optimization for video inverse problems using\nimage diffusion models, our approach leverages latent-space diffusion models to\nachieve enhanced video quality and resolution. To address the high\ncomputational demands of processing high-resolution frames, we introduce a\npseudo-batch consistent sampling strategy, allowing efficient operation on a\nsingle GPU. Additionally, to improve temporal consistency, we present\npseudo-batch inversion, an initialization technique that incorporates\ninformative latents from the measurement. By integrating with SDXL, our\nframework achieves state-of-the-art video reconstruction across a wide range of\nspatio-temporal inverse problems, including complex combinations of frame\naveraging and various spatial degradations, such as deblurring,\nsuper-resolution, and inpainting. Unlike previous methods, our approach\nsupports multiple aspect ratios (landscape, vertical, and square) and delivers\nHD-resolution reconstructions (exceeding 1280x720) in under 6 seconds per frame\non a single NVIDIA 4090 GPU.\n","authors":["Taesung Kwon","Jong Chul Ye"],"pdf_url":"https://arxiv.org/pdf/2412.00156v3.pdf","comment":"Project page: https://vision-xl.github.io/"},{"id":"http://arxiv.org/abs/2501.10814v2","updated":"2025-03-06T11:05:23Z","published":"2025-01-18T16:23:09Z","title":"No More Sliding Window: Efficient 3D Medical Image Segmentation with\n  Differentiable Top-k Patch Sampling","summary":"  3D models surpass 2D models in CT/MRI segmentation by effectively capturing\ninter-slice relationships. However, the added depth dimension substantially\nincreases memory consumption. While patch-based training alleviates memory\nconstraints, it significantly slows down the inference speed due to the sliding\nwindow (SW) approach. We propose No-More-Sliding-Window (NMSW), a novel\nend-to-end trainable framework that enhances the efficiency of generic 3D\nsegmentation backbone during an inference step by eliminating the need for SW.\nNMSW employs a differentiable Top-k module to selectively sample only the most\nrelevant patches, thereby minimizing redundant computations. When patch-level\npredictions are insufficient, the framework intelligently leverages coarse\nglobal predictions to refine results. Evaluated across 3 tasks using 3\nsegmentation backbones, NMSW achieves competitive accuracy compared to SW\ninference while significantly reducing computational complexity by 91% (88.0 to\n8.00 TMACs). Moreover, it delivers a 9.1x faster inference on the H100 GPU\n(99.0 to 8.3 sec) and a 11.1x faster inference on the Xeon Gold CPU (2110 to\n189 sec). NMSW is model-agnostic, further boosting efficiency when integrated\nwith any existing efficient segmentation backbones.\n","authors":["Young Seok Jeon","Hongfei Yang","Huazhu Fu","Mengling Feng"],"pdf_url":"https://arxiv.org/pdf/2501.10814v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04314v1","updated":"2025-03-06T10:58:26Z","published":"2025-03-06T10:58:26Z","title":"S2Gaussian: Sparse-View Super-Resolution 3D Gaussian Splatting","summary":"  In this paper, we aim ambitiously for a realistic yet challenging problem,\nnamely, how to reconstruct high-quality 3D scenes from sparse low-resolution\nviews that simultaneously suffer from deficient perspectives and clarity.\nWhereas existing methods only deal with either sparse views or low-resolution\nobservations, they fail to handle such hybrid and complicated scenarios. To\nthis end, we propose a novel Sparse-view Super-resolution 3D Gaussian Splatting\nframework, dubbed S2Gaussian, that can reconstruct structure-accurate and\ndetail-faithful 3D scenes with only sparse and low-resolution views. The\nS2Gaussian operates in a two-stage fashion. In the first stage, we initially\noptimize a low-resolution Gaussian representation with depth regularization and\ndensify it to initialize the high-resolution Gaussians through a tailored\nGaussian Shuffle Split operation. In the second stage, we refine the\nhigh-resolution Gaussians with the super-resolved images generated from both\noriginal sparse views and pseudo-views rendered by the low-resolution\nGaussians. In which a customized blur-free inconsistency modeling scheme and a\n3D robust optimization strategy are elaborately designed to mitigate multi-view\ninconsistency and eliminate erroneous updates caused by imperfect supervision.\nExtensive experiments demonstrate superior results and in particular\nestablishing new state-of-the-art performances with more consistent geometry\nand finer details.\n","authors":["Yecong Wan","Mingwen Shao","Yuanshuo Cheng","Wangmeng Zuo"],"pdf_url":"https://arxiv.org/pdf/2503.04314v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2503.04308v1","updated":"2025-03-06T10:51:04Z","published":"2025-03-06T10:51:04Z","title":"Shaken, Not Stirred: A Novel Dataset for Visual Understanding of Glasses\n  in Human-Robot Bartending Tasks","summary":"  Datasets for object detection often do not account for enough variety of\nglasses, due to their transparent and reflective properties. Specifically,\nopen-vocabulary object detectors, widely used in embodied robotic agents, fail\nto distinguish subclasses of glasses. This scientific gap poses an issue to\nrobotic applications that suffer from accumulating errors between detection,\nplanning, and action execution. The paper introduces a novel method for the\nacquisition of real-world data from RGB-D sensors that minimizes human effort.\nWe propose an auto-labeling pipeline that generates labels for all the acquired\nframes based on the depth measurements. We provide a novel real-world glass\nobject dataset that was collected on the Neuro-Inspired COLlaborator (NICOL), a\nhumanoid robot platform. The data set consists of 7850 images recorded from\nfive different cameras. We show that our trained baseline model outperforms\nstate-of-the-art open-vocabulary approaches. In addition, we deploy our\nbaseline model in an embodied agent approach to the NICOL platform, on which it\nachieves a success rate of 81% in a human-robot bartending scenario.\n","authors":["Lukáš Gajdošech","Hassan Ali","Jan-Gerrit Habekost","Martin Madaras","Matthias Kerzel","Stefan Wermter"],"pdf_url":"https://arxiv.org/pdf/2503.04308v1.pdf","comment":"Submitted to IEEE/RSJ International Conference on Intelligent Robots\n  and Systems (IROS) 2025"},{"id":"http://arxiv.org/abs/2412.01615v3","updated":"2025-03-06T10:49:58Z","published":"2024-12-02T15:38:44Z","title":"OmniGuard: Hybrid Manipulation Localization via Augmented Versatile Deep\n  Image Watermarking","summary":"  With the rapid growth of generative AI and its widespread application in\nimage editing, new risks have emerged regarding the authenticity and integrity\nof digital content. Existing versatile watermarking approaches suffer from\ntrade-offs between tamper localization precision and visual quality.\nConstrained by the limited flexibility of previous framework, their localized\nwatermark must remain fixed across all images. Under AIGC-editing, their\ncopyright extraction accuracy is also unsatisfactory. To address these\nchallenges, we propose OmniGuard, a novel augmented versatile watermarking\napproach that integrates proactive embedding with passive, blind extraction for\nrobust copyright protection and tamper localization. OmniGuard employs a hybrid\nforensic framework that enables flexible localization watermark selection and\nintroduces a degradation-aware tamper extraction network for precise\nlocalization under challenging conditions. Additionally, a lightweight\nAIGC-editing simulation layer is designed to enhance robustness across global\nand local editing. Extensive experiments show that OmniGuard achieves superior\nfidelity, robustness, and flexibility. Compared to the recent state-of-the-art\napproach EditGuard, our method outperforms it by 4.25dB in PSNR of the\ncontainer image, 20.7% in F1-Score under noisy conditions, and 14.8% in average\nbit accuracy.\n","authors":["Xuanyu Zhang","Zecheng Tang","Zhipei Xu","Runyi Li","Youmin Xu","Bin Chen","Feng Gao","Jian Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.01615v3.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.03370v2","updated":"2025-03-06T10:41:28Z","published":"2025-03-05T10:46:03Z","title":"MIAdapt: Source-free Few-shot Domain Adaptive Object Detection for\n  Microscopic Images","summary":"  Existing generic unsupervised domain adaptation approaches require access to\nboth a large labeled source dataset and a sufficient unlabeled target dataset\nduring adaptation. However, collecting a large dataset, even if unlabeled, is a\nchallenging and expensive endeavor, especially in medical imaging. In addition,\nconstraints such as privacy issues can result in cases where source data is\nunavailable. Taking in consideration these challenges, we propose MIAdapt, an\nadaptive approach for Microscopic Imagery Adaptation as a solution for\nSource-free Few-shot Domain Adaptive Object detection (SF-FSDA). We also define\ntwo competitive baselines (1) Faster-FreeShot and (2) MT-FreeShot. Extensive\nexperiments on the challenging M5-Malaria and Raabin-WBC datasets validate the\neffectiveness of MIAdapt. Without using any image from the source domain\nMIAdapt surpasses state-of-the-art source-free UDA (SF-UDA) methods by +21.3%\nmAP and few-shot domain adaptation (FSDA) approaches by +4.7% mAP on\nRaabin-WBC. Our code and models will be publicly available.\n","authors":["Nimra Dilawar","Sara Nadeem","Javed Iqbal","Waqas Sultani","Mohsen Ali"],"pdf_url":"https://arxiv.org/pdf/2503.03370v2.pdf","comment":"6 pages, 5 figures"},{"id":"http://arxiv.org/abs/2312.03286v2","updated":"2025-03-06T10:12:31Z","published":"2023-12-06T04:32:38Z","title":"Indirect Gradient Matching for Adversarial Robust Distillation","summary":"  Adversarial training significantly improves adversarial robustness, but\nsuperior performance is primarily attained with large models. This substantial\nperformance gap for smaller models has spurred active research into adversarial\ndistillation (AD) to mitigate the difference. Existing AD methods leverage the\nteacher's logits as a guide. In contrast to these approaches, we aim to\ntransfer another piece of knowledge from the teacher, the input gradient. In\nthis paper, we propose a distillation module termed Indirect Gradient\nDistillation Module (IGDM) that indirectly matches the student's input gradient\nwith that of the teacher. Experimental results show that IGDM seamlessly\nintegrates with existing AD methods, significantly enhancing their performance.\nParticularly, utilizing IGDM on the CIFAR-100 dataset improves the AutoAttack\naccuracy from 28.06% to 30.32% with the ResNet-18 architecture and from 26.18%\nto 29.32% with the MobileNetV2 architecture when integrated into the SOTA\nmethod without additional data augmentation.\n","authors":["Hongsin Lee","Seungju Cho","Changick Kim"],"pdf_url":"https://arxiv.org/pdf/2312.03286v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04268v1","updated":"2025-03-06T09:57:26Z","published":"2025-03-06T09:57:26Z","title":"ControlFill: Spatially Adjustable Image Inpainting from Prompt Learning","summary":"  In this report, I present an inpainting framework named \\textit{ControlFill},\nwhich involves training two distinct prompts: one for generating plausible\nobjects within a designated mask (\\textit{creation}) and another for filling\nthe region by extending the background (\\textit{removal}). During the inference\nstage, these learned embeddings guide a diffusion network that operates without\nrequiring heavy text encoders. By adjusting the relative significance of the\ntwo prompts and employing classifier-free guidance, users can control the\nintensity of removal or creation. Furthermore, I introduce a method to\nspatially vary the intensity of guidance by assigning different scales to\nindividual pixels.\n","authors":["Boseong Jeon"],"pdf_url":"https://arxiv.org/pdf/2503.04268v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.18672v4","updated":"2025-03-06T09:55:41Z","published":"2025-01-30T18:51:54Z","title":"Drag Your Gaussian: Effective Drag-Based Editing with Score Distillation\n  for 3D Gaussian Splatting","summary":"  Recent advancements in 3D scene editing have been propelled by the rapid\ndevelopment of generative models. Existing methods typically utilize generative\nmodels to perform text-guided editing on 3D representations, such as 3D\nGaussian Splatting (3DGS). However, these methods are often limited to texture\nmodifications and fail when addressing geometric changes, such as editing a\ncharacter's head to turn around. Moreover, such methods lack accurate control\nover the spatial position of editing results, as language struggles to\nprecisely describe the extent of edits. To overcome these limitations, we\nintroduce DYG, an effective 3D drag-based editing method for 3D Gaussian\nSplatting. It enables users to conveniently specify the desired editing region\nand the desired dragging direction through the input of 3D masks and pairs of\ncontrol points, thereby enabling precise control over the extent of editing.\nDYG integrates the strengths of the implicit triplane representation to\nestablish the geometric scaffold of the editing results, effectively overcoming\nsuboptimal editing outcomes caused by the sparsity of 3DGS in the desired\nediting regions. Additionally, we incorporate a drag-based Latent Diffusion\nModel into our method through the proposed Drag-SDS loss function, enabling\nflexible, multi-view consistent, and fine-grained editing. Extensive\nexperiments demonstrate that DYG conducts effective drag-based editing guided\nby control point prompts, surpassing other baselines in terms of editing effect\nand quality, both qualitatively and quantitatively. Visit our project page at\nhttps://quyans.github.io/Drag-Your-Gaussian.\n","authors":["Yansong Qu","Dian Chen","Xinyang Li","Xiaofan Li","Shengchuan Zhang","Liujuan Cao","Rongrong Ji"],"pdf_url":"https://arxiv.org/pdf/2501.18672v4.pdf","comment":"Visit our project page at https://quyans.github.io/Drag-Your-Gaussian"},{"id":"http://arxiv.org/abs/2405.14736v2","updated":"2025-03-06T09:52:43Z","published":"2024-05-23T16:02:30Z","title":"GIFT: Unlocking Full Potential of Labels in Distilled Dataset at\n  Near-zero Cost","summary":"  Recent advancements in dataset distillation have demonstrated the significant\nbenefits of employing soft labels generated by pre-trained teacher models. In\nthis paper, we introduce a novel perspective by emphasizing the full\nutilization of labels. We first conduct a comprehensive comparison of various\nloss functions for soft label utilization in dataset distillation, revealing\nthat the model trained on the synthetic dataset exhibits high sensitivity to\nthe choice of loss function for soft label utilization. This finding highlights\nthe necessity of a universal loss function for training models on synthetic\ndatasets. Building on these insights, we introduce an extremely simple yet\nsurprisingly effective plug-and-play approach, GIFT, which encompasses soft\nlabel refinement and a cosine similarity-based loss function to efficiently\nleverage full label information. Extensive experiments indicate that GIFT\nconsistently enhances state-of-the-art dataset distillation methods across\nvarious dataset scales, without incurring additional computational costs.\nImportantly, GIFT significantly enhances cross-optimizer generalization, an\narea previously overlooked. For instance, on ImageNet-1K with IPC = 10, GIFT\nenhances the state-of-the-art method RDED by 30.8% in cross-optimizer\ngeneralization. Our code is available at https://github.com/LINs-lab/GIFT.\n","authors":["Xinyi Shang","Peng Sun","Tao Lin"],"pdf_url":"https://arxiv.org/pdf/2405.14736v2.pdf","comment":"https://github.com/LINs-lab/GIFT"},{"id":"http://arxiv.org/abs/2503.04258v1","updated":"2025-03-06T09:39:36Z","published":"2025-03-06T09:39:36Z","title":"TAIL: Text-Audio Incremental Learning","summary":"  Many studies combine text and audio to capture multi-modal information but\nthey overlook the model's generalization ability on new datasets. Introducing\nnew datasets may affect the feature space of the original dataset, leading to\ncatastrophic forgetting. Meanwhile, large model parameters can significantly\nimpact training performance. To address these limitations, we introduce a novel\ntask called Text-Audio Incremental Learning (TAIL) task for text-audio\nretrieval, and propose a new method, PTAT, Prompt Tuning for Audio-Text\nincremental learning. This method utilizes prompt tuning to optimize the model\nparameters while incorporating an audio-text similarity and feature\ndistillation module to effectively mitigate catastrophic forgetting. We\nbenchmark our method and previous incremental learning methods on AudioCaps,\nClotho, BBC Sound Effects and Audioset datasets, and our method outperforms\nprevious methods significantly, particularly demonstrating stronger resistance\nto forgetting on older datasets. Compared to the full-parameters Finetune\n(Sequential) method, our model only requires 2.42\\% of its parameters,\nachieving 4.46\\% higher performance.\n","authors":["Yingfei Sun","Xu Gu","Wei Ji","Hanbin Zhao","Hao Fei","Yifang Yin","Roger Zimmermann"],"pdf_url":"https://arxiv.org/pdf/2503.04258v1.pdf","comment":"4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2503.04257v1","updated":"2025-03-06T09:39:09Z","published":"2025-03-06T09:39:09Z","title":"How to Move Your Dragon: Text-to-Motion Synthesis for Large-Vocabulary\n  Objects","summary":"  Motion synthesis for diverse object categories holds great potential for 3D\ncontent creation but remains underexplored due to two key challenges: (1) the\nlack of comprehensive motion datasets that include a wide range of high-quality\nmotions and annotations, and (2) the absence of methods capable of handling\nheterogeneous skeletal templates from diverse objects. To address these\nchallenges, we contribute the following: First, we augment the Truebones Zoo\ndataset, a high-quality animal motion dataset covering over 70 species, by\nannotating it with detailed text descriptions, making it suitable for\ntext-based motion synthesis. Second, we introduce rig augmentation techniques\nthat generate diverse motion data while preserving consistent dynamics,\nenabling models to adapt to various skeletal configurations. Finally, we\nredesign existing motion diffusion models to dynamically adapt to arbitrary\nskeletal templates, enabling motion synthesis for a diverse range of objects\nwith varying structures. Experiments show that our method learns to generate\nhigh-fidelity motions from textual descriptions for diverse and even unseen\nobjects, setting a strong foundation for motion synthesis across diverse object\ncategories and skeletal templates. Qualitative results are available on this\nlink: t2m4lvo.github.io\n","authors":["Wonkwang Lee","Jongwon Jeong","Taehong Moon","Hyeon-Jong Kim","Jaehyeon Kim","Gunhee Kim","Byeong-Uk Lee"],"pdf_url":"https://arxiv.org/pdf/2503.04257v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04250v1","updated":"2025-03-06T09:33:46Z","published":"2025-03-06T09:33:46Z","title":"An Egocentric Vision-Language Model based Portable Real-time Smart\n  Assistant","summary":"  We present Vinci, a vision-language system designed to provide real-time,\ncomprehensive AI assistance on portable devices. At its core, Vinci leverages\nEgoVideo-VL, a novel model that integrates an egocentric vision foundation\nmodel with a large language model (LLM), enabling advanced functionalities such\nas scene understanding, temporal grounding, video summarization, and future\nplanning. To enhance its utility, Vinci incorporates a memory module for\nprocessing long video streams in real time while retaining contextual history,\na generation module for producing visual action demonstrations, and a retrieval\nmodule that bridges egocentric and third-person perspectives to provide\nrelevant how-to videos for skill acquisition. Unlike existing systems that\noften depend on specialized hardware, Vinci is hardware-agnostic, supporting\ndeployment across a wide range of devices, including smartphones and wearable\ncameras. In our experiments, we first demonstrate the superior performance of\nEgoVideo-VL on multiple public benchmarks, showcasing its vision-language\nreasoning and contextual understanding capabilities. We then conduct a series\nof user studies to evaluate the real-world effectiveness of Vinci, highlighting\nits adaptability and usability in diverse scenarios. We hope Vinci can\nestablish a new framework for portable, real-time egocentric AI systems,\nempowering users with contextual and actionable insights. Including the\nfrontend, backend, and models, all codes of Vinci are available at\nhttps://github.com/OpenGVLab/vinci.\n","authors":["Yifei Huang","Jilan Xu","Baoqi Pei","Yuping He","Guo Chen","Mingfang Zhang","Lijin Yang","Zheng Nie","Jinyao Liu","Guoshun Fan","Dechen Lin","Fang Fang","Kunpeng Li","Chang Yuan","Xinyuan Chen","Yaohui Wang","Yali Wang","Yu Qiao","Limin Wang"],"pdf_url":"https://arxiv.org/pdf/2503.04250v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07481v5","updated":"2025-03-06T09:26:33Z","published":"2024-12-10T13:03:42Z","title":"Manta: Enhancing Mamba for Few-Shot Action Recognition of Long\n  Sub-Sequence","summary":"  In few-shot action recognition (FSAR), long sub-sequences of video naturally\nexpress entire actions more effectively. However, the high computational\ncomplexity of mainstream Transformer-based methods limits their application.\nRecent Mamba demonstrates efficiency in modeling long sequences, but directly\napplying Mamba to FSAR overlooks the importance of local feature modeling and\nalignment. Moreover, long sub-sequences within the same class accumulate\nintra-class variance, which adversely impacts FSAR performance. To solve these\nchallenges, we propose a Matryoshka MAmba and CoNtrasTive LeArning framework\n(Manta). Firstly, the Matryoshka Mamba introduces multiple Inner Modules to\nenhance local feature representation, rather than directly modeling global\nfeatures. An Outer Module captures dependencies of timeline between these local\nfeatures for implicit temporal alignment. Secondly, a hybrid contrastive\nlearning paradigm, combining both supervised and unsupervised methods, is\ndesigned to mitigate the negative effects of intra-class variance accumulation.\nThe Matryoshka Mamba and the hybrid contrastive learning paradigm operate in\ntwo parallel branches within Manta, enhancing Mamba for FSAR of long\nsub-sequence. Manta achieves new state-of-the-art performance on prominent\nbenchmarks, including SSv2, Kinetics, UCF101, and HMDB51. Extensive empirical\nstudies prove that Manta significantly improves FSAR of long sub-sequence from\nmultiple perspectives.\n","authors":["Wenbo Huang","Jinghui Zhang","Guang Li","Lei Zhang","Shuoyuan Wang","Fang Dong","Jiahui Jin","Takahiro Ogawa","Miki Haseyama"],"pdf_url":"https://arxiv.org/pdf/2412.07481v5.pdf","comment":"Accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2408.09110v3","updated":"2025-03-06T09:26:00Z","published":"2024-08-17T06:24:43Z","title":"Locate Anything on Earth: Advancing Open-Vocabulary Object Detection for\n  Remote Sensing Community","summary":"  Object detection, particularly open-vocabulary object detection, plays a\ncrucial role in Earth sciences, such as environmental monitoring, natural\ndisaster assessment, and land-use planning. However, existing open-vocabulary\ndetectors, primarily trained on natural-world images, struggle to generalize to\nremote sensing images due to a significant data domain gap. Thus, this paper\naims to advance the development of open-vocabulary object detection in remote\nsensing community. To achieve this, we first reformulate the task as Locate\nAnything on Earth (LAE) with the goal of detecting any novel concepts on Earth.\nWe then developed the LAE-Label Engine which collects, auto-annotates, and\nunifies up to 10 remote sensing datasets creating the LAE-1M - the first\nlarge-scale remote sensing object detection dataset with broad category\ncoverage. Using the LAE-1M, we further propose and train the novel LAE-DINO\nModel, the first open-vocabulary foundation object detector for the LAE task,\nfeaturing Dynamic Vocabulary Construction (DVC) and Visual-Guided Text Prompt\nLearning (VisGT) modules. DVC dynamically constructs vocabulary for each\ntraining batch, while VisGT maps visual features to semantic space, enhancing\ntext features. We comprehensively conduct experiments on established remote\nsensing benchmark DIOR, DOTAv2.0, as well as our newly introduced 80-class\nLAE-80C benchmark. Results demonstrate the advantages of the LAE-1M dataset and\nthe effectiveness of the LAE-DINO method.\n","authors":["Jiancheng Pan","Yanxing Liu","Yuqian Fu","Muyuan Ma","Jiahao Li","Danda Pani Paudel","Luc Van Gool","Xiaomeng Huang"],"pdf_url":"https://arxiv.org/pdf/2408.09110v3.pdf","comment":"15 pages, 11 figures"},{"id":"http://arxiv.org/abs/2503.00168v2","updated":"2025-03-06T09:23:35Z","published":"2025-02-28T20:30:56Z","title":"SSL4EO-S12 v1.1: A Multimodal, Multiseasonal Dataset for Pretraining,\n  Updated","summary":"  This technical report presents SSL4EO-S12 v1.1, a multimodal, multitemporal\nEarth Observation dataset designed for pretraining large-scale foundation\nmodels. Building on the success of SSL4EO-S12 v1.0, the new version addresses\nthe previous challenges of data misalignment and a limited data structure for\nlow-barrier, analysis-ready EO processing. SSL4EO-S12 v1.1 covers the world's\n10,000 largest cities and its surroundings within a 50 km radius across four\nseasons, resulting in a diverse collection of nearly one million patches.\nSSL4EO-S12 v1.1 packages the data in Zarr file format for cloud-efficient\nloading and representation of meta-information such as including cloud masks\nand geolocation. Released under the CC-BY-4.0 license, SSL4EO-S12 v1.1\nfacilitates open research and provides a robust foundation for future\nadvancements in self-supervised learning and geospatial analysis. The dataset\nis available online through https://datapub.fz-juelich.de/ssl4eo-s12, and we\nprovided additional resources at https://github.com/DLR-MF-DAS/SSL4EO-S12-v1.1.\n","authors":["Benedikt Blumenstiel","Nassim Ait Ali Braham","Conrad M Albrecht","Stefano Maurogiovanni","Paolo Fraccaro"],"pdf_url":"https://arxiv.org/pdf/2503.00168v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04235v1","updated":"2025-03-06T09:15:13Z","published":"2025-03-06T09:15:13Z","title":"Geometry-Constrained Monocular Scale Estimation Using Semantic\n  Segmentation for Dynamic Scenes","summary":"  Monocular visual localization plays a pivotal role in advanced driver\nassistance systems and autonomous driving by estimating a vehicle's ego-motion\nfrom a single pinhole camera. Nevertheless, conventional monocular visual\nodometry encoun-ters challenges in scale estimation due to the absence of depth\ninformation during projection. Previous methodologies, whether rooted in\nphysical constraints or deep learning paradigms, con-tend with issues related\nto computational complexity and the management of dynamic objects. This study\nextends our prior research, presenting innovative strategies for ego-motion\nestima-tion and the selection of ground points. Striving for a nuanced\nequilibrium between computational efficiency and precision, we propose a hybrid\nmethod that leverages the SegNeXt model for real-time applications,\nencompassing both ego-motion estimation and ground point selection. Our\nmethodology incorporates dy-namic object masks to eliminate unstable features\nand employs ground plane masks for meticulous triangulation. Furthermore, we\nexploit Geometry-constraint to delineate road regions for scale recovery. The\nintegration of this approach with the mo-nocular version of ORB-SLAM3\nculminates in the accurate esti-mation of a road model, a pivotal component in\nour scale recov-ery process. Rigorous experiments, conducted on the KITTI\nda-taset, systematically compare our method with existing monocu-lar visual\nodometry algorithms and contemporary scale recovery methodologies. The results\nundeniably confirm the superior ef-fectiveness of our approach, surpassing\nstate-of-the-art visual odometry algorithms. Our source code is available at\nhttps://git hub.com/bFr0zNq/MVOSegScale.\n","authors":["Hui Zhang","Zhiyang Wu","Qianqian Shangguan","Kang An"],"pdf_url":"https://arxiv.org/pdf/2503.04235v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07076v2","updated":"2025-03-06T09:13:28Z","published":"2024-11-11T15:51:48Z","title":"StoryTeller: Improving Long Video Description through Global\n  Audio-Visual Character Identification","summary":"  Existing large vision-language models (LVLMs) are largely limited to\nprocessing short, seconds-long videos and struggle with generating coherent\ndescriptions for extended video spanning minutes or more. Long video\ndescription introduces new challenges, such as consistent character\nidentification and plot-level descriptions incorporating both visual and audio\ninformation. To address these, we figure out audio-visual character\nidentification, matching character names to each dialogue, as a key factor. We\npropose StoryTeller, a system for generating dense descriptions of long videos,\nincorporating both low-level visual concepts and high-level plot information.\nStoryTeller uses a multimodal large language model that integrates visual,\naudio, and text modalities to perform audio-visual character identification on\nminute-long video clips. The results are then fed into a LVLM to enhance\nconsistency of video description. We validate our approach on movie description\ntasks and introduce MovieStory101, a dataset with dense descriptions for\nthree-minute movie clips. To evaluate long video descriptions, we create\nStoryQA, a large set of multiple-choice questions for MovieStory101 test set.\nWe assess descriptions by inputting them into GPT-4 to answer these questions,\nusing accuracy as an automatic evaluation metric. Experiments show that\nStoryTeller outperforms all open and closed-source baselines on StoryQA,\nachieving 9.5% higher accuracy than the strongest baseline, Gemini-1.5-pro, and\ndemonstrating a +15.56% advantage in human side-by-side evaluations.\nAdditionally, incorporating audio-visual character identification from\nStoryTeller improves the performance of all video description models, with\nGemini-1.5-pro and GPT-4o showing relative improvement of 5.5% and 13.0%,\nrespectively, in accuracy on StoryQA.\n","authors":["Yichen He","Yuan Lin","Jianchao Wu","Hanchong Zhang","Yuchen Zhang","Ruicheng Le"],"pdf_url":"https://arxiv.org/pdf/2411.07076v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.09263v3","updated":"2025-03-06T09:10:07Z","published":"2024-11-14T08:02:14Z","title":"Rethinking Weight-Averaged Model-merging","summary":"  Model-merging has emerged as a powerful approach in deep learning, capable of\nenhancing model performance without any training. However, the underlying\nmechanisms that explain its effectiveness remain largely unexplored. In this\npaper, we investigate this technique from three novel perspectives to\nempirically provide deeper insights into why and how weight-averaged\nmodel-merging works: (1) we examine the intrinsic patterns captured by the\nlearning of the model weights, through the visualizations of their patterns on\nseveral datasets, showing that these weights often encode structured and\ninterpretable patterns and that is the essential why model-merging can work;\n(2) we mathematically and empirically investigate model ensemble merging\nstrategies based on averaging on weights versus averaging on features,\nproviding detailed analyses across diverse architectures and datasets; and (3)\nwe explore the impact on model-merging prediction stability in terms of\nchanging the parameter magnitude, revealing insights into the way of weight\naveraging works as regularization by showing the robustness across different\nparameter scales. Our findings shed light on the \"black box\" of weight-averaged\nmodel-merging, offering valuable insights and practical recommendations that\nadvance the model-merging process. The code is available at\nhttps://github.com/billhhh/Rethink-Merge.\n","authors":["Hu Wang","Congbo Ma","Ibrahim Almakky","Ian Reid","Gustavo Carneiro","Mohammad Yaqub"],"pdf_url":"https://arxiv.org/pdf/2411.09263v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04229v1","updated":"2025-03-06T09:09:18Z","published":"2025-03-06T09:09:18Z","title":"Synthetic Data is an Elegant GIFT for Continual Vision-Language Models","summary":"  Pre-trained Vision-Language Models (VLMs) require Continual Learning (CL) to\nefficiently update their knowledge and adapt to various downstream tasks\nwithout retraining from scratch. However, for VLMs, in addition to the loss of\nknowledge previously learned from downstream tasks, pre-training knowledge is\nalso corrupted during continual fine-tuning. This issue is exacerbated by the\nunavailability of original pre-training data, leaving VLM's generalization\nability degrading. In this paper, we propose GIFT, a novel continual\nfine-tuning approach that utilizes synthetic data to overcome catastrophic\nforgetting in VLMs. Taking advantage of recent advances in text-to-image\nsynthesis, we employ a pre-trained diffusion model to recreate both\npre-training and learned downstream task data. In this way, the VLM can revisit\nprevious knowledge through distillation on matching diffusion-generated images\nand corresponding text prompts. Leveraging the broad distribution and high\nalignment between synthetic image-text pairs in VLM's feature space, we propose\na contrastive distillation loss along with an image-text alignment constraint.\nTo further combat in-distribution overfitting and enhance distillation\nperformance with limited amount of generated data, we incorporate adaptive\nweight consolidation, utilizing Fisher information from these synthetic\nimage-text pairs and achieving a better stability-plasticity balance. Extensive\nexperiments demonstrate that our method consistently outperforms previous\nstate-of-the-art approaches across various settings.\n","authors":["Bin Wu","Wuxuan Shi","Jinqiao Wang","Mang Ye"],"pdf_url":"https://arxiv.org/pdf/2503.04229v1.pdf","comment":"This work is accepted by CVPR 2025. Modifications may be performed"},{"id":"http://arxiv.org/abs/2503.04223v1","updated":"2025-03-06T09:06:06Z","published":"2025-03-06T09:06:06Z","title":"Spiking Meets Attention: Efficient Remote Sensing Image Super-Resolution\n  with Attention Spiking Neural Networks","summary":"  Spiking neural networks (SNNs) are emerging as a promising alternative to\ntraditional artificial neural networks (ANNs), offering biological plausibility\nand energy efficiency. Despite these merits, SNNs are frequently hampered by\nlimited capacity and insufficient representation power, yet remain\nunderexplored in remote sensing super-resolution (SR) tasks. In this paper, we\nfirst observe that spiking signals exhibit drastic intensity variations across\ndiverse textures, highlighting an active learning state of the neurons. This\nobservation motivates us to apply SNNs for efficient SR of RSIs. Inspired by\nthe success of attention mechanisms in representing salient information, we\ndevise the spiking attention block (SAB), a concise yet effective component\nthat optimizes membrane potentials through inferred attention weights, which,\nin turn, regulates spiking activity for superior feature representation. Our\nkey contributions include: 1) we bridge the independent modulation between\ntemporal and channel dimensions, facilitating joint feature correlation\nlearning, and 2) we access the global self-similar patterns in large-scale\nremote sensing imagery to infer spatial attention weights, incorporating\neffective priors for realistic and faithful reconstruction. Building upon SAB,\nwe proposed SpikeSR, which achieves state-of-the-art performance across various\nremote sensing benchmarks such as AID, DOTA, and DIOR, while maintaining high\ncomputational efficiency. The code of SpikeSR will be available upon paper\nacceptance.\n","authors":["Yi Xiao","Qiangqiang Yuan","Kui Jiang","Qiang Zhang","Tingting Zheng","Chia-Wen Lin","Liangpei Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04223v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2408.14153v3","updated":"2025-03-06T09:00:18Z","published":"2024-08-26T09:55:34Z","title":"Explaining Caption-Image Interactions in CLIP models with Second-Order\n  Attributions","summary":"  Dual encoder architectures like CLIP models map two types of inputs into a\nshared embedding space and predict similarities between them. Despite their\nsuccess, it is, however, not understood how these models compare their two\ninputs. Common first-order feature-attribution methods can only provide limited\ninsights into dual-encoders since their predictions depend on\nfeature-interactions rather than on individual features. In this paper, we\nfirst derive a second-order method enabling the attribution of predictions by\nany differentiable dual encoder onto feature-interactions between its inputs.\nSecond, we apply our method to CLIP models and show that they learn\nfine-grained correspondences between parts of captions and regions in images.\nThey match objects across input modes also account for mismatches. This\nvisual-linguistic grounding ability, however, varies heavily between object\nclasses and exhibits pronounced out-of-domain effects. We can identify\nindividual errors as well as systematic failure categories including object\ncoverage, unusual scenes and correlated contexts.\n","authors":["Lucas Möller","Pascal Tilli","Ngoc Thang Vu","Sebastian Padó"],"pdf_url":"https://arxiv.org/pdf/2408.14153v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04215v1","updated":"2025-03-06T08:52:29Z","published":"2025-03-06T08:52:29Z","title":"Energy-Guided Optimization for Personalized Image Editing with\n  Pretrained Text-to-Image Diffusion Models","summary":"  The rapid advancement of pretrained text-driven diffusion models has\nsignificantly enriched applications in image generation and editing. However,\nas the demand for personalized content editing increases, new challenges emerge\nespecially when dealing with arbitrary objects and complex scenes. Existing\nmethods usually mistakes mask as the object shape prior, which struggle to\nachieve a seamless integration result. The mostly used inversion noise\ninitialization also hinders the identity consistency towards the target object.\nTo address these challenges, we propose a novel training-free framework that\nformulates personalized content editing as the optimization of edited images in\nthe latent space, using diffusion models as the energy function guidance\nconditioned by reference text-image pairs. A coarse-to-fine strategy is\nproposed that employs text energy guidance at the early stage to achieve a\nnatural transition toward the target class and uses point-to-point\nfeature-level image energy guidance to perform fine-grained appearance\nalignment with the target object. Additionally, we introduce the latent space\ncontent composition to enhance overall identity consistency with the target.\nExtensive experiments demonstrate that our method excels in object replacement\neven with a large domain gap, highlighting its potential for high-quality,\npersonalized image editing.\n","authors":["Rui Jiang","Xinghe Fu","Guangcong Zheng","Teng Li","Taiping Yao","Xi Li"],"pdf_url":"https://arxiv.org/pdf/2503.04215v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.07155v3","updated":"2025-03-06T08:51:28Z","published":"2024-05-12T04:18:10Z","title":"Meta-Learned Modality-Weighted Knowledge Distillation for Robust\n  Multi-Modal Learning with Missing Data","summary":"  In multi-modal learning, some modalities are more influential than others,\nand their absence can have a significant impact on classification/segmentation\naccuracy. Addressing this challenge, we propose a novel approach called\nMeta-learned Modality-weighted Knowledge Distillation (MetaKD), which enables\nmulti-modal models to maintain high accuracy even when key modalities are\nmissing. MetaKD adaptively estimates the importance weight of each modality\nthrough a meta-learning process. These learned importance weights guide a\npairwise modality-weighted knowledge distillation process, allowing\nhigh-importance modalities to transfer knowledge to lower-importance ones,\nresulting in robust performance despite missing inputs. Unlike previous methods\nin the field, which are often task-specific and require significant\nmodifications, our approach is designed to work in multiple tasks (e.g.,\nsegmentation and classification) with minimal adaptation. Experimental results\non five prevalent datasets, including three Brain Tumor Segmentation datasets\n(BraTS2018, BraTS2019 and BraTS2020), the Alzheimer's Disease Neuroimaging\nInitiative (ADNI) classification dataset and the Audiovision-MNIST\nclassification dataset, demonstrate the proposed model is able to outperform\nthe compared models by a large margin. The code is available at\nhttps://github.com/billhhh/MetaKD.\n","authors":["Hu Wang","Salma Hassan","Yuyuan Liu","Congbo Ma","Yuanhong Chen","Yutong Xie","Mostafa Salem","Yu Tian","Jodie Avery","Louise Hull","Ian Reid","Mohammad Yaqub","Gustavo Carneiro"],"pdf_url":"https://arxiv.org/pdf/2405.07155v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04207v1","updated":"2025-03-06T08:31:40Z","published":"2025-03-06T08:31:40Z","title":"Bridging the Vision-Brain Gap with an Uncertainty-Aware Blur Prior","summary":"  Can our brain signals faithfully reflect the original visual stimuli, even\nincluding high-frequency details? Although human perceptual and cognitive\ncapacities enable us to process and remember visual information, these\nabilities are constrained by several factors, such as limited attentional\nresources and the finite capacity of visual memory. When visual stimuli are\nprocessed by human visual system into brain signals, some information is\ninevitably lost, leading to a discrepancy known as the \\textbf{System GAP}.\nAdditionally, perceptual and cognitive dynamics, along with technical noise in\nsignal acquisition, degrade the fidelity of brain signals relative to the\nvisual stimuli, known as the \\textbf{Random GAP}. When encoded brain\nrepresentations are directly aligned with the corresponding pretrained image\nfeatures, the System GAP and Random GAP between paired data challenge the\nmodel, requiring it to bridge these gaps. However, in the context of limited\npaired data, these gaps are difficult for the model to learn, leading to\noverfitting and poor generalization to new data. To address these GAPs, we\npropose a simple yet effective approach called the \\textbf{Uncertainty-aware\nBlur Prior (UBP)}. It estimates the uncertainty within the paired data,\nreflecting the mismatch between brain signals and visual stimuli. Based on this\nuncertainty, UBP dynamically blurs the high-frequency details of the original\nimages, reducing the impact of the mismatch and improving alignment. Our method\nachieves a top-1 accuracy of \\textbf{50.9\\%} and a top-5 accuracy of\n\\textbf{79.7\\%} on the zero-shot brain-to-image retrieval task, surpassing\nprevious state-of-the-art methods by margins of \\textbf{13.7\\%} and\n\\textbf{9.8\\%}, respectively. Code is available at\n\\href{https://github.com/HaitaoWuTJU/Uncertainty-aware-Blur-Prior}{GitHub}.\n","authors":["Haitao Wu","Qing Li","Changqing Zhang","Zhen He","Xiaomin Ying"],"pdf_url":"https://arxiv.org/pdf/2503.04207v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04205v1","updated":"2025-03-06T08:30:33Z","published":"2025-03-06T08:30:33Z","title":"Learning 3D Medical Image Models From Brain Functional Connectivity\n  Network Supervision For Mental Disorder Diagnosis","summary":"  In MRI-based mental disorder diagnosis, most previous studies focus on\nfunctional connectivity network (FCN) derived from functional MRI (fMRI).\nHowever, the small size of annotated fMRI datasets restricts its wide\napplication. Meanwhile, structural MRIs (sMRIs), such as 3D T1-weighted (T1w)\nMRI, which are commonly used and readily accessible in clinical settings, are\noften overlooked. To integrate the complementary information from both function\nand structure for improved diagnostic accuracy, we propose CINP (Contrastive\nImage-Network Pre-training), a framework that employs contrastive learning\nbetween sMRI and FCN. During pre-training, we incorporate masked image modeling\nand network-image matching to enhance visual representation learning and\nmodality alignment. Since the CINP facilitates knowledge transfer from FCN to\nsMRI, we introduce network prompting. It utilizes only sMRI from suspected\npatients and a small amount of FCNs from different patient classes for\ndiagnosing mental disorders, which is practical in real-world clinical\nscenario. The competitive performance on three mental disorder diagnosis tasks\ndemonstrate the effectiveness of the CINP in integrating multimodal MRI\ninformation, as well as the potential of incorporating sMRI into clinical\ndiagnosis using network prompting.\n","authors":["Xingcan Hu","Wei Wang","Li Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.04205v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04204v1","updated":"2025-03-06T08:30:18Z","published":"2025-03-06T08:30:18Z","title":"FUSE: First-Order and Second-Order Unified SynthEsis in Stochastic\n  Optimization","summary":"  Stochastic optimization methods have actively been playing a critical role in\nmodern machine learning algorithms to deliver decent performance. While\nnumerous works have proposed and developed diverse approaches, first-order and\nsecond-order methods are in entirely different situations. The former is\nsignificantly pivotal and dominating in emerging deep learning but only leads\nconvergence to a stationary point. However, second-order methods are less\npopular due to their computational intensity in large-dimensional problems.\nThis paper presents a novel method that leverages both the first-order and\nsecond-order methods in a unified algorithmic framework, termed FUSE, from\nwhich a practical version (PV) is derived accordingly. FUSE-PV stands as a\nsimple yet efficient optimization method involving a switch-over between first\nand second orders. Additionally, we develop different criteria that determine\nwhen to switch. FUSE-PV has provably shown a smaller computational complexity\nthan SGD and Adam. To validate our proposed scheme, we present an ablation\nstudy on several simple test functions and show a comparison with baselines for\nbenchmark datasets.\n","authors":["Zhanhong Jiang","Md Zahid Hasan","Aditya Balu","Joshua R. Waite","Genyi Huang","Soumik Sarkar"],"pdf_url":"https://arxiv.org/pdf/2503.04204v1.pdf","comment":"6 pages, 7 figures"},{"id":"http://arxiv.org/abs/2411.13056v2","updated":"2025-03-06T08:28:09Z","published":"2024-11-20T06:08:21Z","title":"Efficient Masked AutoEncoder for Video Object Counting and A Large-Scale\n  Benchmark","summary":"  The dynamic imbalance of the fore-background is a major challenge in video\nobject counting, which is usually caused by the sparsity of target objects.\nThis remains understudied in existing works and often leads to severe\nunder-/over-prediction errors. To tackle this issue in video object counting,\nwe propose a density-embedded Efficient Masked Autoencoder Counting (E-MAC)\nframework in this paper. To empower the model's representation ability on\ndensity regression, we develop a new $\\mathtt{D}$ensity-$\\mathtt{E}$mbedded\n$\\mathtt{M}$asked m$\\mathtt{O}$deling ($\\mathtt{DEMO}$) method, which first\ntakes the density map as an auxiliary modality to perform multimodal\nself-representation learning for image and density map. Although\n$\\mathtt{DEMO}$ contributes to effective cross-modal regression guidance, it\nalso brings in redundant background information, making it difficult to focus\non the foreground regions. To handle this dilemma, we propose an efficient\nspatial adaptive masking derived from density maps to boost efficiency.\nMeanwhile, we employ an optical flow-based temporal collaborative fusion\nstrategy to effectively capture the dynamic variations across frames, aligning\nfeatures to derive multi-frame density residuals. The counting accuracy of the\ncurrent frame is boosted by harnessing the information from adjacent frames. In\naddition, considering that most existing datasets are limited to human-centric\nscenarios, we first propose a large video bird counting dataset, DroneBird, in\nnatural scenarios for migratory bird protection. Extensive experiments on three\ncrowd datasets and our \\textit{DroneBird} validate our superiority against the\ncounterparts. The code and dataset are available.\n","authors":["Bing Cao","Quanhao Lu","Jiekang Feng","Qilong Wang","Qinghua Hu","Pengfei Zhu"],"pdf_url":"https://arxiv.org/pdf/2411.13056v2.pdf","comment":"ICLR25"},{"id":"http://arxiv.org/abs/2503.04199v1","updated":"2025-03-06T08:27:51Z","published":"2025-03-06T08:27:51Z","title":"MASTER: Multimodal Segmentation with Text Prompts","summary":"  RGB-Thermal fusion is a potential solution for various weather and light\nconditions in challenging scenarios. However, plenty of studies focus on\ndesigning complex modules to fuse different modalities. With the widespread\napplication of large language models (LLMs), valuable information can be more\neffectively extracted from natural language. Therefore, we aim to leverage the\nadvantages of large language models to design a structurally simple and highly\nadaptable multimodal fusion model architecture. We proposed MultimodAl\nSegmentation with TExt PRompts (MASTER) architecture, which integrates LLM into\nthe fusion of RGB-Thermal multimodal data and allows complex query text to\nparticipate in the fusion process. Our model utilizes a dual-path structure to\nextract information from different modalities of images. Additionally, we\nemploy LLM as the core module for multimodal fusion, enabling the model to\ngenerate learnable codebook tokens from RGB, thermal images, and textual\ninformation. A lightweight image decoder is used to obtain semantic\nsegmentation results. The proposed MASTER performs exceptionally well in\nbenchmark tests across various automated driving scenarios, yielding promising\nresults.\n","authors":["Fuyang Liu","Shun Lu","Jilin Mei","Yu Hu"],"pdf_url":"https://arxiv.org/pdf/2503.04199v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04191v1","updated":"2025-03-06T08:06:03Z","published":"2025-03-06T08:06:03Z","title":"Conformal forecasting for surgical instrument trajectory","summary":"  Forecasting surgical instrument trajectories and predicting the next surgical\naction recently started to attract attention from the research community. Both\nthese tasks are crucial for automation and assistance in endoscopy surgery.\nGiven the safety-critical nature of these tasks, reliable uncertainty\nquantification is essential. Conformal prediction is a fast-growing and widely\nrecognized framework for uncertainty estimation in machine learning and\ncomputer vision, offering distribution-free, theoretically valid prediction\nintervals. In this work, we explore the application of standard conformal\nprediction and conformalized quantile regression to estimate uncertainty in\nforecasting surgical instrument motion, i.e., predicting direction and\nmagnitude of surgical instruments' future motion. We analyze and compare their\ncoverage and interval sizes, assessing the impact of multiple hypothesis\ntesting and correction methods. Additionally, we show how these techniques can\nbe employed to produce useful uncertainty heatmaps. To the best of our\nknowledge, this is the first study applying conformal prediction to surgical\nguidance, marking an initial step toward constructing principled prediction\nintervals with formal coverage guarantees in this domain.\n","authors":["Sara Sangalli","Gary Sarwin","Ertunc Erdil","Carlo Serra","Ender Konukoglu"],"pdf_url":"https://arxiv.org/pdf/2503.04191v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04171v1","updated":"2025-03-06T07:36:45Z","published":"2025-03-06T07:36:45Z","title":"DuCos: Duality Constrained Depth Super-Resolution via Foundation Model","summary":"  We introduce DuCos, a novel depth super-resolution framework grounded in\nLagrangian duality theory, offering a flexible integration of multiple\nconstraints and reconstruction objectives to enhance accuracy and robustness.\nOur DuCos is the first to significantly improve generalization across diverse\nscenarios with foundation models as prompts. The prompt design consists of two\nkey components: Correlative Fusion (CF) and Gradient Regulation (GR). CF\nfacilitates precise geometric alignment and effective fusion between prompt and\ndepth features, while GR refines depth predictions by enforcing consistency\nwith sharp-edged depth maps derived from foundation models. Crucially, these\nprompts are seamlessly embedded into the Lagrangian constraint term, forming a\nsynergistic and principled framework. Extensive experiments demonstrate that\nDuCos outperforms existing state-of-the-art methods, achieving superior\naccuracy, robustness, and generalization. The source codes and pre-trained\nmodels will be publicly available.\n","authors":["Zhiqiang Yan","Zhengxue Wang","Haoye Dong","Jun Li","Jian Yang","Gim Hee Lee"],"pdf_url":"https://arxiv.org/pdf/2503.04171v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04167v1","updated":"2025-03-06T07:29:33Z","published":"2025-03-06T07:29:33Z","title":"The Role of Visual Modality in Multimodal Mathematical Reasoning:\n  Challenges and Insights","summary":"  Recent research has increasingly focused on multimodal mathematical\nreasoning, particularly emphasizing the creation of relevant datasets and\nbenchmarks. Despite this, the role of visual information in reasoning has been\nunderexplored. Our findings show that existing multimodal mathematical models\nminimally leverage visual information, and model performance remains largely\nunaffected by changes to or removal of images in the dataset. We attribute this\nto the dominance of textual information and answer options that inadvertently\nguide the model to correct answers. To improve evaluation methods, we introduce\nthe HC-M3D dataset, specifically designed to require image reliance for\nproblem-solving and to challenge models with similar, yet distinct, images that\nchange the correct answer. In testing leading models, their failure to detect\nthese subtle visual differences suggests limitations in current visual\nperception capabilities. Additionally, we observe that the common approach of\nimproving general VQA capabilities by combining various types of image encoders\ndoes not contribute to math reasoning performance. This finding also presents a\nchallenge to enhancing visual reliance during math reasoning. Our benchmark and\ncode would be available at\n\\href{https://github.com/Yufang-Liu/visual_modality_role}{https://github.com/Yufang-Liu/visual\\_modality\\_role}.\n","authors":["Yufang Liu","Yao Du","Tao Ji","Jianing Wang","Yang Liu","Yuanbin Wu","Aimin Zhou","Mengdi Zhang","Xunliang Cai"],"pdf_url":"https://arxiv.org/pdf/2503.04167v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.11505v4","updated":"2025-03-06T07:26:35Z","published":"2024-11-18T12:05:27Z","title":"LaVin-DiT: Large Vision Diffusion Transformer","summary":"  This paper presents the Large Vision Diffusion Transformer (LaVin-DiT), a\nscalable and unified foundation model designed to tackle over 20 computer\nvision tasks in a generative framework. Unlike existing large vision models\ndirectly adapted from natural language processing architectures, which rely on\nless efficient autoregressive techniques and disrupt spatial relationships\nessential for vision data, LaVin-DiT introduces key innovations to optimize\ngenerative performance for vision tasks. First, to address the high\ndimensionality of visual data, we incorporate a spatial-temporal variational\nautoencoder that encodes data into a continuous latent space. Second, for\ngenerative modeling, we develop a joint diffusion transformer that\nprogressively produces vision outputs. Third, for unified multi-task training,\nin-context learning is implemented. Input-target pairs serve as task context,\nwhich guides the diffusion transformer to align outputs with specific tasks\nwithin the latent space. During inference, a task-specific context set and test\ndata as queries allow LaVin-DiT to generalize across tasks without fine-tuning.\nTrained on extensive vision datasets, the model is scaled from 0.1B to 3.4B\nparameters, demonstrating substantial scalability and state-of-the-art\nperformance across diverse vision tasks. This work introduces a novel pathway\nfor large vision foundation models, underscoring the promising potential of\ndiffusion transformers. The code and models are available.\n","authors":["Zhaoqing Wang","Xiaobo Xia","Runnan Chen","Dongdong Yu","Changhu Wang","Mingming Gong","Tongliang Liu"],"pdf_url":"https://arxiv.org/pdf/2411.11505v4.pdf","comment":"37 pages, 30 figures, 4 tables. Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.04165v1","updated":"2025-03-06T07:25:43Z","published":"2025-03-06T07:25:43Z","title":"WeakSupCon: Weakly Supervised Contrastive Learning for Encoder\n  Pre-training","summary":"  Weakly supervised multiple instance learning (MIL) is a challenging task\ngiven that only bag-level labels are provided, while each bag typically\ncontains multiple instances. This topic has been extensively studied in\nhistopathological image analysis, where labels are usually available only at\nthe whole slide image (WSI) level, while each whole slide image can be divided\ninto thousands of small image patches for training. The dominant MIL approaches\ntake fixed patch features as inputs to address computational constraints and\nensure model stability. These features are commonly generated by encoders\npre-trained on ImageNet, foundation encoders pre-trained on large datasets, or\nthrough self-supervised learning on local datasets. While the self-supervised\nencoder pre-training on the same dataset as downstream MIL tasks helps mitigate\ndomain shift and generate better features, the bag-level labels are not\nutilized during the process, and the features of patches from different\ncategories may cluster together, reducing classification performance on MIL\ntasks. Recently, pre-training with supervised contrastive learning (SupCon) has\ndemonstrated superior performance compared to self-supervised contrastive\nlearning and even end-to-end training on traditional image classification\ntasks. In this paper, we propose a novel encoder pre-training method for\ndownstream MIL tasks called Weakly Supervised Contrastive Learning (WeakSupCon)\nthat utilizes bag-level labels. In our method, we employ multi-task learning\nand define distinct contrastive learning losses for samples with different bag\nlabels. Our experiments demonstrate that the features generated using\nWeakSupCon significantly enhance MIL classification performance compared to\nself-supervised approaches across three datasets.\n","authors":["Bodong Zhang","Hamid Manoochehri","Beatrice S. Knudsen","Tolga Tasdizen"],"pdf_url":"https://arxiv.org/pdf/2503.04165v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01234v2","updated":"2025-03-06T07:11:32Z","published":"2025-03-03T06:57:54Z","title":"Self-Adaptive Gamma Context-Aware SSM-based Model for Metal Defect\n  Detection","summary":"  Metal defect detection is critical in industrial quality assurance, yet\nexisting methods struggle with grayscale variations and complex defect states,\nlimiting its robustness. To address these challenges, this paper proposes a\nSelf-Adaptive Gamma Context-Aware SSM-based model(GCM-DET). This advanced\ndetection framework integrating a Dynamic Gamma Correction (GC) module to\nenhance grayscale representation and optimize feature extraction for precise\ndefect reconstruction. A State-Space Search Management (SSM) architecture\ncaptures robust multi-scale features, effectively handling defects of varying\nshapes and scales. Focal Loss is employed to mitigate class imbalance and\nrefine detection accuracy. Additionally, the CD5-DET dataset is introduced,\nspecifically designed for port container maintenance, featuring significant\ngrayscale variations and intricate defect patterns. Experimental results\ndemonstrate that the proposed model achieves substantial improvements, with\nmAP@0.5 gains of 27.6\\%, 6.6\\%, and 2.6\\% on the CD5-DET, NEU-DET, and GC10-DET\ndatasets.\n","authors":["Sijin Sun","Ming Deng","Xingrui Yu","Xinyu Xi","Liangbin Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.01234v2.pdf","comment":"19 pages, 9 figures, under review"},{"id":"http://arxiv.org/abs/2410.14595v2","updated":"2025-03-06T07:06:50Z","published":"2024-10-18T16:48:31Z","title":"DRACO-DehazeNet: An Efficient Image Dehazing Network Combining Detail\n  Recovery and a Novel Contrastive Learning Paradigm","summary":"  Image dehazing is crucial for clarifying images obscured by haze or fog, but\ncurrent learning-based approaches is dependent on large volumes of training\ndata and hence consumed significant computational power. Additionally, their\nperformance is often inadequate under non-uniform or heavy haze. To address\nthese challenges, we developed the Detail Recovery And Contrastive DehazeNet,\nwhich facilitates efficient and effective dehazing via a dense dilated inverted\nresidual block and an attention-based detail recovery network that tailors\nenhancements to specific dehazed scene contexts. A major innovation is its\nability to train effectively with limited data, achieved through a novel\nquadruplet loss-based contrastive dehazing paradigm. This approach distinctly\nseparates hazy and clear image features while also distinguish lower-quality\nand higher-quality dehazed images obtained from each sub-modules of our\nnetwork, thereby refining the dehazing process to a larger extent. Extensive\ntests on a variety of benchmarked haze datasets demonstrated the superiority of\nour approach. The code repository for this work is available at\nhttps://github.com/GreedYLearner1146/DRACO-DehazeNet.\n","authors":["Gao Yu Lee","Tanmoy Dam","Md Meftahul Ferdaus","Daniel Puiu Poenar","Vu Duong"],"pdf_url":"https://arxiv.org/pdf/2410.14595v2.pdf","comment":"Once the paper is accepted and published, the copyright will be\n  transferred to the corresponding journal"},{"id":"http://arxiv.org/abs/2503.04154v1","updated":"2025-03-06T07:02:13Z","published":"2025-03-06T07:02:13Z","title":"CA-W3D: Leveraging Context-Aware Knowledge for Weakly Supervised\n  Monocular 3D Detection","summary":"  Weakly supervised monocular 3D detection, while less annotation-intensive,\noften struggles to capture the global context required for reliable 3D\nreasoning. Conventional label-efficient methods focus on object-centric\nfeatures, neglecting contextual semantic relationships that are critical in\ncomplex scenes. In this work, we propose a Context-Aware Weak Supervision for\nMonocular 3D object detection, namely CA-W3D, to address this limitation in a\ntwo-stage training paradigm. Specifically, we first introduce a pre-training\nstage employing Region-wise Object Contrastive Matching (ROCM), which aligns\nregional object embeddings derived from a trainable monocular 3D encoder and a\nfrozen open-vocabulary 2D visual grounding model. This alignment encourages the\nmonocular encoder to discriminate scene-specific attributes and acquire richer\ncontextual knowledge. In the second stage, we incorporate a pseudo-label\ntraining process with a Dual-to-One Distillation (D2OD) mechanism, which\neffectively transfers contextual priors into the monocular encoder while\npreserving spatial fidelity and maintaining computational efficiency during\ninference. Extensive experiments conducted on the public KITTI benchmark\ndemonstrate the effectiveness of our approach, surpassing the SoTA method over\nall metrics, highlighting the importance of contextual-aware knowledge in\nweakly-supervised monocular 3D detection.\n","authors":["Chupeng Liu","Runkai Zhao","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2503.04154v1.pdf","comment":"The paper includes 8 pages, 6 figures and 4 tables"},{"id":"http://arxiv.org/abs/2503.04151v1","updated":"2025-03-06T07:01:08Z","published":"2025-03-06T07:01:08Z","title":"Robust Multi-View Learning via Representation Fusion of Sample-Level\n  Attention and Alignment of Simulated Perturbation","summary":"  Recently, multi-view learning (MVL) has garnered significant attention due to\nits ability to fuse discriminative information from multiple views. However,\nreal-world multi-view datasets are often heterogeneous and imperfect, which\nusually makes MVL methods designed for specific combinations of views lack\napplication potential and limits their effectiveness. To address this issue, we\npropose a novel robust MVL method (namely RML) with simultaneous representation\nfusion and alignment. Specifically, we introduce a simple yet effective\nmulti-view transformer fusion network where we transform heterogeneous\nmulti-view data into homogeneous word embeddings, and then integrate multiple\nviews by the sample-level attention mechanism to obtain a fused representation.\nFurthermore, we propose a simulated perturbation based multi-view contrastive\nlearning framework that dynamically generates the noise and unusable\nperturbations for simulating imperfect data conditions. The simulated noisy and\nunusable data obtain two distinct fused representations, and we utilize\ncontrastive learning to align them for learning discriminative and robust\nrepresentations. Our RML is self-supervised and can also be applied for\ndownstream tasks as a regularization. In experiments, we employ it in\nunsupervised multi-view clustering, noise-label classification, and as a\nplug-and-play module for cross-modal hashing retrieval. Extensive comparison\nexperiments and ablation studies validate the effectiveness of RML.\n","authors":["Jie Xu","Na Zhao","Gang Niu","Masashi Sugiyama","Xiaofeng Zhu"],"pdf_url":"https://arxiv.org/pdf/2503.04151v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2307.04014v3","updated":"2025-03-06T06:55:15Z","published":"2023-07-08T16:46:16Z","title":"Novel Pipeline for Diagnosing Acute Lymphoblastic Leukemia Sensitive to\n  Related Biomarkers","summary":"  Acute Lymphoblastic Leukemia (ALL) is one of the most common types of\nchildhood blood cancer. The quick start of the treatment process is critical to\nsaving the patient's life, and for this reason, early diagnosis of this disease\nis essential. Examining the blood smear images of these patients is one of the\nmethods used by expert doctors to diagnose this disease. Deep learning-based\nmethods have numerous applications in medical fields, as they have\nsignificantly advanced in recent years. ALL diagnosis is not an exception in\nthis field, and several machine learning-based methods for this problem have\nbeen proposed. In previous methods, high diagnostic accuracy was reported, but\nour work showed that this alone is not sufficient, as it can lead to models\ntaking shortcuts and not making meaningful decisions. This issue arises due to\nthe small size of medical training datasets. To address this, we constrained\nour model to follow a pipeline inspired by experts' work. We also demonstrated\nthat, since a judgement based on only one image is insufficient, redefining the\nproblem as a multiple-instance learning problem is necessary for achieving a\npractical result. Our model is the first to provide a solution to this problem\nin a multiple-instance learning setup. We introduced a novel pipeline for\ndiagnosing ALL that approximates the process used by hematologists, is\nsensitive to disease biomarkers, and achieves an accuracy of 96.15%, an\nF1-score of 94.24%, a sensitivity of 97.56%, and a specificity of 90.91% on ALL\nIDB 1. Our method was further evaluated on an out-of-distribution dataset,\nwhich posed a challenging test and had acceptable performance. Notably, our\nmodel was trained on a relatively small dataset, highlighting the potential for\nour approach to be applied to other medical datasets with limited data\navailability.\n","authors":["Amirhossein Askari Farsangi","Ali Sharifi-Zarchi","Mohammad Hossein Rohban"],"pdf_url":"https://arxiv.org/pdf/2307.04014v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04144v1","updated":"2025-03-06T06:41:38Z","published":"2025-03-06T06:41:38Z","title":"DM-Adapter: Domain-Aware Mixture-of-Adapters for Text-Based Person\n  Retrieval","summary":"  Text-based person retrieval (TPR) has gained significant attention as a\nfine-grained and challenging task that closely aligns with practical\napplications. Tailoring CLIP to person domain is now a emerging research topic\ndue to the abundant knowledge of vision-language pretraining, but challenges\nstill remain during fine-tuning: (i) Previous full-model fine-tuning in TPR is\ncomputationally expensive and prone to overfitting.(ii) Existing\nparameter-efficient transfer learning (PETL) for TPR lacks of fine-grained\nfeature extraction. To address these issues, we propose Domain-Aware\nMixture-of-Adapters (DM-Adapter), which unifies Mixture-of-Experts (MOE) and\nPETL to enhance fine-grained feature representations while maintaining\nefficiency. Specifically, Sparse Mixture-of-Adapters is designed in parallel to\nMLP layers in both vision and language branches, where different experts\nspecialize in distinct aspects of person knowledge to handle features more\nfinely. To promote the router to exploit domain information effectively and\nalleviate the routing imbalance, Domain-Aware Router is then developed by\nbuilding a novel gating function and injecting learnable domain-aware prompts.\nExtensive experiments show that our DM-Adapter achieves state-of-the-art\nperformance, outperforming previous methods by a significant margin.\n","authors":["Yating Liu","Zimo Liu","Xiangyuan Lan","Wenming Yang","Yaowei Li","Qingmin Liao"],"pdf_url":"https://arxiv.org/pdf/2503.04144v1.pdf","comment":"9 pages, 5 figures, accepted by AAAI 2025"},{"id":"http://arxiv.org/abs/2503.04139v1","updated":"2025-03-06T06:35:19Z","published":"2025-03-06T06:35:19Z","title":"Robust Computer-Vision based Construction Site Detection for\n  Assistive-Technology Applications","summary":"  Navigating urban environments poses significant challenges for people with\ndisabilities, particularly those with blindness and low vision. Environments\nwith dynamic and unpredictable elements like construction sites are especially\nchallenging. Construction sites introduce hazards like uneven surfaces,\nobstructive barriers, hazardous materials, and excessive noise, and they can\nalter routing, complicating safe mobility. Existing assistive technologies are\nlimited, as navigation apps do not account for construction sites during trip\nplanning, and detection tools that attempt hazard recognition struggle to\naddress the extreme variability of construction paraphernalia. This study\nintroduces a novel computer vision-based system that integrates open-vocabulary\nobject detection, a YOLO-based scaffolding-pole detection model, and an optical\ncharacter recognition (OCR) module to comprehensively identify and interpret\nconstruction site elements for assistive navigation. In static testing across\nseven construction sites, the system achieved an overall accuracy of 88.56\\%,\nreliably detecting objects from 2m to 10m within a 0$^\\circ$ -- 75$^\\circ$\nangular offset. At closer distances (2--4m), the detection rate was 100\\% at\nall tested angles. At\n","authors":["Junchi Feng","Giles Hamilton-Fletcher","Nikhil Ballem","Michael Batavia","Yifei Wang","Jiuling Zhong","Maurizio Porfiri","John-Ross Rizzo"],"pdf_url":"https://arxiv.org/pdf/2503.04139v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04134v1","updated":"2025-03-06T06:26:57Z","published":"2025-03-06T06:26:57Z","title":"Real-time Spatial-temporal Traversability Assessment via Feature-based\n  Sparse Gaussian Process","summary":"  Terrain analysis is critical for the practical application of ground mobile\nrobots in real-world tasks, especially in outdoor unstructured environments. In\nthis paper, we propose a novel spatial-temporal traversability assessment\nmethod, which aims to enable autonomous robots to effectively navigate through\ncomplex terrains. Our approach utilizes sparse Gaussian processes (SGP) to\nextract geometric features (curvature, gradient, elevation, etc.) directly from\npoint cloud scans. These features are then used to construct a high-resolution\nlocal traversability map. Then, we design a spatial-temporal Bayesian Gaussian\nkernel (BGK) inference method to dynamically evaluate traversability scores,\nintegrating historical and real-time data while considering factors such as\nslope, flatness, gradient, and uncertainty metrics. GPU acceleration is applied\nin the feature extraction step, and the system achieves real-time performance.\nExtensive simulation experiments across diverse terrain scenarios demonstrate\nthat our method outperforms SOTA approaches in both accuracy and computational\nefficiency. Additionally, we develop an autonomous navigation framework\nintegrated with the traversability map and validate it with a differential\ndriven vehicle in complex outdoor environments. Our code will be open-source\nfor further research and development by the community,\nhttps://github.com/ZJU-FAST-Lab/FSGP_BGK.\n","authors":["Senming Tan","Zhenyu Hou","Zhihao Zhang","Long Xu","Mengke Zhang","Zhaoqi He","Chao Xu","Fei Gao","Yanjun Cao"],"pdf_url":"https://arxiv.org/pdf/2503.04134v1.pdf","comment":"8 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.04131v1","updated":"2025-03-06T06:24:51Z","published":"2025-03-06T06:24:51Z","title":"Q-PART: Quasi-Periodic Adaptive Regression with Test-time Training for\n  Pediatric Left Ventricular Ejection Fraction Regression","summary":"  In this work, we address the challenge of adaptive pediatric Left Ventricular\nEjection Fraction (LVEF) assessment. While Test-time Training (TTT) approaches\nshow promise for this task, they suffer from two significant limitations.\nExisting TTT works are primarily designed for classification tasks rather than\ncontinuous value regression, and they lack mechanisms to handle the\nquasi-periodic nature of cardiac signals. To tackle these issues, we propose a\nnovel \\textbf{Q}uasi-\\textbf{P}eriodic \\textbf{A}daptive \\textbf{R}egression\nwith \\textbf{T}est-time Training (Q-PART) framework. In the training stage, the\nproposed Quasi-Period Network decomposes the echocardiogram into periodic and\naperiodic components within latent space by combining parameterized helix\ntrajectories with Neural Controlled Differential Equations. During inference,\nour framework further employs a variance minimization strategy across image\naugmentations that simulate common quality issues in echocardiogram\nacquisition, along with differential adaptation rates for periodic and\naperiodic components. Theoretical analysis is provided to demonstrate that our\nvariance minimization objective effectively bounds the regression error under\nmild conditions. Furthermore, extensive experiments across three pediatric age\ngroups demonstrate that Q-PART not only significantly outperforms existing\napproaches in pediatric LVEF prediction, but also exhibits strong clinical\nscreening capability with high mAUROC scores (up to 0.9747) and maintains\ngender-fair performance across all metrics, validating its robustness and\npractical utility in pediatric echocardiography analysis.\n","authors":["Jie Liu","Tiexin Qin","Hui Liu","Yilei Shi","Lichao Mou","Xiao Xiang Zhu","Shiqi Wang","Haoliang Li"],"pdf_url":"https://arxiv.org/pdf/2503.04131v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.04130v1","updated":"2025-03-06T06:17:38Z","published":"2025-03-06T06:17:38Z","title":"Token-Efficient Long Video Understanding for Multimodal LLMs","summary":"  Recent advances in video-based multimodal large language models (Video-LLMs)\nhave significantly improved video understanding by processing videos as\nsequences of image frames. However, many existing methods treat frames\nindependently in the vision backbone, lacking explicit temporal modeling, which\nlimits their ability to capture dynamic patterns and efficiently handle long\nvideos. To address these limitations, we introduce STORM\n(\\textbf{S}patiotemporal \\textbf{TO}ken \\textbf{R}eduction for\n\\textbf{M}ultimodal LLMs), a novel architecture incorporating a dedicated\ntemporal encoder between the image encoder and the LLM. Our temporal encoder\nleverages the Mamba State Space Model to integrate temporal information into\nimage tokens, generating enriched representations that preserve inter-frame\ndynamics across the entire video sequence. This enriched encoding not only\nenhances video reasoning capabilities but also enables effective token\nreduction strategies, including test-time sampling and training-based temporal\nand spatial pooling, substantially reducing computational demands on the LLM\nwithout sacrificing key temporal information. By integrating these techniques,\nour approach simultaneously reduces training and inference latency while\nimproving performance, enabling efficient and robust video understanding over\nextended temporal contexts. Extensive evaluations show that STORM achieves\nstate-of-the-art results across various long video understanding benchmarks\n(more than 5\\% improvement on MLVU and LongVideoBench) while reducing the\ncomputation costs by up to $8\\times$ and the decoding latency by\n2.4-2.9$\\times$ for the fixed numbers of input frames. Project page is\navailable at https://research.nvidia.com/labs/lpr/storm\n","authors":["Jindong Jiang","Xiuyu Li","Zhijian Liu","Muyang Li","Guo Chen","Zhiqi Li","De-An Huang","Guilin Liu","Zhiding Yu","Kurt Keutzer","Sungjin Ahn","Jan Kautz","Hongxu Yin","Yao Lu","Song Han","Wonmin Byeon"],"pdf_url":"https://arxiv.org/pdf/2503.04130v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04127v1","updated":"2025-03-06T06:13:27Z","published":"2025-03-06T06:13:27Z","title":"Diff-Reg v2: Diffusion-Based Matching Matrix Estimation for Image\n  Matching and 3D Registration","summary":"  Establishing reliable correspondences is crucial for all registration tasks,\nincluding 2D image registration, 3D point cloud registration, and 2D-3D\nimage-to-point cloud registration. However, these tasks are often complicated\nby challenges such as scale inconsistencies, symmetry, and large deformations,\nwhich can lead to ambiguous matches. Previous feature-based and\ncorrespondence-based methods typically rely on geometric or semantic features\nto generate or polish initial potential correspondences. Some methods typically\nleverage specific geometric priors, such as topological preservation, to devise\ndiverse and innovative strategies tailored to a given enhancement goal, which\ncannot be exhaustively enumerated. Additionally, many previous approaches rely\non a single-step prediction head, which can struggle with local minima in\ncomplex matching scenarios. To address these challenges, we introduce an\ninnovative paradigm that leverages a diffusion model in matrix space for robust\nmatching matrix estimation. Our model treats correspondence estimation as a\ndenoising diffusion process in the matching matrix space, gradually refining\nthe intermediate matching matrix to the optimal one. Specifically, we apply the\ndiffusion model in the doubly stochastic matrix space for 3D-3D and 2D-3D\nregistration tasks. In the 2D image registration task, we deploy the diffusion\nmodel in a matrix subspace where dual-softmax projection regularization is\napplied. For all three registration tasks, we provide adaptive matching matrix\nembedding implementations tailored to the specific characteristics of each task\nwhile maintaining a consistent \"match-to-warp\" encoding pattern. Furthermore,\nwe adopt a lightweight design for the denoising module. In inference, once\npoints or image features are extracted and fixed, this module performs\nmulti-step denoising predictions through reverse sampling.\n","authors":["Qianliang Wu","Haobo Jiang","Yaqing Ding","Lei Luo","Jin Xie","Jian Yang"],"pdf_url":"https://arxiv.org/pdf/2503.04127v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2403.19919"},{"id":"http://arxiv.org/abs/2503.04126v1","updated":"2025-03-06T06:10:21Z","published":"2025-03-06T06:10:21Z","title":"DVM-SLAM: Decentralized Visual Monocular Simultaneous Localization and\n  Mapping for Multi-Agent Systems","summary":"  Cooperative Simultaneous Localization and Mapping (C-SLAM) enables multiple\nagents to work together in mapping unknown environments while simultaneously\nestimating their own positions. This approach enhances robustness, scalability,\nand accuracy by sharing information between agents, reducing drift, and\nenabling collective exploration of larger areas. In this paper, we present\nDecentralized Visual Monocular SLAM (DVM-SLAM), the first open-source\ndecentralized monocular C-SLAM system. By only utilizing low-cost and\nlight-weight monocular vision sensors, our system is well suited for small\nrobots and micro aerial vehicles (MAVs). DVM-SLAM's real-world applicability is\nvalidated on physical robots with a custom collision avoidance framework,\nshowcasing its potential in real-time multi-agent autonomous navigation\nscenarios. We also demonstrate comparable accuracy to state-of-the-art\ncentralized monocular C-SLAM systems. We open-source our code and provide\nsupplementary material online.\n","authors":["Joshua Bird","Jan Blumenkamp","Amanda Prorok"],"pdf_url":"https://arxiv.org/pdf/2503.04126v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04123v1","updated":"2025-03-06T06:00:55Z","published":"2025-03-06T06:00:55Z","title":"GAGrasp: Geometric Algebra Diffusion for Dexterous Grasping","summary":"  We propose GAGrasp, a novel framework for dexterous grasp generation that\nleverages geometric algebra representations to enforce equivariance to SE(3)\ntransformations. By encoding the SE(3) symmetry constraint directly into the\narchitecture, our method improves data and parameter efficiency while enabling\nrobust grasp generation across diverse object poses. Additionally, we\nincorporate a differentiable physics-informed refinement layer, which ensures\nthat generated grasps are physically plausible and stable. Extensive\nexperiments demonstrate the model's superior performance in generalization,\nstability, and adaptability compared to existing methods. Additional details at\nhttps://gagrasp.github.io/\n","authors":["Tao Zhong","Christine Allen-Blanchette"],"pdf_url":"https://arxiv.org/pdf/2503.04123v1.pdf","comment":"Accepted at ICRA 2025"},{"id":"http://arxiv.org/abs/2503.00675v2","updated":"2025-03-06T05:59:08Z","published":"2025-03-02T00:40:50Z","title":"Dur360BEV: A Real-world 360-degree Single Camera Dataset and Benchmark\n  for Bird-Eye View Mapping in Autonomous Driving","summary":"  We present Dur360BEV, a novel spherical camera autonomous driving dataset\nequipped with a high-resolution 128-channel 3D LiDAR and a RTK-refined GNSS/INS\nsystem, along with a benchmark architecture designed to generate Bird-Eye-View\n(BEV) maps using only a single spherical camera. This dataset and benchmark\naddress the challenges of BEV generation in autonomous driving, particularly by\nreducing hardware complexity through the use of a single 360-degree camera\ninstead of multiple perspective cameras. Within our benchmark architecture, we\npropose a novel spherical-image-to-BEV module that leverages spherical imagery\nand a refined sampling strategy to project features from 2D to 3D. Our approach\nalso includes an innovative application of focal loss, specifically adapted to\naddress the extreme class imbalance often encountered in BEV segmentation\ntasks, that demonstrates improved segmentation performance on the Dur360BEV\ndataset. The results show that our benchmark not only simplifies the sensor\nsetup but also achieves competitive performance.\n","authors":["Wenke E","Chao Yuan","Li Li","Yixin Sun","Yona Falinie A. Gaus","Amir Atapour-Abarghouei","Toby P. Breckon"],"pdf_url":"https://arxiv.org/pdf/2503.00675v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04121v1","updated":"2025-03-06T05:58:41Z","published":"2025-03-06T05:58:41Z","title":"Simple Self Organizing Map with Visual Transformer","summary":"  Vision Transformers (ViTs) have demonstrated exceptional performance in\nvarious vision tasks. However, they tend to underperform on smaller datasets\ndue to their inherent lack of inductive biases. Current approaches address this\nlimitation implicitly-often by pairing ViTs with pretext tasks or by distilling\nknowledge from convolutional neural networks (CNNs) to strengthen the prior. In\ncontrast, Self-Organizing Maps (SOMs), a widely adopted self-supervised\nframework, are inherently structured to preserve topology and spatial\norganization, making them a promising candidate to directly address the\nlimitations of ViTs in limited or small training datasets. Despite this\npotential, equipping SOMs with modern deep learning architectures remains\nlargely unexplored. In this study, we conduct a novel exploration on how Vision\nTransformers (ViTs) and Self-Organizing Maps (SOMs) can empower each other,\naiming to bridge this critical research gap. Our findings demonstrate that\nthese architectures can synergistically enhance each other, leading to\nsignificantly improved performance in both unsupervised and supervised tasks.\nCode will be publicly available.\n","authors":["Alan Luo","Kaiwen Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.04121v1.pdf","comment":"5 pages, 4 figures. Submitted to IEEE. All experiments and code work\n  were performed by the first author, with the second author serving in a\n  PI/mentor role, guiding the progression of the work"},{"id":"http://arxiv.org/abs/2503.04119v1","updated":"2025-03-06T05:56:25Z","published":"2025-03-06T05:56:25Z","title":"SCSA: A Plug-and-Play Semantic Continuous-Sparse Attention for Arbitrary\n  Semantic Style Transfer","summary":"  Attention-based arbitrary style transfer methods, including CNN-based,\nTransformer-based, and Diffusion-based, have flourished and produced\nhigh-quality stylized images. However, they perform poorly on the content and\nstyle images with the same semantics, i.e., the style of the corresponding\nsemantic region of the generated stylized image is inconsistent with that of\nthe style image. We argue that the root cause lies in their failure to consider\nthe relationship between local regions and semantic regions. To address this\nissue, we propose a plug-and-play semantic continuous-sparse attention, dubbed\nSCSA, for arbitrary semantic style transfer -- each query point considers\ncertain key points in the corresponding semantic region. Specifically, semantic\ncontinuous attention ensures each query point fully attends to all the\ncontinuous key points in the same semantic region that reflect the overall\nstyle characteristics of that region; Semantic sparse attention allows each\nquery point to focus on the most similar sparse key point in the same semantic\nregion that exhibits the specific stylistic texture of that region. By\ncombining the two modules, the resulting SCSA aligns the overall style of the\ncorresponding semantic regions while transferring the vivid textures of these\nregions. Qualitative and quantitative results prove that SCSA enables\nattention-based arbitrary style transfer methods to produce high-quality\nsemantic stylized images.\n","authors":["Chunnan Shang","Zhizhong Wang","Hongwei Wang","Xiangming Meng"],"pdf_url":"https://arxiv.org/pdf/2503.04119v1.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2410.24185v2","updated":"2025-03-06T05:34:17Z","published":"2024-10-31T17:48:45Z","title":"DexMimicGen: Automated Data Generation for Bimanual Dexterous\n  Manipulation via Imitation Learning","summary":"  Imitation learning from human demonstrations is an effective means to teach\nrobots manipulation skills. But data acquisition is a major bottleneck in\napplying this paradigm more broadly, due to the amount of cost and human effort\ninvolved. There has been significant interest in imitation learning for\nbimanual dexterous robots, like humanoids. Unfortunately, data collection is\neven more challenging here due to the challenges of simultaneously controlling\nmultiple arms and multi-fingered hands. Automated data generation in simulation\nis a compelling, scalable alternative to fuel this need for data. To this end,\nwe introduce DexMimicGen, a large-scale automated data generation system that\nsynthesizes trajectories from a handful of human demonstrations for humanoid\nrobots with dexterous hands. We present a collection of simulation environments\nin the setting of bimanual dexterous manipulation, spanning a range of\nmanipulation behaviors and different requirements for coordination among the\ntwo arms. We generate 21K demos across these tasks from just 60 source human\ndemos and study the effect of several data generation and policy learning\ndecisions on agent performance. Finally, we present a real-to-sim-to-real\npipeline and deploy it on a real-world humanoid can sorting task. Generated\ndatasets, simulation environments and additional results are at\nhttps://dexmimicgen.github.io/\n","authors":["Zhenyu Jiang","Yuqi Xie","Kevin Lin","Zhenjia Xu","Weikang Wan","Ajay Mandlekar","Linxi Fan","Yuke Zhu"],"pdf_url":"https://arxiv.org/pdf/2410.24185v2.pdf","comment":"ICRA 2025. Project website: https://dexmimicgen.github.io/"},{"id":"http://arxiv.org/abs/2503.04107v1","updated":"2025-03-06T05:29:20Z","published":"2025-03-06T05:29:20Z","title":"Fractional Correspondence Framework in Detection Transformer","summary":"  The Detection Transformer (DETR), by incorporating the Hungarian algorithm,\nhas significantly simplified the matching process in object detection tasks.\nThis algorithm facilitates optimal one-to-one matching of predicted bounding\nboxes to ground-truth annotations during training. While effective, this strict\nmatching process does not inherently account for the varying densities and\ndistributions of objects, leading to suboptimal correspondences such as failing\nto handle multiple detections of the same object or missing small objects. To\naddress this, we propose the Regularized Transport Plan (RTP). RTP introduces a\nflexible matching strategy that captures the cost of aligning predictions with\nground truths to find the most accurate correspondences between these sets. By\nutilizing the differentiable Sinkhorn algorithm, RTP allows for soft,\nfractional matching rather than strict one-to-one assignments. This approach\nenhances the model's capability to manage varying object densities and\ndistributions effectively. Our extensive evaluations on the MS-COCO and VOC\nbenchmarks demonstrate the effectiveness of our approach. RTP-DETR, surpassing\nthe performance of the Deform-DETR and the recently introduced DINO-DETR,\nachieving absolute gains in mAP of +3.8% and +1.7%, respectively.\n","authors":["Masoumeh Zareapoor","Pourya Shamsolmoali","Huiyu Zhou","Yue Lu","Salvador García"],"pdf_url":"https://arxiv.org/pdf/2503.04107v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04106v1","updated":"2025-03-06T05:28:44Z","published":"2025-03-06T05:28:44Z","title":"WeakMedSAM: Weakly-Supervised Medical Image Segmentation via SAM with\n  Sub-Class Exploration and Prompt Affinity Mining","summary":"  We have witnessed remarkable progress in foundation models in vision tasks.\nCurrently, several recent works have utilized the segmenting anything model\n(SAM) to boost the segmentation performance in medical images, where most of\nthem focus on training an adaptor for fine-tuning a large amount of pixel-wise\nannotated medical images following a fully supervised manner. In this paper, to\nreduce the labeling cost, we investigate a novel weakly-supervised SAM-based\nsegmentation model, namely WeakMedSAM. Specifically, our proposed WeakMedSAM\ncontains two modules: 1) to mitigate severe co-occurrence in medical images, a\nsub-class exploration module is introduced to learn accurate feature\nrepresentations. 2) to improve the quality of the class activation maps, our\nprompt affinity mining module utilizes the prompt capability of SAM to obtain\nan affinity map for random-walk refinement. Our method can be applied to any\nSAM-like backbone, and we conduct experiments with SAMUS and EfficientSAM. The\nexperimental results on three popularly-used benchmark datasets, i.e., BraTS\n2019, AbdomenCT-1K, and MSD Cardiac dataset, show the promising results of our\nproposed WeakMedSAM. Our code is available at\nhttps://github.com/wanghr64/WeakMedSAM.\n","authors":["Haoran Wang","Lian Huai","Wenbin Li","Lei Qi","Xingqun Jiang","Yinghuan Shi"],"pdf_url":"https://arxiv.org/pdf/2503.04106v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02910v2","updated":"2025-03-06T05:19:44Z","published":"2025-03-04T06:17:17Z","title":"LangGas: Introducing Language in Selective Zero-Shot Background\n  Subtraction for Semi-Transparent Gas Leak Detection with a New Dataset","summary":"  Gas leakage poses a significant hazard that requires prevention.\nTraditionally, human inspection has been used for detection, a slow and\nlabour-intensive process. Recent research has applied machine learning\ntechniques to this problem, yet there remains a shortage of high-quality,\npublicly available datasets. This paper introduces a synthetic dataset\nfeaturing diverse backgrounds, interfering foreground objects, diverse leak\nlocations, and precise segmentation ground truth. We propose a zero-shot method\nthat combines background subtraction, zero-shot object detection, filtering,\nand segmentation to leverage this dataset. Experimental results indicate that\nour approach significantly outperforms baseline methods based solely on\nbackground subtraction and zero-shot object detection with segmentation,\nreaching an IoU of 69\\% overall. We also present an analysis of various prompt\nconfigurations and threshold settings to provide deeper insights into the\nperformance of our method. The code and dataset will be released after\npublication.\n","authors":["Wenqi Guo","Yiyang Du","Shan Du"],"pdf_url":"https://arxiv.org/pdf/2503.02910v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.14909v2","updated":"2025-03-06T05:18:12Z","published":"2025-02-19T02:56:27Z","title":"Comparing Deep Neural Network for Multi-Label ECG Diagnosis From Scanned\n  ECG","summary":"  Automated ECG diagnosis has seen significant advancements with deep learning\ntechniques, but real-world applications still face challenges when dealing with\nscanned paper ECGs. In this study, we explore multi-label classification of\nECGs extracted from scanned images, moving beyond traditional binary\nclassification (normal/abnormal). We evaluate the performance of multiple deep\nneural network architectures, including AlexNet, VGG, ResNet, and Vision\nTransformer, on scanned ECG datasets. Our comparative analysis examines model\naccuracy, robustness to image artifacts, and generalizability across different\nECG conditions. Additionally, we investigate whether ECG signals extracted from\nscanned images retain sufficient diagnostic information for reliable automated\nclassification. The findings highlight the strengths and limitations of each\narchitecture, providing insights into the feasibility of image-based ECG\ndiagnosis and its potential integration into clinical workflows.\n","authors":["Cuong V. Nguyen","Hieu X. Nguyen","Dung D. Pham Minh","Cuong D. Do"],"pdf_url":"https://arxiv.org/pdf/2502.14909v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04096v1","updated":"2025-03-06T05:13:19Z","published":"2025-03-06T05:13:19Z","title":"Image-Based Relocalization and Alignment for Long-Term Monitoring of\n  Dynamic Underwater Environments","summary":"  Effective monitoring of underwater ecosystems is crucial for tracking\nenvironmental changes, guiding conservation efforts, and ensuring long-term\necosystem health. However, automating underwater ecosystem management with\nrobotic platforms remains challenging due to the complexities of underwater\nimagery, which pose significant difficulties for traditional visual\nlocalization methods. We propose an integrated pipeline that combines Visual\nPlace Recognition (VPR), feature matching, and image segmentation on\nvideo-derived images. This method enables robust identification of revisited\nareas, estimation of rigid transformations, and downstream analysis of\necosystem changes. Furthermore, we introduce the SQUIDLE+ VPR Benchmark-the\nfirst large-scale underwater VPR benchmark designed to leverage an extensive\ncollection of unstructured data from multiple robotic platforms, spanning time\nintervals from days to years. The dataset encompasses diverse trajectories,\narbitrary overlap and diverse seafloor types captured under varying\nenvironmental conditions, including differences in depth, lighting, and\nturbidity. Our code is available at: https://github.com/bev-gorry/underloc\n","authors":["Beverley Gorry","Tobias Fischer","Michael Milford","Alejandro Fontan"],"pdf_url":"https://arxiv.org/pdf/2503.04096v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.16805v3","updated":"2025-03-06T05:06:49Z","published":"2024-11-25T14:38:43Z","title":"Human Motion Instruction Tuning","summary":"  This paper presents LLaMo (Large Language and Human Motion Assistant), a\nmultimodal framework for human motion instruction tuning. In contrast to\nconventional instruction-tuning approaches that convert non-linguistic inputs,\nsuch as video or motion sequences, into language tokens, LLaMo retains motion\nin its native form for instruction tuning. This method preserves\nmotion-specific details that are often diminished in tokenization, thereby\nimproving the model's ability to interpret complex human behaviors. By\nprocessing both video and motion data alongside textual inputs, LLaMo enables a\nflexible, human-centric analysis. Experimental evaluations across\nhigh-complexity domains, including human behaviors and professional activities,\nindicate that LLaMo effectively captures domain-specific knowledge, enhancing\ncomprehension and prediction in motion-intensive scenarios. We hope LLaMo\noffers a foundation for future multimodal AI systems with broad applications,\nfrom sports analytics to behavioral prediction. Our code and models are\navailable on the project website: https://github.com/ILGLJ/LLaMo.\n","authors":["Lei Li","Sen Jia","Wang Jianhao","Zhongyu Jiang","Feng Zhou","Ju Dai","Tianfang Zhang","Wu Zongkai","Jenq-Neng Hwang"],"pdf_url":"https://arxiv.org/pdf/2411.16805v3.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2503.04087v1","updated":"2025-03-06T04:50:07Z","published":"2025-03-06T04:50:07Z","title":"Brain Tumor Detection in MRI Based on Federated Learning with YOLOv11","summary":"  One of the primary challenges in medical diagnostics is the accurate and\nefficient use of magnetic resonance imaging (MRI) for the detection of brain\ntumors. But the current machine learning (ML) approaches have two major\nlimitations, data privacy and high latency. To solve the problem, in this work\nwe propose a federated learning architecture for a better accurate brain tumor\ndetection incorporating the YOLOv11 algorithm. In contrast to earlier methods\nof centralized learning, our federated learning approach protects the\nunderlying medical data while supporting cooperative deep learning model\ntraining across multiple institutions. To allow the YOLOv11 model to locate and\nidentify tumor areas, we adjust it to handle MRI data. To ensure robustness and\ngeneralizability, the model is trained and tested on a wide range of MRI data\ncollected from several anonymous medical facilities. The results indicate that\nour method significantly maintains higher accuracy than conventional\napproaches.\n","authors":["Sheikh Moonwara Anjum Monisha","Ratun Rahman"],"pdf_url":"https://arxiv.org/pdf/2503.04087v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.14701v4","updated":"2025-03-06T04:38:23Z","published":"2024-05-23T15:35:48Z","title":"DreamText: High Fidelity Scene Text Synthesis","summary":"  Scene text synthesis involves rendering specified texts onto arbitrary\nimages. Current methods typically formulate this task in an end-to-end manner\nbut lack effective character-level guidance during training. Besides, their\ntext encoders, pre-trained on a single font type, struggle to adapt to the\ndiverse font styles encountered in practical applications. Consequently, these\nmethods suffer from character distortion, repetition, and absence, particularly\nin polystylistic scenarios. To this end, this paper proposes DreamText for\nhigh-fidelity scene text synthesis. Our key idea is to reconstruct the\ndiffusion training process, introducing more refined guidance tailored to this\ntask, to expose and rectify the model's attention at the character level and\nstrengthen its learning of text regions. This transformation poses a hybrid\noptimization challenge, involving both discrete and continuous variables. To\neffectively tackle this challenge, we employ a heuristic alternate optimization\nstrategy. Meanwhile, we jointly train the text encoder and generator to\ncomprehensively learn and utilize the diverse font present in the training\ndataset. This joint training is seamlessly integrated into the alternate\noptimization process, fostering a synergistic relationship between learning\ncharacter embedding and re-estimating character attention. Specifically, in\neach step, we first encode potential character-generated position information\nfrom cross-attention maps into latent character masks. These masks are then\nutilized to update the representation of specific characters in the current\nstep, which, in turn, enables the generator to correct the character's\nattention in the subsequent steps. Both qualitative and quantitative results\ndemonstrate the superiority of our method to the state of the art.\n","authors":["Yibin Wang","Weizhong Zhang","Honghui Xu","Cheng Jin"],"pdf_url":"https://arxiv.org/pdf/2405.14701v4.pdf","comment":"Code: https://github.com/CodeGoat24/DreamText, Project page:\n  https://codegoat24.github.io/DreamText/"},{"id":"http://arxiv.org/abs/2503.04082v1","updated":"2025-03-06T04:37:09Z","published":"2025-03-06T04:37:09Z","title":"Instrument-Splatting: Controllable Photorealistic Reconstruction of\n  Surgical Instruments Using Gaussian Splatting","summary":"  Real2Sim is becoming increasingly important with the rapid development of\nsurgical artificial intelligence (AI) and autonomy. In this work, we propose a\nnovel Real2Sim methodology, \\textit{Instrument-Splatting}, that leverages 3D\nGaussian Splatting to provide fully controllable 3D reconstruction of surgical\ninstruments from monocular surgical videos. To maintain both high visual\nfidelity and manipulability, we introduce a geometry pre-training to bind\nGaussian point clouds on part mesh with accurate geometric priors and define a\nforward kinematics to control the Gaussians as flexible as real instruments.\nAfterward, to handle unposed videos, we design a novel instrument pose tracking\nmethod leveraging semantics-embedded Gaussians to robustly refine per-frame\ninstrument poses and joint states in a render-and-compare manner, which allows\nour instrument Gaussian to accurately learn textures and reach photorealistic\nrendering. We validated our method on 2 publicly released surgical videos and 4\nvideos collected on ex vivo tissues and green screens. Quantitative and\nqualitative evaluations demonstrate the effectiveness and superiority of the\nproposed method.\n","authors":["Shuojue Yang","Zijian Wu","Mingxuan Hong","Qian Li","Daiyun Shen","Septimiu E. Salcudean","Yueming Jin"],"pdf_url":"https://arxiv.org/pdf/2503.04082v1.pdf","comment":"11 pages, 5 figures"},{"id":"http://arxiv.org/abs/2503.04079v1","updated":"2025-03-06T04:33:19Z","published":"2025-03-06T04:33:19Z","title":"Surgical Gaussian Surfels: Highly Accurate Real-time Surgical Scene\n  Rendering","summary":"  Accurate geometric reconstruction of deformable tissues in monocular\nendoscopic video remains a fundamental challenge in robot-assisted minimally\ninvasive surgery. Although recent volumetric and point primitive methods based\non neural radiance fields (NeRF) and 3D Gaussian primitives have efficiently\nrendered surgical scenes, they still struggle with handling artifact-free tool\nocclusions and preserving fine anatomical details. These limitations stem from\nunrestricted Gaussian scaling and insufficient surface alignment constraints\nduring reconstruction. To address these issues, we introduce Surgical Gaussian\nSurfels (SGS), which transforms anisotropic point primitives into\nsurface-aligned elliptical splats by constraining the scale component of the\nGaussian covariance matrix along the view-aligned axis. We predict accurate\nsurfel motion fields using a lightweight Multi-Layer Perceptron (MLP) coupled\nwith locality constraints to handle complex tissue deformations. We use\nhomodirectional view-space positional gradients to capture fine image details\nby splitting Gaussian Surfels in over-reconstructed regions. In addition, we\ndefine surface normals as the direction of the steepest density change within\neach Gaussian surfel primitive, enabling accurate normal estimation without\nrequiring monocular normal priors. We evaluate our method on two in-vivo\nsurgical datasets, where it outperforms current state-of-the-art methods in\nsurface geometry, normal map quality, and rendering efficiency, while remaining\ncompetitive in real-time rendering performance. We make our code available at\nhttps://github.com/aloma85/SurgicalGaussianSurfels\n","authors":["Idris O. Sunmola","Zhenjun Zhao","Samuel Schmidgall","Yumeng Wang","Paul Maria Scheikl","Axel Krieger"],"pdf_url":"https://arxiv.org/pdf/2503.04079v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20637v2","updated":"2025-03-06T04:31:21Z","published":"2025-02-28T01:36:38Z","title":"TractCloud-FOV: Deep Learning-based Robust Tractography Parcellation in\n  Diffusion MRI with Incomplete Field of View","summary":"  Tractography parcellation classifies streamlines reconstructed from diffusion\nMRI into anatomically defined fiber tracts for clinical and research\napplications. However, clinical scans often have incomplete fields of view\n(FOV) where brain regions are partially imaged, leading to partial or truncated\nfiber tracts. To address this challenge, we introduce TractCloud-FOV, a deep\nlearning framework that robustly parcellates tractography under conditions of\nincomplete FOV. We propose a novel training strategy, FOV-Cut Augmentation\n(FOV-CA), in which we synthetically cut tractograms to simulate a spectrum of\nreal-world inferior FOV cutoff scenarios. This data augmentation approach\nenriches the training set with realistic truncated streamlines, enabling the\nmodel to achieve superior generalization. We evaluate the proposed\nTractCloud-FOV on both synthetically cut tractography and two real-life\ndatasets with incomplete FOV. TractCloud-FOV significantly outperforms several\nstate-of-the-art methods on all testing datasets in terms of streamline\nclassification accuracy, generalization ability, tract anatomical depiction,\nand computational efficiency. Overall, TractCloud-FOV achieves efficient and\nconsistent tractography parcellation in diffusion MRI with incomplete FOV.\n","authors":["Yuqian Chen","Leo Zekelman","Yui Lo","Suheyla Cetin-Karayumak","Tengfei Xue","Yogesh Rathi","Nikos Makris","Fan Zhang","Weidong Cai","Lauren J. O'Donnell"],"pdf_url":"https://arxiv.org/pdf/2502.20637v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04078v1","updated":"2025-03-06T04:28:11Z","published":"2025-03-06T04:28:11Z","title":"Spatial-Temporal Perception with Causal Inference for Naturalistic\n  Driving Action Recognition","summary":"  Naturalistic driving action recognition is essential for vehicle cabin\nmonitoring systems. However, the complexity of real-world backgrounds presents\nsignificant challenges for this task, and previous approaches have struggled\nwith practical implementation due to their limited ability to observe subtle\nbehavioral differences and effectively learn inter-frame features from video.\nIn this paper, we propose a novel Spatial-Temporal Perception (STP)\narchitecture that emphasizes both temporal information and spatial\nrelationships between key objects, incorporating a causal decoder to perform\nbehavior recognition and temporal action localization. Without requiring\nmultimodal input, STP directly extracts temporal and spatial distance features\nfrom RGB video clips. Subsequently, these dual features are jointly encoded by\nmaximizing the expected likelihood across all possible permutations of the\nfactorization order. By integrating temporal and spatial features at different\nscales, STP can perceive subtle behavioral changes in challenging scenarios.\nAdditionally, we introduce a causal-aware module to explore relationships\nbetween video frame features, significantly enhancing detection efficiency and\nperformance. We validate the effectiveness of our approach using two publicly\navailable driver distraction detection benchmarks. The results demonstrate that\nour framework achieves state-of-the-art performance.\n","authors":["Qing Chang","Wei Dai","Zhihao Shuai","Limin Yu","Yutao Yue"],"pdf_url":"https://arxiv.org/pdf/2503.04078v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.17741v5","updated":"2025-03-06T04:11:30Z","published":"2024-12-23T17:44:05Z","title":"Reasoning to Attend: Try to Understand How <SEG> Token Works","summary":"  Current Large Multimodal Models (LMMs) empowered visual grounding typically\nrely on $\\texttt{<SEG>}$ tokens as a text prompt to jointly optimize the\nvision-language model (e.g., LLaVA) and the downstream task-specific model\n(e.g., SAM). However, we observe that little research has looked into how it\nworks.In this work, we first visualize the similarity maps, which are obtained\nby computing the semantic similarity between the $\\texttt{<SEG>}$ token and the\nimage token embeddings derived from the last hidden layer in both the LLaVA\nencoder and SAM decoder. Intriguingly, we have found that a striking\nconsistency holds in terms of activation responses in the similarity map, which\nreveals that what the $\\texttt{<SEG>}$ token contributes to is semantic\nsimilarity within image-text pairs. Specifically, the $\\texttt{<SEG>}$ token, a\nplaceholder expanded in text vocabulary, extensively queries among individual\ntokenized image patches to match the semantics of an object from text to the\npaired image, while the Large Language Models (LLMs) are being fine-tuned. Upon\nthe above findings, we present READ, which facilitates LMMs' resilient\n$\\textbf{REA}$soning capability of where to atten$\\textbf{D}$ under the\nguidance of highly activated points borrowed from similarity maps. Remarkably,\nREAD features an intuitive design, Similarity as Points module (SasP), which\ncan be seamlessly applied to $\\texttt{<SEG>}$-like paradigms in a plug-and-play\nfashion. Also, extensive experiments have been conducted on ReasonSeg and\nRefCOCO(+/g) datasets. To validate whether READ suffers from catastrophic\nforgetting of previous skills after fine-tuning, we further assess its\ngeneration ability on an augmented FP-RefCOCO(+/g) dataset. All codes and\nmodels are publicly available at https://github.com/rui-qian/READ.\n","authors":["Rui Qian","Xin Yin","Dejing Dou"],"pdf_url":"https://arxiv.org/pdf/2412.17741v5.pdf","comment":"This work has been accepted to CVPR 2025, please refer to\n  https://github.com/rui-qian/READ"},{"id":"http://arxiv.org/abs/2405.15683v3","updated":"2025-03-06T03:59:59Z","published":"2024-05-24T16:21:59Z","title":"Visual Description Grounding Reduces Hallucinations and Boosts Reasoning\n  in LVLMs","summary":"  Large Vision-Language Models (LVLMs) often produce responses that misalign\nwith factual information, a phenomenon known as hallucinations. While\nhallucinations are well-studied, the exact causes behind them remain\nunderexplored. In this paper, we first investigate the root causes of\nhallucinations in LVLMs. Our findings reveal that existing mitigation\ntechniques primarily reduce hallucinations for visual recognition prompts-those\nthat require simple descriptions of visual elements-but fail for cognitive\nprompts that demand deliberate reasoning. We identify the core issue as a lack\nof true visual perception in LVLMs: although they can accurately recognize\nvisual elements, they struggle to fully interpret these elements in the context\nof the input prompt and effectively link this recognition to their internal\nknowledge, which is critical for reasoning. To address this gap, we introduce\nVisual Description Grounded Decoding (VDGD), a simple, robust, and\ntraining-free method designed to enhance visual perception and improve\nreasoning capabilities in LVLMs. VDGD works by first generating a detailed\ndescription of the image and appending it as a prefix to the instruction.\nDuring response generation, tokens are sampled based on their KL divergence to\nthe description, favoring candidates with lower divergence. Experimental\nresults on multiple visual reasoning benchmarks and LVLMs demonstrate that VDGD\nconsistently outperforms existing baselines 2% - 33%. Finally, we introduce\nVaLLu, a benchmark designed for comprehensive evaluation of the cognitive\ncapabilities of LVLMs.\n","authors":["Sreyan Ghosh","Chandra Kiran Reddy Evuru","Sonal Kumar","Utkarsh Tyagi","Oriol Nieto","Zeyu Jin","Dinesh Manocha"],"pdf_url":"https://arxiv.org/pdf/2405.15683v3.pdf","comment":"Accepted to ICLR 2025. Project: https://sreyan88.github.io/VDGD/"},{"id":"http://arxiv.org/abs/2502.16445v3","updated":"2025-03-06T03:55:58Z","published":"2025-02-23T05:08:06Z","title":"Iterative Flow Matching -- Path Correction and Gradual Refinement for\n  Enhanced Generative Modeling","summary":"  Generative models for image generation are now commonly used for a wide\nvariety of applications, ranging from guided image generation for entertainment\nto solving inverse problems. Nonetheless, training a generator is a non-trivial\nfeat that requires fine-tuning and can lead to so-called hallucinations, that\nis, the generation of images that are unrealistic. In this work, we explore\nimage generation using flow matching. We explain and demonstrate why flow\nmatching can generate hallucinations, and propose an iterative process to\nimprove the generation process. Our iterative process can be integrated into\nvirtually $\\textit{any}$ generative modeling technique, thereby enhancing the\nperformance and robustness of image synthesis systems.\n","authors":["Eldad Haber","Shadab Ahamed","Md. Shahriar Rahim Siddiqui","Niloufar Zakariaei","Moshe Eliasof"],"pdf_url":"https://arxiv.org/pdf/2502.16445v3.pdf","comment":"17 pages, 8 figures"},{"id":"http://arxiv.org/abs/2503.04067v1","updated":"2025-03-06T03:52:46Z","published":"2025-03-06T03:52:46Z","title":"FREAK: Frequency-modulated High-fidelity and Real-time Audio-driven\n  Talking Portrait Synthesis","summary":"  Achieving high-fidelity lip-speech synchronization in audio-driven talking\nportrait synthesis remains challenging. While multi-stage pipelines or\ndiffusion models yield high-quality results, they suffer from high\ncomputational costs. Some approaches perform well on specific individuals with\nlow resources, yet still exhibit mismatched lip movements. The aforementioned\nmethods are modeled in the pixel domain. We observed that there are noticeable\ndiscrepancies in the frequency domain between the synthesized talking videos\nand natural videos. Currently, no research on talking portrait synthesis has\nconsidered this aspect. To address this, we propose a FREquency-modulated,\nhigh-fidelity, and real-time Audio-driven talKing portrait synthesis framework,\nnamed FREAK, which models talking portraits from the frequency domain\nperspective, enhancing the fidelity and naturalness of the synthesized\nportraits. FREAK introduces two novel frequency-based modules: 1) the Visual\nEncoding Frequency Modulator (VEFM) to couple multi-scale visual features in\nthe frequency domain, better preserving visual frequency information and\nreducing the gap in the frequency spectrum between synthesized and natural\nframes. and 2) the Audio Visual Frequency Modulator (AVFM) to help the model\nlearn the talking pattern in the frequency domain and improve audio-visual\nsynchronization. Additionally, we optimize the model in both pixel domain and\nfrequency domain jointly. Furthermore, FREAK supports seamless switching\nbetween one-shot and video dubbing settings, offering enhanced flexibility. Due\nto its superior performance, it can simultaneously support high-resolution\nvideo results and real-time inference. Extensive experiments demonstrate that\nour method synthesizes high-fidelity talking portraits with detailed facial\ntextures and precise lip synchronization in real-time, outperforming\nstate-of-the-art methods.\n","authors":["Ziqi Ni","Ao Fu","Yi Zhou"],"pdf_url":"https://arxiv.org/pdf/2503.04067v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00741v2","updated":"2025-03-06T03:44:10Z","published":"2025-03-02T05:36:04Z","title":"LesionDiffusion: Towards Text-controlled General Lesion Synthesis","summary":"  Fully-supervised lesion recognition methods in medical imaging face\nchallenges due to the reliance on large annotated datasets, which are expensive\nand difficult to collect. To address this, synthetic lesion generation has\nbecome a promising approach. However, existing models struggle with\nscalability, fine-grained control over lesion attributes, and the generation of\ncomplex structures. We propose LesionDiffusion, a text-controllable lesion\nsynthesis framework for 3D CT imaging that generates both lesions and\ncorresponding masks. By utilizing a structured lesion report template, our\nmodel provides greater control over lesion attributes and supports a wider\nvariety of lesion types. We introduce a dataset of 1,505 annotated CT scans\nwith paired lesion masks and structured reports, covering 14 lesion types\nacross 8 organs. LesionDiffusion consists of two components: a lesion mask\nsynthesis network (LMNet) and a lesion inpainting network (LINet), both guided\nby lesion attributes and image features. Extensive experiments demonstrate that\nLesionDiffusion significantly improves segmentation performance, with strong\ngeneralization to unseen lesion types and organs, outperforming current\nstate-of-the-art models. Code will be available at\nhttps://github.com/HengruiTianSJTU/LesionDiffusion.\n","authors":["Henrui Tian","Wenhui Lei","Linrui Dai","Hanyu Chen","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.00741v2.pdf","comment":"10 pages, 4 figures"},{"id":"http://arxiv.org/abs/2503.04065v1","updated":"2025-03-06T03:43:21Z","published":"2025-03-06T03:43:21Z","title":"PP-DocBee: Improving Multimodal Document Understanding Through a Bag of\n  Tricks","summary":"  With the rapid advancement of digitalization, various document images are\nbeing applied more extensively in production and daily life, and there is an\nincreasingly urgent need for fast and accurate parsing of the content in\ndocument images. Therefore, this report presents PP-DocBee, a novel multimodal\nlarge language model designed for end-to-end document image understanding.\nFirst, we develop a data synthesis strategy tailored to document scenarios in\nwhich we build a diverse dataset to improve the model generalization. Then, we\napply a few training techniques, including dynamic proportional sampling, data\npreprocessing, and OCR postprocessing strategies. Extensive evaluations\ndemonstrate the superior performance of PP-DocBee, achieving state-of-the-art\nresults on English document understanding benchmarks and even outperforming\nexisting open source and commercial models in Chinese document understanding.\nThe source code and pre-trained models are publicly available at\n\\href{https://github.com/PaddlePaddle/PaddleMIX}{https://github.com/PaddlePaddle/PaddleMIX}.\n","authors":["Feng Ni","Kui Huang","Yao Lu","Wenyu Lv","Guanzhong Wang","Zeyu Chen","Yi Liu"],"pdf_url":"https://arxiv.org/pdf/2503.04065v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00736v2","updated":"2025-03-06T03:35:09Z","published":"2025-03-02T05:20:41Z","title":"Shazam: Unifying Multiple Foundation Models for Advanced Computational\n  Pathology","summary":"  Foundation Models (FMs) in computational pathology (CPath) have significantly\nadvanced the extraction of meaningful features from histopathology image\ndatasets, achieving strong performance across various clinical tasks. Despite\ntheir impressive performance, these models often exhibit variability when\napplied to different tasks, prompting the need for a unified framework capable\nof consistently excelling across various applications. In this work, we propose\nShazam, a novel framework designed to efficiently combine multiple CPath\nmodels. Unlike previous approaches that train a fixed-parameter FM, Shazam\ndynamically extracts and refines information from diverse FMs for each specific\ntask. To ensure that each FM contributes effectively without dominance, a novel\ndistillation strategy is applied, guiding the student model with features from\nall teacher models, which enhances its generalization ability. Experimental\nresults on two pathology patch classification datasets demonstrate that Shazam\noutperforms existing CPath models and other fusion methods. Its lightweight,\nflexible design makes it a promising solution for improving CPath analysis in\nreal-world settings. Code will be available at\nhttps://github.com/Tuner12/Shazam.\n","authors":["Wenhui Lei","Anqi Li","Yusheng Tan","Hanyu Chen","Xiaofan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.00736v2.pdf","comment":"9 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.03190v2","updated":"2025-03-06T03:32:56Z","published":"2025-03-05T05:13:53Z","title":"DSPNet: Dual-vision Scene Perception for Robust 3D Question Answering","summary":"  3D Question Answering (3D QA) requires the model to comprehensively\nunderstand its situated 3D scene described by the text, then reason about its\nsurrounding environment and answer a question under that situation. However,\nexisting methods usually rely on global scene perception from pure 3D point\nclouds and overlook the importance of rich local texture details from\nmulti-view images. Moreover, due to the inherent noise in camera poses and\ncomplex occlusions, there exists significant feature degradation and reduced\nfeature robustness problems when aligning 3D point cloud with multi-view\nimages. In this paper, we propose a Dual-vision Scene Perception Network\n(DSPNet), to comprehensively integrate multi-view and point cloud features to\nimprove robustness in 3D QA. Our Text-guided Multi-view Fusion (TGMF) module\nprioritizes image views that closely match the semantic content of the text. To\nadaptively fuse back-projected multi-view images with point cloud features, we\ndesign the Adaptive Dual-vision Perception (ADVP) module, enhancing 3D scene\ncomprehension. Additionally, our Multimodal Context-guided Reasoning (MCGR)\nmodule facilitates robust reasoning by integrating contextual information\nacross visual and linguistic modalities. Experimental results on SQA3D and\nScanQA datasets demonstrate the superiority of our DSPNet. Codes will be\navailable at https://github.com/LZ-CH/DSPNet.\n","authors":["Jingzhou Luo","Yang Liu","Weixing Chen","Zhen Li","Yaowei Wang","Guanbin Li","Liang Lin"],"pdf_url":"https://arxiv.org/pdf/2503.03190v2.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2410.21259v4","updated":"2025-03-06T03:31:32Z","published":"2024-10-28T17:55:08Z","title":"AutoBench-V: Can Large Vision-Language Models Benchmark Themselves?","summary":"  Large Vision-Language Models (LVLMs) have become essential for advancing the\nintegration of visual and linguistic information. However, the evaluation of\nLVLMs presents significant challenges as the evaluation benchmark always\ndemands lots of human cost for its construction, and remains static, lacking\nflexibility once constructed. Even though automatic evaluation has been\nexplored in textual modality, the visual modality remains under-explored. As a\nresult, in this work, we address a question: \"Can LVLMs themselves be used to\nbenchmark each other in the visual automatically domain?\". We introduce\nAutoBench-V, an automated framework for serving evaluation on demand, i.e.,\nbenchmarking LVLMs based on specific aspects of model capability. AutoBench-V\nleverages text-to-image models to generate relevant image samples and then\nutilizes LVLMs to orchestrate visual question-answering (VQA) tasks, completing\nthe evaluation process efficiently and flexibly. Through an extensive\nevaluation of nine popular LVLMs across five demanded user inputs (i.e.,\nevaluation capabilities), the framework shows effectiveness and reliability.\n","authors":["Han Bao","Yue Huang","Yanbo Wang","Jiayi Ye","Xiangqi Wang","Xiuying Chen","Yue Zhao","Tianyi Zhou","Mohamed Elhoseiny","Xiangliang Zhang"],"pdf_url":"https://arxiv.org/pdf/2410.21259v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04059v1","updated":"2025-03-06T03:27:14Z","published":"2025-03-06T03:27:14Z","title":"H3O: Hyper-Efficient 3D Occupancy Prediction with Heterogeneous\n  Supervision","summary":"  3D occupancy prediction has recently emerged as a new paradigm for holistic\n3D scene understanding and provides valuable information for downstream\nplanning in autonomous driving. Most existing methods, however, are\ncomputationally expensive, requiring costly attention-based 2D-3D\ntransformation and 3D feature processing. In this paper, we present a novel 3D\noccupancy prediction approach, H3O, which features highly efficient\narchitecture designs that incur a significantly lower computational cost as\ncompared to the current state-of-the-art methods. In addition, to compensate\nfor the ambiguity in ground-truth 3D occupancy labels, we advocate leveraging\nauxiliary tasks to complement the direct 3D supervision. In particular, we\nintegrate multi-camera depth estimation, semantic segmentation, and surface\nnormal estimation via differentiable volume rendering, supervised by\ncorresponding 2D labels that introduces rich and heterogeneous supervision\nsignals. We conduct extensive experiments on the Occ3D-nuScenes and\nSemanticKITTI benchmarks that demonstrate the superiority of our proposed H3O.\n","authors":["Yunxiao Shi","Hong Cai","Amin Ansari","Fatih Porikli"],"pdf_url":"https://arxiv.org/pdf/2503.04059v1.pdf","comment":"ICRA 2025"}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.04653v1","updated":"2025-03-06T17:43:03Z","published":"2025-03-06T17:43:03Z","title":"RadIR: A Scalable Framework for Multi-Grained Medical Image Retrieval\n  via Radiology Report Mining","summary":"  Developing advanced medical imaging retrieval systems is challenging due to\nthe varying definitions of `similar images' across different medical contexts.\nThis challenge is compounded by the lack of large-scale, high-quality medical\nimaging retrieval datasets and benchmarks. In this paper, we propose a novel\nmethodology that leverages dense radiology reports to define image-wise\nsimilarity ordering at multiple granularities in a scalable and fully automatic\nmanner. Using this approach, we construct two comprehensive medical imaging\nretrieval datasets: MIMIC-IR for Chest X-rays and CTRATE-IR for CT scans,\nproviding detailed image-image ranking annotations conditioned on diverse\nanatomical structures. Furthermore, we develop two retrieval systems, RadIR-CXR\nand model-ChestCT, which demonstrate superior performance in traditional\nimage-image and image-report retrieval tasks. These systems also enable\nflexible, effective image retrieval conditioned on specific anatomical\nstructures described in text, achieving state-of-the-art results on 77 out of\n78 metrics.\n","authors":["Tengfei Zhang","Ziheng Zhao","Chaoyi Wu","Xiao Zhou","Ya Zhang","Yangfeng Wang","Weidi Xie"],"pdf_url":"https://arxiv.org/pdf/2503.04653v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04644v1","updated":"2025-03-06T17:32:22Z","published":"2025-03-06T17:32:22Z","title":"IFIR: A Comprehensive Benchmark for Evaluating Instruction-Following in\n  Expert-Domain Information Retrieval","summary":"  We introduce IFIR, the first comprehensive benchmark designed to evaluate\ninstruction-following information retrieval (IR) in expert domains. IFIR\nincludes 2,426 high-quality examples and covers eight subsets across four\nspecialized domains: finance, law, healthcare, and science literature. Each\nsubset addresses one or more domain-specific retrieval tasks, replicating\nreal-world scenarios where customized instructions are critical. IFIR enables a\ndetailed analysis of instruction-following retrieval capabilities by\nincorporating instructions at different levels of complexity. We also propose a\nnovel LLM-based evaluation method to provide a more precise and reliable\nassessment of model performance in following instructions. Through extensive\nexperiments on 15 frontier retrieval models, including those based on LLMs, our\nresults reveal that current models face significant challenges in effectively\nfollowing complex, domain-specific instructions. We further provide in-depth\nanalyses to highlight these limitations, offering valuable insights to guide\nfuture advancements in retriever development.\n","authors":["Tingyu Song","Guo Gan","Mingsheng Shang","Yilun Zhao"],"pdf_url":"https://arxiv.org/pdf/2503.04644v1.pdf","comment":"NAACL 2025 Main"},{"id":"http://arxiv.org/abs/2503.03606v2","updated":"2025-03-06T14:28:36Z","published":"2025-03-05T15:42:37Z","title":"Decoupled Recommender Systems: Exploring Alternative Recommender\n  Ecosystem Designs","summary":"  Recommender ecosystems are an emerging subject of research. Such research\nexamines how the characteristics of algorithms, recommendation consumers, and\nitem providers influence system dynamics and long-term outcomes. One\narchitectural possibility that has not yet been widely explored in this line of\nresearch is the consequences of a configuration in which recommendation\nalgorithms are decoupled from the platforms they serve. This is sometimes\ncalled \"the friendly neighborhood algorithm store\" or \"middleware\" model. We\nare particularly interested in how such architectures might offer a range of\ndifferent distributions of utility across consumers, providers, and\nrecommendation platforms. In this paper, we create a model of a recommendation\necosystem that incorporates algorithm choice and examine the outcomes of such a\ndesign.\n","authors":["Anas Buhayh","Elizabeth McKinnie","Robin Burke"],"pdf_url":"https://arxiv.org/pdf/2503.03606v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13959v2","updated":"2025-03-06T13:51:24Z","published":"2025-01-21T06:32:25Z","title":"Assisting Mathematical Formalization with A Learning-based Premise\n  Retriever","summary":"  Premise selection is a crucial yet challenging step in mathematical\nformalization, especially for users with limited experience. Due to the lack of\navailable formalization projects, existing approaches that leverage language\nmodels often suffer from data scarcity. In this work, we introduce an\ninnovative method for training a premise retriever to support the formalization\nof mathematics. Our approach employs a BERT model to embed proof states and\npremises into a shared latent space. The retrieval model is trained within a\ncontrastive learning framework and incorporates a domain-specific tokenizer\nalong with a fine-grained similarity computation method. Experimental results\nshow that our model is highly competitive compared to existing baselines,\nachieving strong performance while requiring fewer computational resources.\nPerformance is further enhanced through the integration of a re-ranking module.\nTo streamline the formalization process, we will release a search engine that\nenables users to query Mathlib theorems directly using proof states,\nsignificantly improving accessibility and efficiency. Codes are available at\nhttps://github.com/ruc-ai4math/Premise-Retrieval.\n","authors":["Yicheng Tao","Haotian Liu","Shanwen Wang","Hongteng Xu"],"pdf_url":"https://arxiv.org/pdf/2501.13959v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04406v1","updated":"2025-03-06T13:00:53Z","published":"2025-03-06T13:00:53Z","title":"Training-Free Graph Filtering via Multimodal Feature Refinement for\n  Extremely Fast Multimodal Recommendation","summary":"  Multimodal recommender systems improve the performance of canonical\nrecommender systems with no item features by utilizing diverse content types\nsuch as text, images, and videos, while alleviating inherent sparsity of\nuser-item interactions and accelerating user engagement. However, current\nneural network-based models often incur significant computational overhead due\nto the complex training process required to learn and integrate information\nfrom multiple modalities. To overcome this limitation, we propose\nMultiModal-Graph Filtering (MM-GF), a training-free method based on the notion\nof graph filtering (GF) for efficient and accurate multimodal recommendations.\nSpecifically, MM-GF first constructs multiple similarity graphs through\nnontrivial multimodal feature refinement such as robust scaling and vector\nshifting by addressing the heterogeneous characteristics across modalities.\nThen, MM-GF optimally fuses multimodal information using linear low-pass\nfilters across different modalities. Extensive experiments on real-world\nbenchmark datasets demonstrate that MM-GF not only improves recommendation\naccuracy by up to 13.35% compared to the best competitor but also dramatically\nreduces computational costs by achieving the runtime of less than 10 seconds.\n","authors":["Yu-Seung Roh","Joo-Young Kim","Jin-Duk Park","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2503.04406v1.pdf","comment":"10 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.01346v2","updated":"2025-03-06T12:27:24Z","published":"2025-03-03T09:37:33Z","title":"SRAG: Structured Retrieval-Augmented Generation for Multi-Entity\n  Question Answering over Wikipedia Graph","summary":"  Multi-entity question answering (MEQA) poses significant challenges for large\nlanguage models (LLMs), which often struggle to consolidate scattered\ninformation across multiple documents. An example question might be \"What is\nthe distribution of IEEE Fellows among various fields of study?\", which\nrequires retrieving information from diverse sources e.g., Wikipedia pages. The\neffectiveness of current retrieval-augmented generation (RAG) methods is\nlimited by the LLMs' capacity to aggregate insights from numerous pages. To\naddress this gap, this paper introduces a structured RAG (SRAG) framework that\nsystematically organizes extracted entities into relational tables (e.g.,\ntabulating entities with schema columns like \"name\" and \"field of study\") and\nthen apply table-based reasoning techniques. Our approach decouples retrieval\nand reasoning, enabling LLMs to focus on structured data analysis rather than\nraw text aggregation. Extensive experiments on Wikipedia-based multi-entity QA\ntasks demonstrate that SRAG significantly outperforms state-of-the-art\nlong-context LLMs and RAG solutions, achieving a 29.6% improvement in accuracy.\nThe results underscore the efficacy of structuring unstructured data to enhance\nLLMs' reasoning capabilities.\n","authors":["Teng Lin","Yizhang Zhu","Yuyu Luo","Nan Tang"],"pdf_url":"https://arxiv.org/pdf/2503.01346v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.12468v3","updated":"2025-03-06T11:53:49Z","published":"2024-07-17T10:40:39Z","title":"Evaluating Search Engines and Large Language Models for Answering Health\n  Questions","summary":"  Search engines (SEs) have traditionally been primary tools for information\nseeking, but the new Large Language Models (LLMs) are emerging as powerful\nalternatives, particularly for question-answering tasks. This study compares\nthe performance of four popular SEs, seven LLMs, and retrieval-augmented (RAG)\nvariants in answering 150 health-related questions from the TREC Health\nMisinformation (HM) Track. Results reveal SEs correctly answer between 50 and\n70% of questions, often hindered by many retrieval results not responding to\nthe health question. LLMs deliver higher accuracy, correctly answering about\n80% of questions, though their performance is sensitive to input prompts. RAG\nmethods significantly enhance smaller LLMs' effectiveness, improving accuracy\nby up to 30% by integrating retrieval evidence.\n","authors":["Marcos Fernández-Pichel","Juan C. Pichel","David E. Losada"],"pdf_url":"https://arxiv.org/pdf/2407.12468v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04338v1","updated":"2025-03-06T11:34:49Z","published":"2025-03-06T11:34:49Z","title":"In-depth Analysis of Graph-based RAG in a Unified Framework","summary":"  Graph-based Retrieval-Augmented Generation (RAG) has proven effective in\nintegrating external knowledge into large language models (LLMs), improving\ntheir factual accuracy, adaptability, interpretability, and trustworthiness. A\nnumber of graph-based RAG methods have been proposed in the literature.\nHowever, these methods have not been systematically and comprehensively\ncompared under the same experimental settings. In this paper, we first\nsummarize a unified framework to incorporate all graph-based RAG methods from a\nhigh-level perspective. We then extensively compare representative graph-based\nRAG methods over a range of questing-answering (QA) datasets -- from specific\nquestions to abstract questions -- and examine the effectiveness of all\nmethods, providing a thorough analysis of graph-based RAG approaches. As a\nbyproduct of our experimental analysis, we are also able to identify new\nvariants of the graph-based RAG methods over specific QA and abstract QA tasks\nrespectively, by combining existing techniques, which outperform the\nstate-of-the-art methods. Finally, based on these findings, we offer promising\nresearch opportunities. We believe that a deeper understanding of the behavior\nof existing methods can provide new valuable insights for future research.\n","authors":["Yingli Zhou","Yaodong Su","Youran Sun","Shu Wang","Taotao Wang","Runyuan He","Yongwei Zhang","Sicong Liang","Xilin Liu","Yuchi Ma","Yixiang Fang"],"pdf_url":"https://arxiv.org/pdf/2503.04338v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.01924v2","updated":"2025-03-06T10:39:48Z","published":"2024-05-03T08:34:13Z","title":"Semi-Parametric Retrieval via Binary Bag-of-Tokens Index","summary":"  Information retrieval has transitioned from standalone systems into essential\ncomponents across broader applications, with indexing efficiency,\ncost-effectiveness, and freshness becoming increasingly critical yet often\noverlooked. In this paper, we introduce SemI-parametric Disentangled Retrieval\n(SiDR), a bi-encoder retrieval framework that decouples retrieval index from\nneural parameters to enable efficient, low-cost, and parameter-agnostic\nindexing for emerging use cases. Specifically, in addition to using embeddings\nas indexes like existing neural retrieval methods, SiDR supports a\nnon-parametric tokenization index for search, achieving BM25-like indexing\ncomplexity with significantly better effectiveness. Our comprehensive\nevaluation across 16 retrieval benchmarks demonstrates that SiDR outperforms\nboth neural and term-based retrieval baselines under the same indexing\nworkload: (i) When using an embedding-based index, SiDR exceeds the performance\nof conventional neural retrievers while maintaining similar training\ncomplexity; (ii) When using a tokenization-based index, SiDR drastically\nreduces indexing cost and time, matching the complexity of traditional\nterm-based retrieval, while consistently outperforming BM25 on all in-domain\ndatasets; (iii) Additionally, we introduce a late parametric mechanism that\nmatches BM25 index preparation time while outperforming other neural retrieval\nbaselines in effectiveness.\n","authors":["Jiawei Zhou","Li Dong","Furu Wei","Lei Chen"],"pdf_url":"https://arxiv.org/pdf/2405.01924v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04188v1","updated":"2025-03-06T08:03:51Z","published":"2025-03-06T08:03:51Z","title":"Measuring temporal effects of agent knowledge by date-controlled tool\n  use","summary":"  Temporal progression is an integral part of knowledge accumulation and\nupdate. Web search is frequently adopted as grounding for agent knowledge, yet\nits inappropriate configuration affects the quality of agent responses. Here,\nwe construct a tool-based out-of-sample testing framework to measure the\nknowledge variability of large language model (LLM) agents from distinct\ndate-controlled tools (DCTs). We demonstrate the temporal effects of an LLM\nagent as a writing assistant, which can use web search to help complete\nscientific publication abstracts. We show that temporal effects of the search\nengine translates into tool-dependent agent performance but can be alleviated\nwith base model choice and explicit reasoning instructions such as\nchain-of-thought prompting. Our results indicate that agent evaluation should\ntake a dynamical view and account for the temporal influence of tools and the\nupdates of external resources.\n","authors":["R. Patrick Xian","Qiming Cui","Stefan Bauer","Reza Abbasi-Asl"],"pdf_url":"https://arxiv.org/pdf/2503.04188v1.pdf","comment":"comments welcome"},{"id":"http://arxiv.org/abs/2503.04162v1","updated":"2025-03-06T07:25:19Z","published":"2025-03-06T07:25:19Z","title":"Semantic Retrieval Augmented Contrastive Learning for Sequential\n  Recommendation","summary":"  Sequential recommendation aims to model user preferences based on historical\nbehavior sequences, which is crucial for various online platforms. Data\nsparsity remains a significant challenge in this area as most users have\nlimited interactions and many items receive little attention. To mitigate this\nissue, contrastive learning has been widely adopted. By constructing positive\nsample pairs from the data itself and maximizing their agreement in the\nembedding space,it can leverage available data more effectively. Constructing\nreasonable positive sample pairs is crucial for the success of contrastive\nlearning. However, current approaches struggle to generate reliable positive\npairs as they either rely on representations learned from inherently sparse\ncollaborative signals or use random perturbations which introduce significant\nuncertainty. To address these limitations, we propose a novel approach named\nSemantic Retrieval Augmented Contrastive Learning (SRA-CL), which leverages\nsemantic information to improve the reliability of contrastive samples. SRA-CL\ncomprises two main components: (1) Cross-Sequence Contrastive Learning via User\nSemantic Retrieval, which utilizes large language models (LLMs) to understand\ndiverse user preferences and retrieve semantically similar users to form\nreliable positive samples through a learnable sample synthesis method; and (2)\nIntra-Sequence Contrastive Learning via Item Semantic Retrieval, which employs\nLLMs to comprehend items and retrieve similar items to perform semantic-based\nitem substitution, thereby creating semantically consistent augmented views for\ncontrastive learning. SRA-CL is plug-and-play and can be integrated into\nstandard sequential recommendation models. Extensive experiments on four public\ndatasets demonstrate the effectiveness and generalizability of the proposed\napproach.\n","authors":["Ziqiang Cui","Yunpeng Weng","Xing Tang","Xiaokun Zhang","Dugang Liu","Shiwei Li","Peiyang Liu","Bowei He","Weihong Luo","Xiuqiang He","Chen Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04162v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04141v1","updated":"2025-03-06T06:39:25Z","published":"2025-03-06T06:39:25Z","title":"HEISIR: Hierarchical Expansion of Inverted Semantic Indexing for\n  Training-free Retrieval of Conversational Data using LLMs","summary":"  The growth of conversational AI services has increased demand for effective\ninformation retrieval from dialogue data. However, existing methods often face\nchallenges in capturing semantic intent or require extensive labeling and\nfine-tuning. This paper introduces HEISIR (Hierarchical Expansion of Inverted\nSemantic Indexing for Retrieval), a novel framework that enhances semantic\nunderstanding in conversational data retrieval through optimized data\ningestion, eliminating the need for resource-intensive labeling or model\nadaptation. HEISIR implements a two-step process: (1) Hierarchical Triplets\nFormulation and (2) Adjunct Augmentation, creating semantic indices consisting\nof Subject-Verb-Object-Adjunct (SVOA) quadruplets. This structured\nrepresentation effectively captures the underlying semantic information from\ndialogue content. HEISIR achieves high retrieval performance while maintaining\nlow latency during the actual retrieval process. Our experimental results\ndemonstrate that HEISIR outperforms fine-tuned models across various embedding\ntypes and language models. Beyond improving retrieval capabilities, HEISIR also\noffers opportunities for intent and topic analysis in conversational data,\nproviding a versatile solution for dialogue systems.\n","authors":["Sangyeop Kim","Hangyeul Lee","Yohan Lee"],"pdf_url":"https://arxiv.org/pdf/2503.04141v1.pdf","comment":"Accepted by NAACL 2025 (Findings)"}],"Machine Learning":[{"id":"http://arxiv.org/abs/2503.04725v1","updated":"2025-03-06T18:59:48Z","published":"2025-03-06T18:59:48Z","title":"L$^2$M: Mutual Information Scaling Law for Long-Context Language\n  Modeling","summary":"  We rigorously establish a bipartite mutual information scaling law in natural\nlanguage that governs long-range dependencies. This scaling law, which we show\nis distinct from and scales independently of the conventional two-point mutual\ninformation, is the key to understanding long-context language modeling. Using\nthis scaling law, we formulate the Long-context Language Modeling (L$^2$M)\ncondition, which relates a model's capacity for effective long context length\nmodeling to the scaling of its latent state size for storing past information.\nOur results are validated through experiments on both transformers and state\nspace models. This work establishes a theoretical foundation that guides the\ndevelopment of large language models toward longer context lengths.\n","authors":["Zhuo Chen","Oriol Mayné i Comas","Zhuotao Jin","Di Luo","Marin Soljačić"],"pdf_url":"https://arxiv.org/pdf/2503.04725v1.pdf","comment":"29 pages, 12 figures, 1 table"},{"id":"http://arxiv.org/abs/2503.04722v1","updated":"2025-03-06T18:59:23Z","published":"2025-03-06T18:59:23Z","title":"Enough Coin Flips Can Make LLMs Act Bayesian","summary":"  Large language models (LLMs) exhibit the ability to generalize given few-shot\nexamples in their input prompt, an emergent capability known as in-context\nlearning (ICL). We investigate whether LLMs utilize ICL to perform structured\nreasoning in ways that are consistent with a Bayesian framework or rely on\npattern matching. Using a controlled setting of biased coin flips, we find\nthat: (1) LLMs often possess biased priors, causing initial divergence in\nzero-shot settings, (2) in-context evidence outweighs explicit bias\ninstructions, (3) LLMs broadly follow Bayesian posterior updates, with\ndeviations primarily due to miscalibrated priors rather than flawed updates,\nand (4) attention magnitude has negligible effect on Bayesian inference. With\nsufficient demonstrations of biased coin flips via ICL, LLMs update their\npriors in a Bayesian manner.\n","authors":["Ritwik Gupta","Rodolfo Corona","Jiaxin Ge","Eric Wang","Dan Klein","Trevor Darrell","David M. Chan"],"pdf_url":"https://arxiv.org/pdf/2503.04722v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04718v1","updated":"2025-03-06T18:58:45Z","published":"2025-03-06T18:58:45Z","title":"Floxels: Fast Unsupervised Voxel Based Scene Flow Estimation","summary":"  Scene flow estimation is a foundational task for many robotic applications,\nincluding robust dynamic object detection, automatic labeling, and sensor\nsynchronization. Two types of approaches to the problem have evolved: 1)\nSupervised and 2) optimization-based methods. Supervised methods are fast\nduring inference and achieve high-quality results, however, they are limited by\nthe need for large amounts of labeled training data and are susceptible to\ndomain gaps. In contrast, unsupervised test-time optimization methods do not\nface the problem of domain gaps but usually suffer from substantial runtime,\nexhibit artifacts, or fail to converge to the right solution. In this work, we\nmitigate several limitations of existing optimization-based methods. To this\nend, we 1) introduce a simple voxel grid-based model that improves over the\nstandard MLP-based formulation in multiple dimensions and 2) introduce a new\nmultiframe loss formulation. 3) We combine both contributions in our new\nmethod, termed Floxels. On the Argoverse 2 benchmark, Floxels is surpassed only\nby EulerFlow among unsupervised methods while achieving comparable performance\nat a fraction of the computational cost. Floxels achieves a massive speedup of\nmore than ~60 - 140x over EulerFlow, reducing the runtime from a day to 10\nminutes per sequence. Over the faster but low-quality baseline, NSFP, Floxels\nachieves a speedup of ~14x.\n","authors":["David T. Hoffmann","Syed Haseeb Raza","Hanqiu Jiang","Denis Tananaev","Steffen Klingenhoefer","Martin Meinke"],"pdf_url":"https://arxiv.org/pdf/2503.04718v1.pdf","comment":"Accepted at CVPR 2025"},{"id":"http://arxiv.org/abs/2503.04715v1","updated":"2025-03-06T18:58:29Z","published":"2025-03-06T18:58:29Z","title":"Predictable Scale: Part I -- Optimal Hyperparameter Scaling Law in Large\n  Language Model Pretraining","summary":"  The impressive capabilities of Large Language Models (LLMs) across diverse\ntasks are now well-established, yet their effective deployment necessitates\ncareful hyperparameter optimization. Through extensive empirical studies\ninvolving grid searches across diverse configurations, we discover universal\nscaling laws governing these hyperparameters: optimal learning rate follows a\npower-law relationship with both model parameters and data sizes, while optimal\nbatch size scales primarily with data sizes. Our analysis reveals a convex\noptimization landscape for hyperparameters under fixed models and data size\nconditions. This convexity implies an optimal hyperparameter plateau. We\ncontribute a universal, plug-and-play optimal hyperparameter tool for the\ncommunity. Its estimated values on the test set are merely 0.07\\% away from the\nglobally optimal LLM performance found via an exhaustive search. These laws\ndemonstrate remarkable robustness across variations in model sparsity, training\ndata distribution, and model shape. To our best known, this is the first work\nthat unifies different model shapes and structures, such as Mixture-of-Experts\nmodels and dense transformers, as well as establishes optimal hyperparameter\nscaling laws across diverse data distributions. This exhaustive optimization\nprocess demands substantial computational resources, utilizing nearly one\nmillion NVIDIA H800 GPU hours to train 3,700 LLMs of varying sizes and\nhyperparameters from scratch and consuming approximately 100 trillion tokens in\ntotal. To facilitate reproducibility and further research, we will\nprogressively release all loss measurements and model checkpoints through our\ndesignated repository https://step-law.github.io/\n","authors":["Houyi Li","Wenzheng Zheng","Jingcheng Hu","Qiufeng Wang","Hanshan Zhang","Zili Wang","Yangshijie Xu","Shuigeng Zhou","Xiangyu Zhang","Daxin Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.04715v1.pdf","comment":"19 pages"},{"id":"http://arxiv.org/abs/2503.04713v1","updated":"2025-03-06T18:57:40Z","published":"2025-03-06T18:57:40Z","title":"Scaling Rich Style-Prompted Text-to-Speech Datasets","summary":"  We introduce Paralinguistic Speech Captions (ParaSpeechCaps), a large-scale\ndataset that annotates speech utterances with rich style captions. While rich\nabstract tags (e.g. guttural, nasal, pained) have been explored in small-scale\nhuman-annotated datasets, existing large-scale datasets only cover basic tags\n(e.g. low-pitched, slow, loud). We combine off-the-shelf text and speech\nembedders, classifiers and an audio language model to automatically scale rich\ntag annotations for the first time. ParaSpeechCaps covers a total of 59 style\ntags, including both speaker-level intrinsic tags and utterance-level\nsituational tags. It consists of 342 hours of human-labelled data (PSC-Base)\nand 2427 hours of automatically annotated data (PSC-Scaled). We finetune\nParler-TTS, an open-source style-prompted TTS model, on ParaSpeechCaps, and\nachieve improved style consistency (+7.9% Consistency MOS) and speech quality\n(+15.5% Naturalness MOS) over the best performing baseline that combines\nexisting rich style tag datasets. We ablate several of our dataset design\nchoices to lay the foundation for future work in this space. Our dataset,\nmodels and code are released at https://github.com/ajd12342/paraspeechcaps .\n","authors":["Anuj Diwan","Zhisheng Zheng","David Harwath","Eunsol Choi"],"pdf_url":"https://arxiv.org/pdf/2503.04713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04712v1","updated":"2025-03-06T18:57:34Z","published":"2025-03-06T18:57:34Z","title":"Efficiently Escaping Saddle Points under Generalized Smoothness via\n  Self-Bounding Regularity","summary":"  In this paper, we study the problem of non-convex optimization on functions\nthat are not necessarily smooth using first order methods. Smoothness\n(functions whose gradient and/or Hessian are Lipschitz) is not satisfied by\nmany machine learning problems in both theory and practice, motivating a recent\nline of work studying the convergence of first order methods to first order\nstationary points under appropriate generalizations of smoothness.\n  We develop a novel framework to study convergence of first order methods to\nfirst and \\textit{second} order stationary points under generalized smoothness,\nunder more general smoothness assumptions than the literature. Using our\nframework, we show appropriate variants of GD and SGD (e.g. with appropriate\nperturbations) can converge not just to first order but also \\textit{second\norder stationary points} in runtime polylogarithmic in the dimension. To our\nknowledge, our work contains the first such result, as well as the first\n'non-textbook' rate for non-convex optimization under generalized smoothness.\nWe demonstrate that several canonical non-convex optimization problems fall\nunder our setting and framework.\n","authors":["Daniel Yiming Cao","August Y. Chen","Karthik Sridharan","Benjamin Tang"],"pdf_url":"https://arxiv.org/pdf/2503.04712v1.pdf","comment":"79 pages"},{"id":"http://arxiv.org/abs/2503.04706v1","updated":"2025-03-06T18:54:42Z","published":"2025-03-06T18:54:42Z","title":"Sample-Optimal Agnostic Boosting with Unlabeled Data","summary":"  Boosting provides a practical and provably effective framework for\nconstructing accurate learning algorithms from inaccurate rules of thumb. It\nextends the promise of sample-efficient learning to settings where direct\nEmpirical Risk Minimization (ERM) may not be implementable efficiently. In the\nrealizable setting, boosting is known to offer this computational reprieve\nwithout compromising on sample efficiency. However, in the agnostic case,\nexisting boosting algorithms fall short of achieving the optimal sample\ncomplexity.\n  This paper highlights an unexpected and previously unexplored avenue of\nimprovement: unlabeled samples. We design a computationally efficient agnostic\nboosting algorithm that matches the sample complexity of ERM, given\npolynomially many additional unlabeled samples. In fact, we show that the total\nnumber of samples needed, unlabeled and labeled inclusive, is never more than\nthat for the best known agnostic boosting algorithm -- so this result is never\nworse -- while only a vanishing fraction of these need to be labeled for the\nalgorithm to succeed. This is particularly fortuitous for learning-theoretic\napplications of agnostic boosting, which often take place in the\ndistribution-specific setting, where unlabeled samples can be availed for free.\nWe detail other applications of this result in reinforcement learning.\n","authors":["Udaya Ghai","Karan Singh"],"pdf_url":"https://arxiv.org/pdf/2503.04706v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04704v1","updated":"2025-03-06T18:54:32Z","published":"2025-03-06T18:54:32Z","title":"Universality of Layer-Level Entropy-Weighted Quantization Beyond Model\n  Architecture and Size","summary":"  We present a novel approach to selective model quantization that transcends\nthe limitations of architecture-specific and size-dependent compression methods\nfor Large Language Models (LLMs) using Entropy-Weighted Quantization (EWQ). By\nanalyzing the entropy distribution across transformer blocks, EWQ determines\nwhich blocks can be safely quantized without causing significant performance\ndegradation, independent of model architecture or size. Our method outperforms\nuniform quantization approaches, maintaining Massive Multitask Language\nUnderstanding (MMLU) accuracy scores within 0.5% of unquantized models while\nreducing memory usage by up to 18%. We demonstrate the effectiveness of EWQ\nacross multiple architectures-from 1.6B to 70B parameters-showcasing consistent\nimprovements in the quality-compression trade-off regardless of model scale or\narchitectural design. A surprising finding of EWQ is its ability to reduce\nperplexity compared to unquantized models, suggesting the presence of\nbeneficial regularization through selective precision reduction. This\nimprovement holds across different model families, indicating a fundamental\nrelationship between layer-level entropy and optimal precision requirements.\nAdditionally, we introduce FastEWQ, a rapid method for entropy distribution\nanalysis that eliminates the need for loading model weights. This technique\nleverages universal characteristics of entropy distribution that persist across\nvarious architectures and scales, enabling near-instantaneous quantization\ndecisions while maintaining 80% classification accuracy with full entropy\nanalysis. Our results demonstrate that effective quantization strategies can be\ndeveloped independently of specific architectural choices or model sizes,\nopening new possibilities for efficient LLM deployment.\n","authors":["Alireza Behtash","Marijan Fofonjka","Ethan Baird","Tyler Mauer","Hossein Moghimifam","David Stout","Joel Dennison"],"pdf_url":"https://arxiv.org/pdf/2503.04704v1.pdf","comment":"29 pages, 7 figures, 14 tables; Comments are welcome"},{"id":"http://arxiv.org/abs/2503.04697v1","updated":"2025-03-06T18:43:29Z","published":"2025-03-06T18:43:29Z","title":"L1: Controlling How Long A Reasoning Model Thinks With Reinforcement\n  Learning","summary":"  Reasoning language models have shown an uncanny ability to improve\nperformance at test-time by ``thinking longer''-that is, by generating longer\nchain-of-thought sequences and hence using more compute. However, the length of\ntheir chain-of-thought reasoning is not controllable, making it impossible to\nallocate test-time compute to achieve a desired level of performance. We\nintroduce Length Controlled Policy Optimization (LCPO), a simple reinforcement\nlearning method that optimizes for accuracy and adherence to user-specified\nlength constraints. We use LCPO to train L1, a reasoning language model that\nproduces outputs satisfying a length constraint given in its prompt. L1's\nlength control allows for smoothly trading off computational cost and accuracy\non a wide range of tasks, and outperforms the state-of-the-art S1 method for\nlength control. Furthermore, we uncover an unexpected short chain-of-thought\ncapability in models trained with LCPO. For instance, our 1.5B L1 model\nsurpasses GPT-4o at equal reasoning lengths. Overall, LCPO enables precise\ncontrol over reasoning length, allowing for fine-grained allocation of\ntest-time compute and accuracy. We release code and models at\nhttps://www.cmu-l3.github.io/l1\n","authors":["Pranjal Aggarwal","Sean Welleck"],"pdf_url":"https://arxiv.org/pdf/2503.04697v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01843v2","updated":"2025-03-06T18:38:33Z","published":"2025-03-03T18:59:40Z","title":"When Can You Get Away with Low Memory Adam?","summary":"  Adam is the go-to optimizer for training modern machine learning models, but\nit requires additional memory to maintain the moving averages of the gradients\nand their squares. While various low-memory optimizers have been proposed that\nsometimes match the performance of Adam, their lack of reliability has left\nAdam as the default choice. In this work, we apply a simple layer-wise\nSignal-to-Noise Ratio (SNR) analysis to quantify when second-moment tensors can\nbe effectively replaced by their means across different dimensions. Our SNR\nanalysis reveals how architecture, training hyperparameters, and dataset\nproperties impact compressibility along Adam's trajectory, naturally leading to\n$\\textit{SlimAdam}$, a memory-efficient Adam variant. $\\textit{SlimAdam}$\ncompresses the second moments along dimensions with high SNR when feasible, and\nleaves when compression would be detrimental. Through experiments across a\ndiverse set of architectures and training scenarios, we show that\n$\\textit{SlimAdam}$ matches Adam's performance and stability while saving up to\n$98\\%$ of total second moments. Code for $\\textit{SlimAdam}$ is available at\nhttps://github.com/dayal-kalra/low-memory-adam.\n","authors":["Dayal Singh Kalra","John Kirchenbauer","Maissam Barkeshli","Tom Goldstein"],"pdf_url":"https://arxiv.org/pdf/2503.01843v2.pdf","comment":"Acknowledgement updates and minor writing edits"},{"id":"http://arxiv.org/abs/2503.04690v1","updated":"2025-03-06T18:32:35Z","published":"2025-03-06T18:32:35Z","title":"Coarse graining and reduced order models for plume ejection dynamics","summary":"  Monitoring the atmospheric dispersion of pollutants is increasingly critical\nfor environmental impact assessments. High-fidelity computational models are\noften employed to simulate plume dynamics, guiding decision-making and\nprioritizing resource deployment. However, such models can be prohibitively\nexpensive to simulate, as they require resolving turbulent flows at fine\nspatial and temporal resolutions. Moreover, there are at least two distinct\ndynamical regimes of interest in the plume: (i) the initial ejection of the\nplume where turbulent mixing is generated by the shear-driven Kelvin-Helmholtz\ninstability, and (ii) the ensuing turbulent diffusion and advection which is\noften modeled by the Gaussian plume model. We address the challenge of modeling\nthe initial plume generation. Specifically, we propose a data-driven framework\nthat identifies a reduced-order analytical model for plume dynamics -- directly\nfrom video data. We extract a time series of plume center and edge points from\nvideo snapshots and evaluate different regressions based to their extrapolation\nperformance to generate a time series of coefficients that characterize the\nplume's overall direction and spread. We regress to a sinusoidal model inspired\nby the Kelvin-Helmholtz instability for the edge points in order to identify\nthe plume's dispersion and vorticity. Overall, this reduced-order modeling\nframework provides a data-driven and lightweight approach to capture the\ndominant features of the initial nonlinear point-source plume dynamics,\nagnostic to plume type and starting only from video. The resulting model is a\npre-cursor to standard models such as the Gaussian plume model and has the\npotential to enable rapid assessment and evaluation of critical environmental\nhazards, such as methane leaks, chemical spills, and pollutant dispersal from\nsmokestacks.\n","authors":["Ike Griss Salas","Megan R. Ebers","Jake Stevens-Haas","J. Nathan Kutz"],"pdf_url":"https://arxiv.org/pdf/2503.04690v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02800v2","updated":"2025-03-06T18:30:45Z","published":"2025-03-04T17:20:43Z","title":"RAAD-LLM: Adaptive Anomaly Detection Using LLMs and RAG Integration","summary":"  Anomaly detection in complex industrial environments poses unique challenges,\nparticularly in contexts characterized by data sparsity and evolving\noperational conditions. Predictive maintenance (PdM) in such settings demands\nmethodologies that are adaptive, transferable, and capable of integrating\ndomain-specific knowledge. In this paper, we present RAAD-LLM, a novel\nframework for adaptive anomaly detection, leveraging large language models\n(LLMs) integrated with Retrieval-Augmented Generation (RAG). This approach\naddresses the aforementioned PdM challenges. By effectively utilizing\ndomain-specific knowledge, RAAD-LLM enhances the detection of anomalies in time\nseries data without requiring fine-tuning on specific datasets. The framework's\nadaptability mechanism enables it to adjust its understanding of normal\noperating conditions dynamically, thus increasing detection accuracy. We\nvalidate this methodology through a real-world application for a plastics\nmanufacturing plant and the Skoltech Anomaly Benchmark (SKAB). Results show\nsignificant improvements over our previous model with an accuracy increase from\n70.7% to 89.1% on the real-world dataset. By allowing for the enriching of\ninput series data with semantics, RAAD-LLM incorporates multimodal capabilities\nthat facilitate more collaborative decision-making between the model and plant\noperators. Overall, our findings support RAAD-LLM's ability to revolutionize\nanomaly detection methodologies in PdM, potentially leading to a paradigm shift\nin how anomaly detection is implemented across various industries.\n","authors":["Alicia Russell-Gilbert","Sudip Mittal","Shahram Rahimi","Maria Seale","Joseph Jabour","Thomas Arnold","Joshua Church"],"pdf_url":"https://arxiv.org/pdf/2503.02800v2.pdf","comment":"arXiv admin note: substantial text overlap with arXiv:2411.00914"},{"id":"http://arxiv.org/abs/2503.04687v1","updated":"2025-03-06T18:29:45Z","published":"2025-03-06T18:29:45Z","title":"Compositional World Knowledge leads to High Utility Synthetic data","summary":"  Machine learning systems struggle with robustness, under subpopulation\nshifts. This problem becomes especially pronounced in scenarios where only a\nsubset of attribute combinations is observed during training -a severe form of\nsubpopulation shift, referred as compositional shift. To address this problem,\nwe ask the following question: Can we improve the robustness by training on\nsynthetic data, spanning all possible attribute combinations? We first show\nthat training of conditional diffusion models on limited data lead to incorrect\nunderlying distribution. Therefore, synthetic data sampled from such models\nwill result in unfaithful samples and does not lead to improve performance of\ndownstream machine learning systems. To address this problem, we propose CoInD\nto reflect the compositional nature of the world by enforcing conditional\nindependence through minimizing Fisher's divergence between joint and marginal\ndistributions. We demonstrate that synthetic data generated by CoInD is\nfaithful and this translates to state-of-the-art worst-group accuracy on\ncompositional shift tasks on CelebA.\n","authors":["Sachit Gaudi","Gautam Sreekumar","Vishnu Boddeti"],"pdf_url":"https://arxiv.org/pdf/2503.04687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04684v1","updated":"2025-03-06T18:26:42Z","published":"2025-03-06T18:26:42Z","title":"Propagating Model Uncertainty through Filtering-based Probabilistic\n  Numerical ODE Solvers","summary":"  Filtering-based probabilistic numerical solvers for ordinary differential\nequations (ODEs), also known as ODE filters, have been established as efficient\nmethods for quantifying numerical uncertainty in the solution of ODEs. In\npractical applications, however, the underlying dynamical system often contains\nuncertain parameters, requiring the propagation of this model uncertainty to\nthe ODE solution. In this paper, we demonstrate that ODE filters, despite their\nprobabilistic nature, do not automatically solve this uncertainty propagation\nproblem. To address this limitation, we present a novel approach that combines\nODE filters with numerical quadrature to properly marginalize over uncertain\nparameters, while accounting for both parameter uncertainty and numerical\nsolver uncertainty. Experiments across multiple dynamical systems demonstrate\nthat the resulting uncertainty estimates closely match reference solutions.\nNotably, we show how the numerical uncertainty from the ODE solver can help\nprevent overconfidence in the propagated uncertainty estimates, especially when\nusing larger step sizes. Our results illustrate that probabilistic numerical\nmethods can effectively quantify both numerical and parametric uncertainty in\ndynamical systems.\n","authors":["Dingling Yao","Filip Tronarp","Nathanael Bosch"],"pdf_url":"https://arxiv.org/pdf/2503.04684v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04680v1","updated":"2025-03-06T18:22:46Z","published":"2025-03-06T18:22:46Z","title":"Matrix Factorization for Inferring Associations and Missing Links","summary":"  Missing link prediction is a method for network analysis, with applications\nin recommender systems, biology, social sciences, cybersecurity, information\nretrieval, and Artificial Intelligence (AI) reasoning in Knowledge Graphs.\nMissing link prediction identifies unseen but potentially existing connections\nin a network by analyzing the observed patterns and relationships. In\nproliferation detection, this supports efforts to identify and characterize\nattempts by state and non-state actors to acquire nuclear weapons or associated\ntechnology - a notoriously challenging but vital mission for global security.\nDimensionality reduction techniques like Non-Negative Matrix Factorization\n(NMF) and Logistic Matrix Factorization (LMF) are effective but require\nselection of the matrix rank parameter, that is, of the number of hidden\nfeatures, k, to avoid over/under-fitting. We introduce novel Weighted (WNMFk),\nBoolean (BNMFk), and Recommender (RNMFk) matrix factorization methods, along\nwith ensemble variants incorporating logistic factorization, for link\nprediction. Our methods integrate automatic model determination for rank\nestimation by evaluating stability and accuracy using a modified bootstrap\nmethodology and uncertainty quantification (UQ), assessing prediction\nreliability under random perturbations. We incorporate Otsu threshold selection\nand k-means clustering for Boolean matrix factorization, comparing them to\ncoordinate descent-based Boolean thresholding. Our experiments highlight the\nimpact of rank k selection, evaluate model performance under varying test-set\nsizes, and demonstrate the benefits of UQ for reliable predictions using\nabstention. We validate our methods on three synthetic datasets (Boolean and\nuniformly distributed) and benchmark them against LMF and symmetric LMF\n(symLMF) on five real-world protein-protein interaction networks, showcasing an\nimproved prediction performance.\n","authors":["Ryan Barron","Maksim E. Eren","Duc P. Truong","Cynthia Matuszek","James Wendelberger","Mary F. Dorn","Boian Alexandrov"],"pdf_url":"https://arxiv.org/pdf/2503.04680v1.pdf","comment":"35 pages, 14 figures, 3 tables, 1 algorithm"},{"id":"http://arxiv.org/abs/2503.04679v1","updated":"2025-03-06T18:22:29Z","published":"2025-03-06T18:22:29Z","title":"Multi-Agent Inverse Q-Learning from Demonstrations","summary":"  When reward functions are hand-designed, deep reinforcement learning\nalgorithms often suffer from reward misspecification, causing them to learn\nsuboptimal policies in terms of the intended task objectives. In the\nsingle-agent case, inverse reinforcement learning (IRL) techniques attempt to\naddress this issue by inferring the reward function from expert demonstrations.\nHowever, in multi-agent problems, misalignment between the learned and true\nobjectives is exacerbated due to increased environment non-stationarity and\nvariance that scales with multiple agents. As such, in multi-agent general-sum\ngames, multi-agent IRL algorithms have difficulty balancing cooperative and\ncompetitive objectives. To address these issues, we propose Multi-Agent\nMarginal Q-Learning from Demonstrations (MAMQL), a novel sample-efficient\nframework for multi-agent IRL. For each agent, MAMQL learns a critic\nmarginalized over the other agents' policies, allowing for a well-motivated use\nof Boltzmann policies in the multi-agent context. We identify a connection\nbetween optimal marginalized critics and single-agent soft-Q IRL, allowing us\nto apply a direct, simple optimization criterion from the single-agent domain.\nAcross our experiments on three different simulated domains, MAMQL\nsignificantly outperforms previous multi-agent methods in average reward,\nsample efficiency, and reward recovery by often more than 2-5x. We make our\ncode available at https://sites.google.com/view/mamql .\n","authors":["Nathaniel Haynam","Adam Khoja","Dhruv Kumar","Vivek Myers","Erdem Bıyık"],"pdf_url":"https://arxiv.org/pdf/2503.04679v1.pdf","comment":"8 pages, 4 figures, 2 tables. Published at the International\n  Conference on Robotics and Automation (ICRA) 2025"},{"id":"http://arxiv.org/abs/2410.06186v4","updated":"2025-03-06T18:20:00Z","published":"2024-10-08T16:51:10Z","title":"The Last Iterate Advantage: Empirical Auditing and Principled Heuristic\n  Analysis of Differentially Private SGD","summary":"  We propose a simple heuristic privacy analysis of noisy clipped stochastic\ngradient descent (DP-SGD) in the setting where only the last iterate is\nreleased and the intermediate iterates remain hidden. Namely, our heuristic\nassumes a linear structure for the model.\n  We show experimentally that our heuristic is predictive of the outcome of\nprivacy auditing applied to various training procedures. Thus it can be used\nprior to training as a rough estimate of the final privacy leakage. We also\nprobe the limitations of our heuristic by providing some artificial\ncounterexamples where it underestimates the privacy leakage.\n  The standard composition-based privacy analysis of DP-SGD effectively assumes\nthat the adversary has access to all intermediate iterates, which is often\nunrealistic. However, this analysis remains the state of the art in practice.\nWhile our heuristic does not replace a rigorous privacy analysis, it\nillustrates the large gap between the best theoretical upper bounds and the\nprivacy auditing lower bounds and sets a target for further work to improve the\ntheoretical privacy analyses. We also empirically support our heuristic and\nshow existing privacy auditing attacks are bounded by our heuristic analysis in\nboth vision and language tasks.\n","authors":["Thomas Steinke","Milad Nasr","Arun Ganesh","Borja Balle","Christopher A. Choquette-Choo","Matthew Jagielski","Jamie Hayes","Abhradeep Guha Thakurta","Adam Smith","Andreas Terzis"],"pdf_url":"https://arxiv.org/pdf/2410.06186v4.pdf","comment":"ICLR 2025 camera-ready version"},{"id":"http://arxiv.org/abs/2402.10065v2","updated":"2025-03-06T18:17:02Z","published":"2024-02-15T16:30:55Z","title":"Some Targets Are Harder to Identify than Others: Quantifying the\n  Target-dependent Membership Leakage","summary":"  In a Membership Inference (MI) game, an attacker tries to infer whether a\ntarget point was included or not in the input of an algorithm. Existing works\nshow that some target points are easier to identify, while others are harder.\nThis paper explains the target-dependent hardness of membership attacks by\nstudying the powers of the optimal attacks in a fixed-target MI game. We\ncharacterise the optimal advantage and trade-off functions of attacks against\nthe empirical mean in terms of the Mahalanobis distance between the target\npoint and the data-generating distribution. We further derive the impacts of\ntwo privacy defences, i.e. adding Gaussian noise and sub-sampling, and that of\ntarget misspecification on optimal attacks. As by-products of our novel\nanalysis of the Likelihood Ratio (LR) test, we provide a new covariance attack\nwhich generalises and improves the scalar product attack. Also, we propose a\nnew optimal canary-choosing strategy for auditing privacy in the white-box\nfederated learning setting. Our experiments validate that the Mahalanobis score\nexplains the hardness of fixed-target MI games.\n","authors":["Achraf Azize","Debabrota Basu"],"pdf_url":"https://arxiv.org/pdf/2402.10065v2.pdf","comment":"Appears in AISTATS 2025 (Oral)"},{"id":"http://arxiv.org/abs/2502.02067v2","updated":"2025-03-06T18:09:38Z","published":"2025-02-04T07:32:39Z","title":"AdaptBot: Combining LLM with Knowledge Graphs and Human Input for\n  Generic-to-Specific Task Decomposition and Knowledge Refinement","summary":"  An embodied agent assisting humans is often asked to complete new tasks, and\nthere may not be sufficient time or labeled examples to train the agent to\nperform these new tasks. Large Language Models (LLMs) trained on considerable\nknowledge across many domains can be used to predict a sequence of abstract\nactions for completing such tasks, although the agent may not be able to\nexecute this sequence due to task-, agent-, or domain-specific constraints. Our\nframework addresses these challenges by leveraging the generic predictions\nprovided by LLM and the prior domain knowledge encoded in a Knowledge Graph\n(KG), enabling an agent to quickly adapt to new tasks. The robot also solicits\nand uses human input as needed to refine its existing knowledge. Based on\nexperimental evaluation in the context of cooking and cleaning tasks in\nsimulation domains, we demonstrate that the interplay between LLM, KG, and\nhuman input leads to substantial performance gains compared with just using the\nLLM. Project website{\\S}: https://sssshivvvv.github.io/adaptbot/\n","authors":["Shivam Singh","Karthik Swaminathan","Nabanita Dash","Ramandeep Singh","Snehasis Banerjee","Mohan Sridharan","Madhava Krishna"],"pdf_url":"https://arxiv.org/pdf/2502.02067v2.pdf","comment":"Accepted to IEEE International Conference on Robotics and Automation\n  (ICRA) 2025"},{"id":"http://arxiv.org/abs/2502.12360v2","updated":"2025-03-06T18:07:00Z","published":"2025-02-17T22:50:45Z","title":"Detecting Systematic Weaknesses in Vision Models along Predefined\n  Human-Understandable Dimensions","summary":"  Slice discovery methods (SDMs) are prominent algorithms for finding\nsystematic weaknesses in DNNs. They identify top-k semantically coherent\nslices/subsets of data where a DNN-under-test has low performance. For being\ndirectly useful, slices should be aligned with human-understandable and\nrelevant dimensions, which, for example, are defined by safety and domain\nexperts as part of the operational design domain (ODD). While SDMs can be\napplied effectively on structured data, their application on image data is\ncomplicated by the lack of semantic metadata. To address these issues, we\npresent an algorithm that combines foundation models for zero-shot image\nclassification to generate semantic metadata with methods for combinatorial\nsearch to find systematic weaknesses in images. In contrast to existing\napproaches, ours identifies weak slices that are in line with pre-defined\nhuman-understandable dimensions. As the algorithm includes foundation models,\nits intermediate and final results may not always be exact. Therefore, we\ninclude an approach to address the impact of noisy metadata. We validate our\nalgorithm on both synthetic and real-world datasets, demonstrating its ability\nto recover human-understandable systematic weaknesses. Furthermore, using our\napproach, we identify systematic weaknesses of multiple pre-trained and\npublicly available state-of-the-art computer vision DNNs.\n","authors":["Sujan Sai Gannamaneni","Rohil Prakash Rao","Michael Mock","Maram Akila","Stefan Wrobel"],"pdf_url":"https://arxiv.org/pdf/2502.12360v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04667v1","updated":"2025-03-06T17:59:51Z","published":"2025-03-06T17:59:51Z","title":"An Information-theoretic Multi-task Representation Learning Framework\n  for Natural Language Understanding","summary":"  This paper proposes a new principled multi-task representation learning\nframework (InfoMTL) to extract noise-invariant sufficient representations for\nall tasks. It ensures sufficiency of shared representations for all tasks and\nmitigates the negative effect of redundant features, which can enhance language\nunderstanding of pre-trained language models (PLMs) under the multi-task\nparadigm. Firstly, a shared information maximization principle is proposed to\nlearn more sufficient shared representations for all target tasks. It can avoid\nthe insufficiency issue arising from representation compression in the\nmulti-task paradigm. Secondly, a task-specific information minimization\nprinciple is designed to mitigate the negative effect of potential redundant\nfeatures in the input for each task. It can compress task-irrelevant redundant\ninformation and preserve necessary information relevant to the target for\nmulti-task prediction. Experiments on six classification benchmarks show that\nour method outperforms 12 comparative multi-task methods under the same\nmulti-task settings, especially in data-constrained and noisy scenarios.\nExtensive experiments demonstrate that the learned representations are more\nsufficient, data-efficient, and robust.\n","authors":["Dou Hu","Lingwei Wei","Wei Zhou","Songlin Hu"],"pdf_url":"https://arxiv.org/pdf/2503.04667v1.pdf","comment":"11 pages, accepted to AAAI 2025 (main conference), the code is\n  available at https://github.com/zerohd4869/InfoMTL"},{"id":"http://arxiv.org/abs/2503.04655v1","updated":"2025-03-06T17:49:13Z","published":"2025-03-06T17:49:13Z","title":"CLDyB: Towards Dynamic Benchmarking for Continual Learning with\n  Pre-trained Models","summary":"  The advent of the foundation model era has sparked significant research\ninterest in leveraging pre-trained representations for continual learning (CL),\nyielding a series of top-performing CL methods on standard evaluation\nbenchmarks. Nonetheless, there are growing concerns regarding potential data\ncontamination during the pre-training stage. Furthermore, standard evaluation\nbenchmarks, which are typically static, fail to capture the complexities of\nreal-world CL scenarios, resulting in saturated performance. To address these\nissues, we describe CL on dynamic benchmarks (CLDyB), a general computational\nframework based on Markov decision processes for evaluating CL methods\nreliably. CLDyB dynamically identifies inherently difficult and\nalgorithm-dependent tasks for the given CL methods, and determines challenging\ntask orders using Monte Carlo tree search. Leveraging CLDyB, we first conduct a\njoint evaluation of multiple state-of-the-art CL methods, leading to a set of\ncommonly challenging and generalizable task sequences where existing CL methods\ntend to perform poorly. We then conduct separate evaluations of individual CL\nmethods using CLDyB, discovering their respective strengths and weaknesses. The\nsource code and generated task sequences are publicly accessible at\nhttps://github.com/szc12153/CLDyB.\n","authors":["Shengzhuang Chen","Yikai Liao","Xiaoxiao Sun","Kede Ma","Ying Wei"],"pdf_url":"https://arxiv.org/pdf/2503.04655v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04650v1","updated":"2025-03-06T17:39:12Z","published":"2025-03-06T17:39:12Z","title":"Joint Masked Reconstruction and Contrastive Learning for Mining\n  Interactions Between Proteins","summary":"  Protein-protein interaction (PPI) prediction is an instrumental means in\nelucidating the mechanisms underlying cellular operations, holding significant\npractical implications for the realms of pharmaceutical development and\nclinical treatment. Presently, the majority of research methods primarily\nconcentrate on the analysis of amino acid sequences, while investigations\npredicated on protein structures remain in the nascent stages of exploration.\nDespite the emergence of several structure-based algorithms in recent years,\nthese are still confronted with inherent challenges: (1) the extraction of\nintrinsic structural information of proteins typically necessitates the\nexpenditure of substantial computational resources; (2) these models are overly\nreliant on seen protein data, struggling to effectively unearth interaction\ncues between unknown proteins. To further propel advancements in this domain,\nthis paper introduces a novel PPI prediction method jointing masked\nreconstruction and contrastive learning, termed JmcPPI. This methodology\ndissects the PPI prediction task into two distinct phases: during the residue\nstructure encoding phase, JmcPPI devises two feature reconstruction tasks and\nemploys graph attention mechanism to capture structural information between\nresidues; during the protein interaction inference phase, JmcPPI perturbs the\noriginal PPI graph and employs a multi-graph contrastive learning strategy to\nthoroughly mine extrinsic interaction information of novel proteins. Extensive\nexperiments conducted on three widely utilized PPI datasets demonstrate that\nJmcPPI surpasses existing optimal baseline models across various data partition\nschemes. The associated code can be accessed via\nhttps://github.com/lijfrank-open/JmcPPI.\n","authors":["Jiang Li","Xiaoping Wang"],"pdf_url":"https://arxiv.org/pdf/2503.04650v1.pdf","comment":"Submitted"},{"id":"http://arxiv.org/abs/2503.04649v1","updated":"2025-03-06T17:35:37Z","published":"2025-03-06T17:35:37Z","title":"Transferable Foundation Models for Geometric Tasks on Point Cloud\n  Representations: Geometric Neural Operators","summary":"  We introduce methods for obtaining pretrained Geometric Neural Operators\n(GNPs) that can serve as basal foundation models for use in obtaining geometric\nfeatures. These can be used within data processing pipelines for machine\nlearning tasks and numerical methods. We show how our GNPs can be trained to\nlearn robust latent representations for the differential geometry of\npoint-clouds to provide estimates of metric, curvature, and other shape-related\nfeatures. We demonstrate how our pre-trained GNPs can be used (i) to estimate\nthe geometric properties of surfaces of arbitrary shape and topologies with\nrobustness in the presence of noise, (ii) to approximate solutions of geometric\npartial differential equations (PDEs) on manifolds, and (iii) to solve\nequations for shape deformations such as curvature driven flows. We also\nrelease a package of the codes and weights for using our pre-trained GNPs for\nprocessing point cloud representations. This allows for incorporating our\npre-trained GNPs as components for reuse within existing and new data\nprocessing pipelines. The GNPs also can be used as part of numerical solvers\ninvolving geometry or as part of methods for performing inference and other\ngeometric tasks.\n","authors":["Blaine Quackenbush","Paul J. Atzberger"],"pdf_url":"https://arxiv.org/pdf/2503.04649v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.04873v2","updated":"2025-03-06T17:35:19Z","published":"2025-01-08T23:07:10Z","title":"Back Home: A Machine Learning Approach to Seashell Classification and\n  Ecosystem Restoration","summary":"  In Costa Rica, an average of 5 tons of seashells are extracted from\necosystems annually. Confiscated seashells, cannot be returned to their\necosystems due to the lack of origin recognition. To address this issue, we\ndeveloped a convolutional neural network (CNN) specifically for seashell\nidentification. We built a dataset from scratch, consisting of approximately\n19000 images from the Pacific and Caribbean coasts. Using this dataset, the\nmodel achieved a classification accuracy exceeding 85%. The model has been\nintegrated into a user-friendly application, which has classified over 36,000\nseashells to date, delivering real-time results within 3 seconds per image. To\nfurther enhance the system's accuracy, an anomaly detection mechanism was\nincorporated to filter out irrelevant or anomalous inputs, ensuring only valid\nseashell images are processed.\n","authors":["Alexander Valverde","Luis Solano"],"pdf_url":"https://arxiv.org/pdf/2501.04873v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04641v1","updated":"2025-03-06T17:31:43Z","published":"2025-03-06T17:31:43Z","title":"Simulating the Real World: A Unified Survey of Multimodal Generative\n  Models","summary":"  Understanding and replicating the real world is a critical challenge in\nArtificial General Intelligence (AGI) research. To achieve this, many existing\napproaches, such as world models, aim to capture the fundamental principles\ngoverning the physical world, enabling more accurate simulations and meaningful\ninteractions. However, current methods often treat different modalities,\nincluding 2D (images), videos, 3D, and 4D representations, as independent\ndomains, overlooking their interdependencies. Additionally, these methods\ntypically focus on isolated dimensions of reality without systematically\nintegrating their connections. In this survey, we present a unified survey for\nmultimodal generative models that investigate the progression of data\ndimensionality in real-world simulation. Specifically, this survey starts from\n2D generation (appearance), then moves to video (appearance+dynamics) and 3D\ngeneration (appearance+geometry), and finally culminates in 4D generation that\nintegrate all dimensions. To the best of our knowledge, this is the first\nattempt to systematically unify the study of 2D, video, 3D and 4D generation\nwithin a single framework. To guide future research, we provide a comprehensive\nreview of datasets, evaluation metrics and future directions, and fostering\ninsights for newcomers. This survey serves as a bridge to advance the study of\nmultimodal generative models and real-world simulation within a unified\nframework.\n","authors":["Yuqi Hu","Longguang Wang","Xian Liu","Ling-Hao Chen","Yuwei Guo","Yukai Shi","Ce Liu","Anyi Rao","Zeyu Wang","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.04641v1.pdf","comment":"Repository for the related papers at\n  https://github.com/ALEEEHU/World-Simulator"},{"id":"http://arxiv.org/abs/2503.04639v1","updated":"2025-03-06T17:28:48Z","published":"2025-03-06T17:28:48Z","title":"Enhancing SAM with Efficient Prompting and Preference Optimization for\n  Semi-supervised Medical Image Segmentation","summary":"  Foundational models such as the Segment Anything Model (SAM) are gaining\ntraction in medical imaging segmentation, supporting multiple downstream tasks.\nHowever, such models are supervised in nature, still relying on large annotated\ndatasets or prompts supplied by experts. Conventional techniques such as active\nlearning to alleviate such limitations are limited in scope and still\nnecessitate continuous human involvement and complex domain knowledge for label\nrefinement or establishing reward ground truth. To address these challenges, we\npropose an enhanced Segment Anything Model (SAM) framework that utilizes\nannotation-efficient prompts generated in a fully unsupervised fashion, while\nstill capturing essential semantic, location, and shape information through\ncontrastive language-image pretraining and visual question answering. We adopt\nthe direct preference optimization technique to design an optimal policy that\nenables the model to generate high-fidelity segmentations with simple ratings\nor rankings provided by a virtual annotator simulating the human annotation\nprocess. State-of-the-art performance of our framework in tasks such as lung\nsegmentation, breast tumor segmentation, and organ segmentation across various\nmodalities, including X-ray, ultrasound, and abdominal CT, justifies its\neffectiveness in low-annotation data scenarios.\n","authors":["Aishik Konwer","Zhijian Yang","Erhan Bas","Cao Xiao","Prateek Prasanna","Parminder Bhatia","Taha Kass-Hout"],"pdf_url":"https://arxiv.org/pdf/2503.04639v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2503.04638v1","updated":"2025-03-06T17:25:46Z","published":"2025-03-06T17:25:46Z","title":"No Forgetting Learning: Memory-free Continual Learning","summary":"  Continual Learning (CL) remains a central challenge in deep learning, where\nmodels must sequentially acquire new knowledge while mitigating Catastrophic\nForgetting (CF) of prior tasks. Existing approaches often struggle with\nefficiency and scalability, requiring extensive memory or model buffers. This\nwork introduces ``No Forgetting Learning\" (NFL), a memory-free CL framework\nthat leverages knowledge distillation to maintain stability while preserving\nplasticity. Memory-free means the NFL does not rely on any memory buffer.\nThrough extensive evaluations of three benchmark datasets, we demonstrate that\nNFL achieves competitive performance while utilizing approximately 14.75 times\nless memory than state-of-the-art methods. Furthermore, we introduce a new\nmetric to better assess CL's plasticity-stability trade-off.\n","authors":["Mohammad Ali Vahedifar","Qi Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04638v1.pdf","comment":"This paper is submitted to ICCV 2025"},{"id":"http://arxiv.org/abs/2202.00665v4","updated":"2025-03-06T17:24:46Z","published":"2022-02-01T18:58:33Z","title":"Tutorial on amortized optimization","summary":"  Optimization is a ubiquitous modeling tool and is often deployed in settings\nwhich repeatedly solve similar instances of the same problem. Amortized\noptimization methods use learning to predict the solutions to problems in these\nsettings, exploiting the shared structure between similar problem instances.\nThese methods have been crucial in variational inference and reinforcement\nlearning and are capable of solving optimization problems many orders of\nmagnitudes times faster than traditional optimization methods that do not use\namortization. This tutorial presents an introduction to the amortized\noptimization foundations behind these advancements and overviews their\napplications in variational inference, sparse coding, gradient-based\nmeta-learning, control, reinforcement learning, convex optimization, optimal\ntransport, and deep equilibrium networks. The source code for this tutorial is\navailable at\nhttps://github.com/facebookresearch/amortized-optimization-tutorial.\n","authors":["Brandon Amos"],"pdf_url":"https://arxiv.org/pdf/2202.00665v4.pdf","comment":"Foundations and Trends in Machine Learning"},{"id":"http://arxiv.org/abs/2503.04636v1","updated":"2025-03-06T17:24:06Z","published":"2025-03-06T17:24:06Z","title":"Mark Your LLM: Detecting the Misuse of Open-Source Large Language Models\n  via Watermarking","summary":"  As open-source large language models (LLMs) like Llama3 become more capable,\nit is crucial to develop watermarking techniques to detect their potential\nmisuse. Existing watermarking methods either add watermarks during LLM\ninference, which is unsuitable for open-source LLMs, or primarily target\nclassification LLMs rather than recent generative LLMs. Adapting these\nwatermarks to open-source LLMs for misuse detection remains an open challenge.\nThis work defines two misuse scenarios for open-source LLMs: intellectual\nproperty (IP) violation and LLM Usage Violation. Then, we explore the\napplication of inference-time watermark distillation and backdoor watermarking\nin these contexts. We propose comprehensive evaluation methods to assess the\nimpact of various real-world further fine-tuning scenarios on watermarks and\nthe effect of these watermarks on LLM performance. Our experiments reveal that\nbackdoor watermarking could effectively detect IP Violation, while\ninference-time watermark distillation is applicable in both scenarios but less\nrobust to further fine-tuning and has a more significant impact on LLM\nperformance compared to backdoor watermarking. Exploring more advanced\nwatermarking methods for open-source LLMs to detect their misuse should be an\nimportant future direction.\n","authors":["Yijie Xu","Aiwei Liu","Xuming Hu","Lijie Wen","Hui Xiong"],"pdf_url":"https://arxiv.org/pdf/2503.04636v1.pdf","comment":"Accepted by the 1st Workshop on GenAI Watermarking, collocated with\n  ICLR 2025"},{"id":"http://arxiv.org/abs/2503.00897v3","updated":"2025-03-06T17:19:22Z","published":"2025-03-02T13:43:53Z","title":"A Simple and Effective Reinforcement Learning Method for Text-to-Image\n  Diffusion Fine-tuning","summary":"  Reinforcement learning (RL)-based fine-tuning has emerged as a powerful\napproach for aligning diffusion models with black-box objectives. Proximal\npolicy optimization (PPO) is the most popular choice of method for policy\noptimization. While effective in terms of performance, PPO is highly sensitive\nto hyper-parameters and involves substantial computational overhead. REINFORCE,\non the other hand, mitigates some computational complexities such as high\nmemory overhead and sensitive hyper-parameter tuning, but has suboptimal\nperformance due to high-variance and sample inefficiency. While the variance of\nthe REINFORCE can be reduced by sampling multiple actions per input prompt and\nusing a baseline correction term, it still suffers from sample inefficiency. To\naddress these challenges, we systematically analyze the\nefficiency-effectiveness trade-off between REINFORCE and PPO, and propose\nleave-one-out PPO (LOOP), a novel RL for diffusion fine-tuning method. LOOP\ncombines variance reduction techniques from REINFORCE, such as sampling\nmultiple actions per input prompt and a baseline correction term, with the\nrobustness and sample efficiency of PPO via clipping and importance sampling.\nOur results demonstrate that LOOP effectively improves diffusion models on\nvarious black-box objectives, and achieves a better balance between\ncomputational efficiency and performance.\n","authors":["Shashank Gupta","Chaitanya Ahuja","Tsung-Yu Lin","Sreya Dutta Roy","Harrie Oosterhuis","Maarten de Rijke","Satya Narayan Shukla"],"pdf_url":"https://arxiv.org/pdf/2503.00897v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04626v1","updated":"2025-03-06T17:12:46Z","published":"2025-03-06T17:12:46Z","title":"IDInit: A Universal and Stable Initialization Method for Neural Network\n  Training","summary":"  Deep neural networks have achieved remarkable accomplishments in practice.\nThe success of these networks hinges on effective initialization methods, which\nare vital for ensuring stable and rapid convergence during training. Recently,\ninitialization methods that maintain identity transition within layers have\nshown good efficiency in network training. These techniques (e.g., Fixup) set\nspecific weights to zero to achieve identity control. However, settings of\nremaining weight (e.g., Fixup uses random values to initialize non-zero\nweights) will affect the inductive bias that is achieved only by a zero weight,\nwhich may be harmful to training. Addressing this concern, we introduce fully\nidentical initialization (IDInit), a novel method that preserves identity in\nboth the main and sub-stem layers of residual networks. IDInit employs a padded\nidentity-like matrix to overcome rank constraints in non-square weight\nmatrices. Furthermore, we show the convergence problem of an identity matrix\ncan be solved by stochastic gradient descent. Additionally, we enhance the\nuniversality of IDInit by processing higher-order weights and addressing dead\nneuron problems. IDInit is a straightforward yet effective initialization\nmethod, with improved convergence, stability, and performance across various\nsettings, including large-scale datasets and deep models.\n","authors":["Yu Pan","Chaozheng Wang","Zekai Wu","Qifan Wang","Min Zhang","Zenglin Xu"],"pdf_url":"https://arxiv.org/pdf/2503.04626v1.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2410.05116v2","updated":"2025-03-06T17:11:55Z","published":"2024-10-07T15:12:01Z","title":"Human-Feedback Efficient Reinforcement Learning for Online Diffusion\n  Model Finetuning","summary":"  Controllable generation through Stable Diffusion (SD) fine-tuning aims to\nimprove fidelity, safety, and alignment with human guidance. Existing\nreinforcement learning from human feedback methods usually rely on predefined\nheuristic reward functions or pretrained reward models built on large-scale\ndatasets, limiting their applicability to scenarios where collecting such data\nis costly or difficult. To effectively and efficiently utilize human feedback,\nwe develop a framework, HERO, which leverages online human feedback collected\non the fly during model learning. Specifically, HERO features two key\nmechanisms: (1) Feedback-Aligned Representation Learning, an online training\nmethod that captures human feedback and provides informative learning signals\nfor fine-tuning, and (2) Feedback-Guided Image Generation, which involves\ngenerating images from SD's refined initialization samples, enabling faster\nconvergence towards the evaluator's intent. We demonstrate that HERO is 4x more\nefficient in online feedback for body part anomaly correction compared to the\nbest existing method. Additionally, experiments show that HERO can effectively\nhandle tasks like reasoning, counting, personalization, and reducing NSFW\ncontent with only 0.5K online feedback.\n","authors":["Ayano Hiranaka","Shang-Fu Chen","Chieh-Hsin Lai","Dongjun Kim","Naoki Murata","Takashi Shibuya","Wei-Hsiang Liao","Shao-Hua Sun","Yuki Mitsufuji"],"pdf_url":"https://arxiv.org/pdf/2410.05116v2.pdf","comment":"Published in International Conference on Learning Representations\n  (ICLR) 2025"},{"id":"http://arxiv.org/abs/2412.16561v2","updated":"2025-03-06T17:04:11Z","published":"2024-12-21T10:07:40Z","title":"A learning-based approach to stochastic optimal control under\n  reach-avoid constraint","summary":"  We develop a model-free approach to optimally control stochastic, Markovian\nsystems subject to a reach-avoid constraint. Specifically, the state trajectory\nmust remain within a safe set while reaching a target set within a finite time\nhorizon. Due to the time-dependent nature of these constraints, we show that,\nin general, the optimal policy for this constrained stochastic control problem\nis non-Markovian, which increases the computational complexity. To address this\nchallenge, we apply the state-augmentation technique from arXiv:2402.19360,\nreformulating the problem as a constrained Markov decision process (CMDP) on an\nextended state space. This transformation allows us to search for a Markovian\npolicy, avoiding the complexity of non-Markovian policies. To learn the optimal\npolicy without a system model, and using only trajectory data, we develop a\nlog-barrier policy gradient approach. We prove that under suitable assumptions,\nthe policy parameters converge to the optimal parameters, while ensuring that\nthe system trajectories satisfy the stochastic reach-avoid constraint with high\nprobability.\n","authors":["Tingting Ni","Maryam Kamgarpour"],"pdf_url":"https://arxiv.org/pdf/2412.16561v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04606v1","updated":"2025-03-06T16:53:14Z","published":"2025-03-06T16:53:14Z","title":"The Best of Both Worlds: Integrating Language Models and Diffusion\n  Models for Video Generation","summary":"  Recent advancements in text-to-video (T2V) generation have been driven by two\ncompeting paradigms: autoregressive language models and diffusion models.\nHowever, each paradigm has intrinsic limitations: language models struggle with\nvisual quality and error accumulation, while diffusion models lack semantic\nunderstanding and causal modeling. In this work, we propose LanDiff, a hybrid\nframework that synergizes the strengths of both paradigms through\ncoarse-to-fine generation. Our architecture introduces three key innovations:\n(1) a semantic tokenizer that compresses 3D visual features into compact 1D\ndiscrete representations through efficient semantic compression, achieving a\n$\\sim$14,000$\\times$ compression ratio; (2) a language model that generates\nsemantic tokens with high-level semantic relationships; (3) a streaming\ndiffusion model that refines coarse semantics into high-fidelity videos.\nExperiments show that LanDiff, a 5B model, achieves a score of 85.43 on the\nVBench T2V benchmark, surpassing the state-of-the-art open-source models\nHunyuan Video (13B) and other commercial models such as Sora, Keling, and\nHailuo. Furthermore, our model also achieves state-of-the-art performance in\nlong video generation, surpassing other open-source models in this field. Our\ndemo can be viewed at https://landiff.github.io/.\n","authors":["Aoxiong Yin","Kai Shen","Yichong Leng","Xu Tan","Xinyu Zhou","Juncheng Li","Siliang Tang"],"pdf_url":"https://arxiv.org/pdf/2503.04606v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03770v1","updated":"2025-03-06T16:42:53Z","published":"2025-03-06T16:42:53Z","title":"Fusion of Various Optimization Based Feature Smoothing Methods for\n  Wearable and Non-invasive Blood Glucose Estimation","summary":"  Recently, the wearable and non-invasive blood glucose estimation approach has\nbeen proposed. However, due to the unreliability of the acquisition device, the\npresence of the noise and the variations of the acquisition environments, the\nobtained features and the reference blood glucose values are highly unreliable.\nTo address this issue, this paper proposes a polynomial fitting approach to\nsmooth the obtained features or the reference blood glucose values. First, the\nblood glucose values are estimated based on the individual optimization\napproaches. Second, the absolute difference values between the estimated blood\nglucose values and the actual blood glucose values based on each optimization\napproach are computed. Third, these absolute difference values for each\noptimization approach are sorted in the ascending order. Fourth, for each\nsorted blood glucose value, the optimization method corresponding to the\nminimum absolute difference value is selected. Fifth, the accumulate\nprobability of each selected optimization method is computed. If the accumulate\nprobability of any selected optimization method at a point is greater than a\nthreshold value, then the accumulate probabilities of these three selected\noptimization methods at that point are reset to zero. A range of the sorted\nblood glucose values are defined as that with the corresponding boundaries\npoints being the previous reset point and this reset point. Hence, after\nperforming the above procedures for all the sorted reference blood glucose\nvalues in the validation set, the regions of the sorted reference blood glucose\nvalues and the corresponding optimization methods in these regions are\ndetermined. The computer numerical simulation results show that our proposed\nmethod yields the mean absolute relative deviation (MARD) at 0.0930 and the\npercentage of the test data falling in the zone A of the Clarke error grid at\n94.1176%.\n","authors":["Yiting Wei","Bingo Wing-Kuen Ling","Danni Chen","Yuheng Dai","Qing Liu"],"pdf_url":"https://arxiv.org/pdf/2503.03770v1.pdf","comment":"This version corrects several typos"},{"id":"http://arxiv.org/abs/2503.04598v1","updated":"2025-03-06T16:40:48Z","published":"2025-03-06T16:40:48Z","title":"HybridNorm: Towards Stable and Efficient Transformer Training via Hybrid\n  Normalization","summary":"  Transformers have become the de facto architecture for a wide range of\nmachine learning tasks, particularly in large language models (LLMs). Despite\ntheir remarkable performance, challenges remain in training deep transformer\nnetworks, especially regarding the location of layer normalization. While\nPre-Norm structures facilitate easier training due to their more prominent\nidentity path, they often yield suboptimal performance compared to Post-Norm.\nIn this paper, we propose $\\textbf{HybridNorm}$, a straightforward yet\neffective hybrid normalization strategy that integrates the advantages of both\nPre-Norm and Post-Norm approaches. Specifically, HybridNorm employs QKV\nnormalization within the attention mechanism and Post-Norm in the feed-forward\nnetwork (FFN) of each transformer block. This design not only stabilizes\ntraining but also enhances performance, particularly in the context of LLMs.\nComprehensive experiments in both dense and sparse architectures show that\nHybridNorm consistently outperforms both Pre-Norm and Post-Norm approaches,\nachieving state-of-the-art results across various benchmarks. These findings\nhighlight the potential of HybridNorm as a more stable and effective technique\nfor improving the training and performance of deep transformer models. %Code\nwill be made publicly available. Code is available at\nhttps://github.com/BryceZhuo/HybridNorm.\n","authors":["Zhijian Zhuo","Yutao Zeng","Ya Wang","Sijun Zhang","Jian Yang","Xiaoqing Li","Xun Zhou","Jinwen Ma"],"pdf_url":"https://arxiv.org/pdf/2503.04598v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17412v3","updated":"2025-03-06T16:22:22Z","published":"2024-05-27T17:57:12Z","title":"Towards One Model for Classical Dimensionality Reduction: A\n  Probabilistic Perspective on UMAP and t-SNE","summary":"  This paper shows that dimensionality reduction methods such as UMAP and\nt-SNE, can be approximately recast as MAP inference methods corresponding to a\nmodel introduced in ProbDR, that describes the graph Laplacian (an estimate of\nthe data precision matrix) using a Wishart distribution, with a mean given by a\nnon-linear covariance function evaluated on the latents. This interpretation\noffers deeper theoretical and semantic insights into such algorithms, by\nshowing that variances corresponding to these covariances are low (potentially\nmisspecified), and forging a connection to Gaussian process latent variable\nmodels by showing that well-known kernels can be used to describe covariances\nimplied by graph Laplacians. We also introduce tools with which similar\ndimensionality reduction methods can be studied.\n","authors":["Aditya Ravuri","Neil D. Lawrence"],"pdf_url":"https://arxiv.org/pdf/2405.17412v3.pdf","comment":"Updated preprint"},{"id":"http://arxiv.org/abs/2503.04585v1","updated":"2025-03-06T16:22:19Z","published":"2025-03-06T16:22:19Z","title":"Advancing Solutions for the Three-Body Problem Through Physics-Informed\n  Neural Networks","summary":"  First formulated by Sir Isaac Newton in his work \"Philosophiae Naturalis\nPrincipia Mathematica\", the concept of the Three-Body Problem was put forth as\na study of the motion of the three celestial bodies within the Earth-Sun-Moon\nsystem. In a generalized definition, it seeks to predict the motion for an\nisolated system composed of three point masses freely interacting under\nNewton's law of universal attraction. This proves to be analogous to a\nmultitude of interactions between celestial bodies, and thus, the problem finds\napplicability within the studies of celestial mechanics. Despite numerous\nattempts by renowned physicists to solve it throughout the last three\ncenturies, no general closed-form solutions have been reached due to its\ninherently chaotic nature for most initial conditions. Current state-of-the-art\nsolutions are based on two approaches, either numerical high-precision\nintegration or machine learning-based. Notwithstanding the breakthroughs of\nneural networks, these present a significant limitation, which is their\nignorance of any prior knowledge of the chaotic systems presented. Thus, in\nthis work, we propose a novel method that utilizes Physics-Informed Neural\nNetworks (PINNs). These deep neural networks are able to incorporate any prior\nsystem knowledge expressible as an Ordinary Differential Equation (ODE) into\ntheir learning processes as a regularizing agent. Our findings showcase that\nPINNs surpass current state-of-the-art machine learning methods with comparable\nprediction quality. Despite a better prediction quality, the usability of\nnumerical integrators suffers due to their prohibitively high computational\ncost. These findings confirm that PINNs are both effective and time-efficient\nopen-form solvers of the Three-Body Problem that capitalize on the extensive\nknowledge we hold of classical mechanics.\n","authors":["Manuel Santos Pereira","Luís Tripa","Nélson Lima","Francisco Caldas","Cláudia Soares"],"pdf_url":"https://arxiv.org/pdf/2503.04585v1.pdf","comment":"14 pages, 25 figures, 3 tables. 75th International Astronautical\n  Congress (IAC), Milan, Italy, 14-18 October"},{"id":"http://arxiv.org/abs/2503.04582v1","updated":"2025-03-06T16:20:25Z","published":"2025-03-06T16:20:25Z","title":"PSDNorm: Test-Time Temporal Normalization for Deep Learning on EEG\n  Signals","summary":"  Distribution shift poses a significant challenge in machine learning,\nparticularly in biomedical applications such as EEG signals collected across\ndifferent subjects, institutions, and recording devices. While existing\nnormalization layers, Batch-Norm, LayerNorm and InstanceNorm, help address\ndistribution shifts, they fail to capture the temporal dependencies inherent in\ntemporal signals. In this paper, we propose PSDNorm, a layer that leverages\nMonge mapping and temporal context to normalize feature maps in deep learning\nmodels. Notably, the proposed method operates as a test-time domain adaptation\ntechnique, addressing distribution shifts without additional training.\nEvaluations on 10 sleep staging datasets using the U-Time model demonstrate\nthat PSDNorm achieves state-of-the-art performance at test time on datasets not\nseen during training while being 4x more data-efficient than the best baseline.\nAdditionally, PSDNorm provides a significant improvement in robustness,\nachieving markedly higher F1 scores for the 20% hardest subjects.\n","authors":["Théo Gnassounou","Antoine Collas","Rémi Flamary","Alexandre Gramfort"],"pdf_url":"https://arxiv.org/pdf/2503.04582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.17504v2","updated":"2025-03-06T16:14:45Z","published":"2025-02-21T19:22:10Z","title":"Protein Large Language Models: A Comprehensive Survey","summary":"  Protein-specific large language models (Protein LLMs) are revolutionizing\nprotein science by enabling more efficient protein structure prediction,\nfunction annotation, and design. While existing surveys focus on specific\naspects or applications, this work provides the first comprehensive overview of\nProtein LLMs, covering their architectures, training datasets, evaluation\nmetrics, and diverse applications. Through a systematic analysis of over 100\narticles, we propose a structured taxonomy of state-of-the-art Protein LLMs,\nanalyze how they leverage large-scale protein sequence data for improved\naccuracy, and explore their potential in advancing protein engineering and\nbiomedical research. Additionally, we discuss key challenges and future\ndirections, positioning Protein LLMs as essential tools for scientific\ndiscovery in protein science. Resources are maintained at\nhttps://github.com/Yijia-Xiao/Protein-LLM-Survey.\n","authors":["Yijia Xiao","Wanjia Zhao","Junkai Zhang","Yiqiao Jin","Han Zhang","Zhicheng Ren","Renliang Sun","Haixin Wang","Guancheng Wan","Pan Lu","Xiao Luo","Yu Zhang","James Zou","Yizhou Sun","Wei Wang"],"pdf_url":"https://arxiv.org/pdf/2502.17504v2.pdf","comment":"24 pages, 4 figures, 5 tables"},{"id":"http://arxiv.org/abs/2412.07093v2","updated":"2025-03-06T16:14:01Z","published":"2024-12-10T01:21:56Z","title":"Streaming Private Continual Counting via Binning","summary":"  In differential privacy, $\\textit{continual observation}$ refers to problems\nin which we wish to continuously release a function of a dataset that is\nrevealed one element at a time. The challenge is to maintain a good\napproximation while keeping the combined output over all time steps\ndifferentially private. In the special case of $\\textit{continual counting}$ we\nseek to approximate a sum of binary input elements. This problem has received\nconsiderable attention lately, in part due to its relevance in implementations\nof differentially private stochastic gradient descent. $\\textit{Factorization\nmechanisms}$ are the leading approach to continual counting, but the best such\nmechanisms do not work well in $\\textit{streaming}$ settings since they require\nspace proportional to the size of the input. In this paper, we present a simple\napproach to approximating factorization mechanisms in low space via\n$\\textit{binning}$, where adjacent matrix entries with similar values are\nchanged to be identical in such a way that a matrix-vector product can be\nmaintained in sublinear space. Our approach has provable sublinear space\nguarantees for a class of lower triangular matrices whose entries are\nmonotonically decreasing away from the diagonal. We show empirically that even\nwith very low space usage we are able to closely match, and sometimes surpass,\nthe performance of asymptotically optimal factorization mechanisms. Recently,\nand independently of our work, Dvijotham et al. have also suggested an approach\nto implementing factorization mechanisms in a streaming setting. Their work\ndiffers from ours in several respects: It only addresses factorization into\n$\\textit{Toeplitz}$ matrices, only considers $\\textit{maximum}$ error, and uses\na different technique based on rational function approximation that seems less\nversatile than our binning approach.\n","authors":["Joel Daniel Andersson","Rasmus Pagh"],"pdf_url":"https://arxiv.org/pdf/2412.07093v2.pdf","comment":"Accepted to SaTML 2025. Final version to appear on IEEE eXplore"},{"id":"http://arxiv.org/abs/2503.04579v1","updated":"2025-03-06T16:13:32Z","published":"2025-03-06T16:13:32Z","title":"Data-augmented Learning of Geodesic Distances in Irregular Domains\n  through Soner Boundary Conditions","summary":"  Geodesic distances play a fundamental role in robotics, as they efficiently\nencode global geometric information of the domain. Recent methods use neural\nnetworks to approximate geodesic distances by solving the Eikonal equation\nthrough physics-informed approaches. While effective, these approaches often\nsuffer from unstable convergence during training in complex environments. We\npropose a framework to learn geodesic distances in irregular domains by using\nthe Soner boundary condition, and systematically evaluate the impact of data\nlosses on training stability and solution accuracy. Our experiments demonstrate\nthat incorporating data losses significantly improves convergence robustness,\nreducing training instabilities and sensitivity to initialization. These\nfindings suggest that hybrid data-physics approaches can effectively enhance\nthe reliability of learning-based geodesic distance solvers with sparse data.\n","authors":["Rafael I. Cabral Muchacho","Florian T. Pokorny"],"pdf_url":"https://arxiv.org/pdf/2503.04579v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01804v2","updated":"2025-03-06T16:07:43Z","published":"2025-03-03T18:33:46Z","title":"$\\texttt{SEM-CTRL}$: Semantically Controlled Decoding","summary":"  Ensuring both syntactic and semantic correctness in Large Language Model\n(LLM) outputs remains a significant challenge, despite being critical for\nreal-world deployment. In this paper, we introduce $\\texttt{SEM-CTRL}$, a\nunified approach that enforces rich context-sensitive constraints and task- and\ninstance-specific semantics directly on an LLM decoder. Our approach integrates\ntoken-level MCTS, which is guided by specific syntactic and semantic\nconstraints. The constraints over the desired outputs are expressed using\nAnswer Set Grammars -- a logic-based formalism that generalizes\ncontext-sensitive grammars while incorporating background knowledge to\nrepresent task-specific semantics. We show that our approach guarantees correct\ncompletions for any off-the-shelf LLM without the need for fine-tuning. We\nevaluate $\\texttt{SEM-CTRL}$ on a range of tasks, including synthetic grammar\nsynthesis, combinatorial reasoning, and planning. Our results demonstrate that\n$\\texttt{SEM-CTRL}$ allows small pre-trained LLMs to efficiently outperform\nlarger variants and state-of-the-art reasoning models (e.g., o1-preview) while\nsimultaneously guaranteeing solution correctness.\n","authors":["Mohammad Albinhassan","Pranava Madhyastha","Alessandra Russo"],"pdf_url":"https://arxiv.org/pdf/2503.01804v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04570v1","updated":"2025-03-06T16:05:29Z","published":"2025-03-06T16:05:29Z","title":"Meta Learning not to Learn: Robustly Informing Meta-Learning under\n  Nuisance-Varying Families","summary":"  In settings where both spurious and causal predictors are available, standard\nneural networks trained under the objective of empirical risk minimization\n(ERM) with no additional inductive biases tend to have a dependence on a\nspurious feature. As a result, it is necessary to integrate additional\ninductive biases in order to guide the network toward generalizable hypotheses.\nOften these spurious features are shared across related tasks, such as\nestimating disease prognoses from image scans coming from different hospitals,\nmaking the challenge of generalization more difficult. In these settings, it is\nimportant that methods are able to integrate the proper inductive biases to\ngeneralize across both nuisance-varying families as well as task families.\nMotivated by this setting, we present RIME (Robustly Informed Meta lEarning), a\nnew method for meta learning under the presence of both positive and negative\ninductive biases (what to learn and what not to learn). We first develop a\ntheoretical causal framework showing why existing approaches at knowledge\nintegration can lead to worse performance on distributionally robust\nobjectives. We then show that RIME is able to simultaneously integrate both\nbiases, reaching state of the art performance under distributionally robust\nobjectives in informed meta-learning settings under nuisance-varying families.\n","authors":["Louis McConnell"],"pdf_url":"https://arxiv.org/pdf/2503.04570v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.18049v2","updated":"2025-03-06T16:03:59Z","published":"2025-02-25T10:15:16Z","title":"Golden Ratio Weighting Prevents Model Collapse","summary":"  Recent studies identified an intriguing phenomenon in recursive generative\nmodel training known as model collapse, where models trained on data generated\nby previous models exhibit severe performance degradation. Addressing this\nissue and developing more effective training strategies have become central\nchallenges in generative model research. In this paper, we investigate this\nphenomenon theoretically within a novel framework, where generative models are\niteratively trained on a combination of newly collected real data and synthetic\ndata from the previous training step. To develop an optimal training strategy\nfor integrating real and synthetic data, we evaluate the performance of a\nweighted training scheme in various scenarios, including Gaussian distribution\nestimation and linear regression. We theoretically characterize the impact of\nthe mixing proportion and weighting scheme of synthetic data on the final\nmodel's performance. Our key finding is that, across different settings, the\noptimal weighting scheme under different proportions of synthetic data\nasymptotically follows a unified expression, revealing a fundamental trade-off\nbetween leveraging synthetic data and generative model performance. Notably, in\nsome cases, the optimal weight assigned to real data corresponds to the\nreciprocal of the golden ratio. Finally, we validate our theoretical results on\nextensive simulated datasets and a real tabular dataset.\n","authors":["Hengzhi He","Shirong Xu","Guang Cheng"],"pdf_url":"https://arxiv.org/pdf/2502.18049v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.00153v2","updated":"2025-03-06T15:50:28Z","published":"2024-09-30T18:52:53Z","title":"Beyond Single Concept Vector: Modeling Concept Subspace in LLMs with\n  Gaussian Distribution","summary":"  Probing learned concepts in large language models (LLMs) is crucial for\nunderstanding how semantic knowledge is encoded internally. Training linear\nclassifiers on probing tasks is a principle approach to denote the vector of a\ncertain concept in the representation space. However, the single vector\nidentified for a concept varies with both data and training, making it less\nrobust and weakening its effectiveness in real-world applications. To address\nthis challenge, we propose an approach to approximate the subspace representing\na specific concept. Built on linear probing classifiers, we extend the concept\nvectors into Gaussian Concept Subspace (GCS). We demonstrate GCS's\neffectiveness through measuring its faithfulness and plausibility across\nmultiple LLMs with different sizes and architectures. Additionally, we use\nrepresentation intervention tasks to showcase its efficacy in real-world\napplications such as emotion steering. Experimental results indicate that GCS\nconcept vectors have the potential to balance steering performance and\nmaintaining the fluency in natural language generation tasks.\n","authors":["Haiyan Zhao","Heng Zhao","Bo Shen","Ali Payani","Fan Yang","Mengnan Du"],"pdf_url":"https://arxiv.org/pdf/2410.00153v2.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2501.02436v2","updated":"2025-03-06T15:49:50Z","published":"2025-01-05T04:23:21Z","title":"An Analysis Framework for Understanding Deep Neural Networks Based on\n  Network Dynamics","summary":"  Advancing artificial intelligence demands a deeper understanding of the\nmechanisms underlying deep learning. Here, we propose a straightforward\nanalysis framework based on the dynamics of learning models. Neurons are\ncategorized into two modes based on whether their transformation functions\npreserve order. This categorization reveals how deep neural networks (DNNs)\nmaximize information extraction by rationally allocating the proportion of\nneurons in different modes across deep layers. We further introduce the\nattraction basins of the training samples in both the sample vector space and\nthe weight vector space to characterize the generalization ability of DNNs.\nThis framework allows us to identify optimal depth and width configurations,\nproviding a unified explanation for fundamental DNN behaviors such as the \"flat\nminima effect,\" \"grokking,\" and double descent phenomena. Our analysis extends\nto networks with depths up to 100 layers.\n","authors":["Yuchen Lin","Yong Zhang","Sihan Feng","Hong Zhao"],"pdf_url":"https://arxiv.org/pdf/2501.02436v2.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2503.04556v1","updated":"2025-03-06T15:47:19Z","published":"2025-03-06T15:47:19Z","title":"Compositional Causal Reasoning Evaluation in Language Models","summary":"  Causal reasoning and compositional reasoning are two core aspirations in\ngenerative AI. Measuring the extent of these behaviors requires principled\nevaluation methods. We explore a unified perspective that considers both\nbehaviors simultaneously, termed compositional causal reasoning (CCR): the\nability to infer how causal measures compose and, equivalently, how causal\nquantities propagate through graphs. We instantiate a framework for the\nsystematic evaluation of CCR for the average treatment effect and the\nprobability of necessity and sufficiency. As proof of concept, we demonstrate\nthe design of CCR tasks for language models in the LLama, Phi, and GPT\nfamilies. On a math word problem, our framework revealed a range of\ntaxonomically distinct error patterns. Additionally, CCR errors increased with\nthe complexity of causal paths for all models except o1.\n","authors":["Jacqueline R. M. A. Maasch","Alihan Hüyük","Xinnuo Xu","Aditya V. Nori","Javier Gonzalez"],"pdf_url":"https://arxiv.org/pdf/2503.04556v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.18691v2","updated":"2025-03-06T15:47:01Z","published":"2024-07-26T12:16:53Z","title":"Graph Neural Networks for Virtual Sensing in Complex Systems: Addressing\n  Heterogeneous Temporal Dynamics","summary":"  Real-time condition monitoring is crucial for the reliable and efficient\noperation of complex systems. However, relying solely on physical sensors can\nbe limited due to their cost, placement constraints, or inability to directly\nmeasure certain critical parameters. Virtual sensing addresses these\nlimitations by leveraging readily available sensor data and system knowledge to\nestimate inaccessible parameters or infer system states. The increasing\ncomplexity of industrial systems necessitates deployments of sensors with\ndiverse modalities to provide a comprehensive understanding of system states.\nThese sensors capture data at varying frequencies to monitor both rapid and\nslowly varying system dynamics, as well as local and global state evolutions of\nthe systems. This leads to heterogeneous temporal dynamics, which, particularly\nunder varying operational end environmental conditions, pose a significant\nchallenge for accurate virtual sensing. To address this, we propose a\nHeterogeneous Temporal Graph Neural Network (HTGNN) framework. HTGNN explicitly\nmodels signals from diverse sensors and integrates operating conditions into\nthe model architecture. We evaluate HTGNN using two newly released datasets: a\nbearing dataset with diverse load conditions for bearing load prediction and a\nyear-long simulated dataset for predicting bridge live loads. Our results\ndemonstrate that HTGNN significantly outperforms established baseline methods\nin both tasks, particularly under highly varying operating conditions. These\nresults highlight HTGNN's potential as a robust and accurate virtual sensing\napproach for complex systems, paving the way for improved monitoring,\npredictive maintenance, and enhanced system performance. Our code and data are\navailable under https://github.com/EPFL-IMOS/htgnn.\n","authors":["Mengjie Zhao","Cees Taal","Stephan Baggerohr","Olga Fink"],"pdf_url":"https://arxiv.org/pdf/2407.18691v2.pdf","comment":"This paper extends our previous conference paper (Best Paper at\n  European Conference of the PHM Society 2024,\n  https://doi.org/10.36001/phme.2024.v8i1.3998). Accepted by Mechanical Systems\n  and Signal Processing (MSSP)"},{"id":"http://arxiv.org/abs/2502.18394v4","updated":"2025-03-06T15:39:55Z","published":"2025-02-25T17:43:43Z","title":"The FFT Strikes Back: An Efficient Alternative to Self-Attention","summary":"  Conventional self-attention mechanisms incur quadratic complexity, limiting\ntheir scalability on long sequences. We introduce \\textbf{FFTNet}, an adaptive\nspectral filtering framework that leverages the Fast Fourier Transform (FFT) to\nachieve global token mixing in $\\mathcal{O}(n\\log n)$ time. By transforming\ninputs into the frequency domain, FFTNet exploits the orthogonality and energy\npreservation guaranteed by Parseval's theorem to capture long-range\ndependencies efficiently. Our main theoretical contributions are 1) an adaptive\nspectral filter, 2) combining local windowing with a global FFT branch, and 3)\nrich nonlinearity introduction in both the frequency and token domains.\nExperiments on the Long Range Arena and ImageNet benchmarks validate our\ntheoretical insights and demonstrate superior performance over fixed Fourier\nand standard attention models.\n","authors":["Jacob Fein-Ashley"],"pdf_url":"https://arxiv.org/pdf/2502.18394v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09990v2","updated":"2025-03-06T15:38:31Z","published":"2025-02-14T08:22:51Z","title":"X-Boundary: Establishing Exact Safety Boundary to Shield LLMs from\n  Multi-Turn Jailbreaks without Compromising Usability","summary":"  Despite the rapid development of safety alignment techniques for LLMs,\ndefending against multi-turn jailbreaks is still a challenging task. In this\npaper, we conduct a comprehensive comparison, revealing that some existing\ndefense methods can improve the robustness of LLMs against multi-turn\njailbreaks but compromise usability, i.e., reducing general capabilities or\ncausing the over-refusal problem. From the perspective of mechanism\ninterpretability of LLMs, we discover that these methods fail to establish a\nboundary that exactly distinguishes safe and harmful feature representations.\nTherefore, boundary-safe representations close to harmful representations are\ninevitably disrupted, leading to a decline in usability. To address this issue,\nwe propose X-Boundary to push harmful representations away from boundary-safe\nrepresentations and obtain an exact distinction boundary. In this way, harmful\nrepresentations can be precisely erased without disrupting safe ones.\nExperimental results show that X-Boundary achieves state-of-the-art defense\nperformance against multi-turn jailbreaks, while reducing the over-refusal rate\nby about 20% and maintaining nearly complete general capability. Furthermore,\nwe theoretically prove and empirically verify that X-Boundary can accelerate\nthe convergence process during training. Please see our code at:\nhttps://github.com/AI45Lab/X-Boundary.\n","authors":["Xiaoya Lu","Dongrui Liu","Yi Yu","Luxin Xu","Jing Shao"],"pdf_url":"https://arxiv.org/pdf/2502.09990v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03660v2","updated":"2025-03-06T15:32:00Z","published":"2025-03-05T16:47:36Z","title":"Chunking the Critic: A Transformer-based Soft Actor-Critic with N-Step\n  Returns","summary":"  Soft Actor-Critic (SAC) critically depends on its critic network, which\ntypically evaluates a single state-action pair to guide policy updates. Using\nN-step returns is a common practice to reduce the bias in the target values of\nthe critic. However, using N-step returns can again introduce high variance and\nnecessitates importance sampling, often destabilizing training. Recent\nalgorithms have also explored action chunking-such as direct action repetition\nand movement primitives-to enhance exploration. In this paper, we propose a\nTransformer-based Critic Network for SAC that integrates the N-returns\nframework in a stable and efficient manner. Unlike approaches that perform\nchunking in the actor network, we feed chunked actions into the critic network\nto explore potential performance gains. Our architecture leverages the\nTransformer's ability to process sequential information, facilitating more\nrobust value estimation. Empirical results show that this method not only\nachieves efficient, stable training but also excels in sparse\nreward/multi-phase environments-traditionally a challenge for step-based\nmethods. These findings underscore the promise of combining Transformer-based\ncritics with N-returns to advance reinforcement learning performance\n","authors":["Dong Tian","Ge Li","Hongyi Zhou","Onur Celik","Gerhard Neumann"],"pdf_url":"https://arxiv.org/pdf/2503.03660v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2312.07516v3","updated":"2025-03-06T15:30:10Z","published":"2023-12-12T18:47:12Z","title":"Learning finitely correlated states: stability of the spectral\n  reconstruction","summary":"  Matrix product operators allow efficient descriptions (or realizations) of\nstates on a 1D lattice. We consider the task of learning a realization of\nminimal dimension from copies of an unknown state, such that the resulting\noperator is close to the density matrix in trace norm. For finitely correlated\ntranslation-invariant states on an infinite chain, a realization of minimal\ndimension can be exactly reconstructed via linear algebra operations from the\nmarginals of a size depending on the representation dimension. We establish a\nbound on the trace norm error for an algorithm that estimates a candidate\nrealization from estimates of these marginals and outputs a matrix product\noperator, estimating the state of a chain of arbitrary length $t$. This bound\nallows us to establish an $O(t^2)$ upper bound on the sample complexity of the\nlearning task, with an explicit dependence on the site dimension, realization\ndimension and spectral properties of a certain map constructed from the state.\nA refined error bound can be proven for $C^*$-finitely correlated states, which\nhave an operational interpretation in terms of sequential quantum channels\napplied to the memory system. We can also obtain an analogous error bound for a\nclass of matrix product density operators on a finite chain reconstructible by\nlocal marginals. In this case, a linear number of marginals must be estimated,\nobtaining a sample complexity of $\\tilde{O}(t^3)$. The learning algorithm also\nworks for states that are sufficiently close to a finitely correlated state,\nwith the potential of providing competitive algorithms for other interesting\nfamilies of states.\n","authors":["Marco Fanizza","Niklas Galke","Josep Lumbreras","Cambyse Rouzé","Andreas Winter"],"pdf_url":"https://arxiv.org/pdf/2312.07516v3.pdf","comment":"42 pages, 7 figures. Manuscript restructured, with minor corrections\n  and clarifications"},{"id":"http://arxiv.org/abs/2403.00025v2","updated":"2025-03-06T15:29:41Z","published":"2024-02-28T15:19:33Z","title":"On the Challenges and Opportunities in Generative AI","summary":"  The field of deep generative modeling has grown rapidly in the last few\nyears. With the availability of massive amounts of training data coupled with\nadvances in scalable unsupervised learning paradigms, recent large-scale\ngenerative models show tremendous promise in synthesizing high-resolution\nimages and text, as well as structured data such as videos and molecules.\nHowever, we argue that current large-scale generative AI models exhibit several\nfundamental shortcomings that hinder their widespread adoption across domains.\nIn this work, our objective is to identify these issues and highlight key\nunresolved challenges in modern generative AI paradigms that should be\naddressed to further enhance their capabilities, versatility, and reliability.\nBy identifying these challenges, we aim to provide researchers with insights\nfor exploring fruitful research directions, thus fostering the development of\nmore robust and accessible generative AI solutions.\n","authors":["Laura Manduchi","Kushagra Pandey","Clara Meister","Robert Bamler","Ryan Cotterell","Sina Däubener","Sophie Fellenz","Asja Fischer","Thomas Gärtner","Matthias Kirchler","Marius Kloft","Yingzhen Li","Christoph Lippert","Gerard de Melo","Eric Nalisnick","Björn Ommer","Rajesh Ranganath","Maja Rudolph","Karen Ullrich","Guy Van den Broeck","Julia E Vogt","Yixin Wang","Florian Wenzel","Frank Wood","Stephan Mandt","Vincent Fortuin"],"pdf_url":"https://arxiv.org/pdf/2403.00025v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.07180v5","updated":"2025-03-06T15:26:56Z","published":"2024-11-11T17:57:30Z","title":"Gumbel Counterfactual Generation From Language Models","summary":"  Understanding and manipulating the causal generation mechanisms in language\nmodels is essential for controlling their behavior. Previous work has primarily\nrelied on techniques such as representation surgery -- e.g., model ablations or\nmanipulation of linear subspaces tied to specific concepts -- to\n\\emph{intervene} on these models. To understand the impact of interventions\nprecisely, it is useful to examine \\emph{counterfactuals} -- e.g., how a given\nsentence would have appeared had it been generated by the model following a\nspecific intervention. We highlight that counterfactual reasoning is\nconceptually distinct from interventions, as articulated in Pearl's causal\nhierarchy. Based on this observation, we propose a framework for generating\ntrue string counterfactuals by reformulating language models as a structural\nequation model using the Gumbel-max trick, which we called Gumbel\ncounterfactual generation. This reformulation allows us to model the joint\ndistribution over original strings and their counterfactuals resulting from the\nsame instantiation of the sampling noise. We develop an algorithm based on\nhindsight Gumbel sampling that allows us to infer the latent noise variables\nand generate counterfactuals of observed strings. Our experiments demonstrate\nthat the approach produces meaningful counterfactuals while at the same time\nshowing that commonly used intervention techniques have considerable undesired\nside effects.\n","authors":["Shauli Ravfogel","Anej Svete","Vésteinn Snæbjarnarson","Ryan Cotterell"],"pdf_url":"https://arxiv.org/pdf/2411.07180v5.pdf","comment":"Accepted in ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04528v1","updated":"2025-03-06T15:16:57Z","published":"2025-03-06T15:16:57Z","title":"Federated Dynamic Modeling and Learning for Spatiotemporal Data\n  Forecasting","summary":"  This paper presents an advanced Federated Learning (FL) framework for\nforecasting complex spatiotemporal data, improving upon recent state-of-the-art\nmodels. In the proposed approach, the original Gated Recurrent Unit (GRU)\nmodule within previous Dynamic Spatial--Temporal Graph Convolutional Recurrent\nNetwork (DSTGCRN) modeling is first replaced with a Long Short-Term Memory\n(LSTM) network, enabling the resulting model to more effectively capture\nlong-term dependencies inherent to time series data. The resulting architecture\nsignificantly improves the model's capacity to handle complex temporal patterns\nin diverse forecasting applications. Furthermore, the proposed FL framework\nintegrates a novel Client-Side Validation (CSV) mechanism, introducing a\ncritical validation step at the client level before incorporating aggregated\nparameters from the central server into local models. This ensures that only\nthe most effective updates are adopted, improving both the robustness and\naccuracy of the forecasting model across clients. The efficiency of our\napproach is demonstrated through extensive experiments on real-world\napplications, including public datasets for multimodal transport demand\nforecasting and private datasets for Origin-Destination (OD) matrix forecasting\nin urban areas. The results demonstrate substantial improvements over\nconventional methods, highlighting the framework's ability to capture complex\nspatiotemporal dependencies while preserving data privacy. This work not only\nprovides a scalable and privacy-preserving solution for real-time,\nregion-specific forecasting and management but also underscores the potential\nof leveraging distributed data sources in a FL context. We provide our\nalgorithms as open-source on GitHub.\n","authors":["Thien Pham","Angelo Furno","Faïcel Chamroukhi","Latifa Oukhellou"],"pdf_url":"https://arxiv.org/pdf/2503.04528v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.07775v2","updated":"2025-03-06T15:15:58Z","published":"2024-12-10T18:59:58Z","title":"Efficient Diversity-Preserving Diffusion Alignment via Gradient-Informed\n  GFlowNets","summary":"  While one commonly trains large diffusion models by collecting datasets on\ntarget downstream tasks, it is often desired to align and finetune pretrained\ndiffusion models with some reward functions that are either designed by experts\nor learned from small-scale datasets. Existing post-training methods for reward\nfinetuning of diffusion models typically suffer from lack of diversity in\ngenerated samples, lack of prior preservation, and/or slow convergence in\nfinetuning. Inspired by recent successes in generative flow networks\n(GFlowNets), a class of probabilistic models that sample with the unnormalized\ndensity of a reward function, we propose a novel GFlowNet method dubbed\nNabla-GFlowNet (abbreviated as \\methodname), the first GFlowNet method that\nleverages the rich signal in reward gradients, together with an objective\ncalled \\graddb plus its variant \\resgraddb designed for prior-preserving\ndiffusion finetuning. We show that our proposed method achieves fast yet\ndiversity- and prior-preserving finetuning of Stable Diffusion, a large-scale\ntext-conditioned image diffusion model, on different realistic reward\nfunctions.\n","authors":["Zhen Liu","Tim Z. Xiao","Weiyang Liu","Yoshua Bengio","Dinghuai Zhang"],"pdf_url":"https://arxiv.org/pdf/2412.07775v2.pdf","comment":"Technical Report (35 pages, 31 figures), Accepted at ICLR 2025"},{"id":"http://arxiv.org/abs/2411.12580v2","updated":"2025-03-06T15:14:17Z","published":"2024-11-19T15:47:12Z","title":"Procedural Knowledge in Pretraining Drives Reasoning in Large Language\n  Models","summary":"  The capabilities and limitations of Large Language Models have been sketched\nout in great detail in recent years, providing an intriguing yet conflicting\npicture. On the one hand, LLMs demonstrate a general ability to solve problems.\nOn the other hand, they show surprising reasoning gaps when compared to humans,\ncasting doubt on the robustness of their generalisation strategies. The sheer\nvolume of data used in the design of LLMs has precluded us from applying the\nmethod traditionally used to measure generalisation: train-test set separation.\nTo overcome this, we study what kind of generalisation strategies LLMs employ\nwhen performing reasoning tasks by investigating the pretraining data they rely\non. For two models of different sizes (7B and 35B) and 2.5B of their\npretraining tokens, we identify what documents influence the model outputs for\nthree simple mathematical reasoning tasks and contrast this to the data that\nare influential for answering factual questions. We find that, while the models\nrely on mostly distinct sets of data for each factual question, a document\noften has a similar influence across different reasoning questions within the\nsame task, indicating the presence of procedural knowledge. We further find\nthat the answers to factual questions often show up in the most influential\ndata. However, for reasoning questions the answers usually do not show up as\nhighly influential, nor do the answers to the intermediate reasoning steps.\nWhen we characterise the top ranked documents for the reasoning questions\nqualitatively, we confirm that the influential documents often contain\nprocedural knowledge, like demonstrating how to obtain a solution using\nformulae or code. Our findings indicate that the approach to reasoning the\nmodels use is unlike retrieval, and more like a generalisable strategy that\nsynthesises procedural knowledge from documents doing a similar form of\nreasoning.\n","authors":["Laura Ruis","Maximilian Mozes","Juhan Bae","Siddhartha Rao Kamalakara","Dwarak Talupuru","Acyr Locatelli","Robert Kirk","Tim Rocktäschel","Edward Grefenstette","Max Bartolo"],"pdf_url":"https://arxiv.org/pdf/2411.12580v2.pdf","comment":"Published at ICLR 2025"},{"id":"http://arxiv.org/abs/2410.04166v2","updated":"2025-03-06T15:11:57Z","published":"2024-10-05T14:04:03Z","title":"Learning from negative feedback, or positive feedback or both","summary":"  Existing preference optimization methods often assume scenarios where paired\npreference feedback (preferred/positive vs. dis-preferred/negative examples) is\navailable. This requirement limits their applicability in scenarios where only\nunpaired feedback--for example, either positive or negative--is available. To\naddress this, we introduce a novel approach that decouples learning from\npositive and negative feedback. This decoupling enables control over the\ninfluence of each feedback type and, importantly, allows learning even when\nonly one feedback type is present. A key contribution is demonstrating stable\nlearning from negative feedback alone, a capability not well-addressed by\ncurrent methods. Our approach builds upon the probabilistic framework\nintroduced in (Dayan and Hinton, 1997), which uses expectation-maximization\n(EM) to directly optimize the probability of positive outcomes (as opposed to\nclassic expected reward maximization). We address a key limitation in current\nEM-based methods: they solely maximize the likelihood of positive examples,\nwhile neglecting negative ones. We show how to extend EM algorithms to\nexplicitly incorporate negative examples, leading to a theoretically grounded\nalgorithm that offers an intuitive and versatile way to learn from both\npositive and negative feedback. We evaluate our approach for training language\nmodels based on human feedback as well as training policies for sequential\ndecision-making problems, where learned value functions are available.\n","authors":["Abbas Abdolmaleki","Bilal Piot","Bobak Shahriari","Jost Tobias Springenberg","Tim Hertweck","Rishabh Joshi","Junhyuk Oh","Michael Bloesch","Thomas Lampe","Nicolas Heess","Jonas Buchli","Martin Riedmiller"],"pdf_url":"https://arxiv.org/pdf/2410.04166v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04518v1","updated":"2025-03-06T15:06:01Z","published":"2025-03-06T15:06:01Z","title":"Leveraging priors on distribution functions for multi-arm bandits","summary":"  We introduce Dirichlet Process Posterior Sampling (DPPS), a Bayesian\nnon-parametric algorithm for multi-arm bandits based on Dirichlet Process (DP)\npriors. Like Thompson-sampling, DPPS is a probability-matching algorithm, i.e.,\nit plays an arm based on its posterior-probability of being optimal. Instead of\nassuming a parametric class for the reward generating distribution of each arm,\nand then putting a prior on the parameters, in DPPS the reward generating\ndistribution is directly modeled using DP priors. DPPS provides a principled\napproach to incorporate prior belief about the bandit environment, and in the\nnoninformative limit of the DP posteriors (i.e. Bayesian Bootstrap), we recover\nNon Parametric Thompson Sampling (NPTS), a popular non-parametric bandit\nalgorithm, as a special case of DPPS. We employ stick-breaking representation\nof the DP priors, and show excellent empirical performance of DPPS in\nchallenging synthetic and real world bandit environments. Finally, using an\ninformation-theoretic analysis, we show non-asymptotic optimality of DPPS in\nthe Bayesian regret setup.\n","authors":["Sumit Vashishtha","Odalric-Ambrym Maillard"],"pdf_url":"https://arxiv.org/pdf/2503.04518v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.05874v2","updated":"2025-03-06T15:02:33Z","published":"2025-02-09T12:23:40Z","title":"MMGDreamer: Mixed-Modality Graph for Geometry-Controllable 3D Indoor\n  Scene Generation","summary":"  Controllable 3D scene generation has extensive applications in virtual\nreality and interior design, where the generated scenes should exhibit high\nlevels of realism and controllability in terms of geometry. Scene graphs\nprovide a suitable data representation that facilitates these applications.\nHowever, current graph-based methods for scene generation are constrained to\ntext-based inputs and exhibit insufficient adaptability to flexible user\ninputs, hindering the ability to precisely control object geometry. To address\nthis issue, we propose MMGDreamer, a dual-branch diffusion model for scene\ngeneration that incorporates a novel Mixed-Modality Graph, visual enhancement\nmodule, and relation predictor. The mixed-modality graph allows object nodes to\nintegrate textual and visual modalities, with optional relationships between\nnodes. It enhances adaptability to flexible user inputs and enables meticulous\ncontrol over the geometry of objects in the generated scenes. The visual\nenhancement module enriches the visual fidelity of text-only nodes by\nconstructing visual representations using text embeddings. Furthermore, our\nrelation predictor leverages node representations to infer absent relationships\nbetween nodes, resulting in more coherent scene layouts. Extensive experimental\nresults demonstrate that MMGDreamer exhibits superior control of object\ngeometry, achieving state-of-the-art scene generation performance. Project\npage: https://yangzhifeio.github.io/project/MMGDreamer.\n","authors":["Zhifei Yang","Keyang Lu","Chao Zhang","Jiaxing Qi","Hanqi Jiang","Ruifei Ma","Shenglin Yin","Yifan Xu","Mingzhe Xing","Zhen Xiao","Jieyi Long","Xiangde Liu","Guangyao Zhai"],"pdf_url":"https://arxiv.org/pdf/2502.05874v2.pdf","comment":"Accepted by AAAI 2025 Main Track"},{"id":"http://arxiv.org/abs/2503.04509v1","updated":"2025-03-06T14:55:25Z","published":"2025-03-06T14:55:25Z","title":"STX-Search: Explanation Search for Continuous Dynamic Spatio-Temporal\n  Models","summary":"  Recent improvements in the expressive power of spatio-temporal models have\nled to performance gains in many real-world applications, such as traffic\nforecasting and social network modelling. However, understanding the\npredictions from a model is crucial to ensure reliability and trustworthiness,\nparticularly for high-risk applications, such as healthcare and transport. Few\nexisting methods are able to generate explanations for models trained on\ncontinuous-time dynamic graph data and, of these, the computational complexity\nand lack of suitable explanation objectives pose challenges. In this paper, we\npropose $\\textbf{S}$patio-$\\textbf{T}$emporal E$\\textbf{X}$planation\n$\\textbf{Search}$ (STX-Search), a novel method for generating instance-level\nexplanations that is applicable to static and dynamic temporal graph\nstructures. We introduce a novel search strategy and objective function, to\nfind explanations that are highly faithful and interpretable. When compared\nwith existing methods, STX-Search produces explanations of higher fidelity\nwhilst optimising explanation size to maintain interpretability.\n","authors":["Saif Anwar","Nathan Griffiths","Thomas Popham","Abhir Bhalerao"],"pdf_url":"https://arxiv.org/pdf/2503.04509v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04507v1","updated":"2025-03-06T14:54:28Z","published":"2025-03-06T14:54:28Z","title":"A Morse Transform for Drug Discovery","summary":"  We introduce a new ligand-based virtual screening (LBVS) framework that uses\npiecewise linear (PL) Morse theory to predict ligand binding potential. We\nmodel ligands as simplicial complexes via a pruned Delaunay triangulation, and\ncatalogue the critical points across multiple directional height functions.\nThis produces a rich feature vector, consisting of crucial topological features\n-- peaks, troughs, and saddles -- that characterise ligand surfaces relevant to\nbinding interactions. Unlike contemporary LBVS methods that rely on\ncomputationally-intensive deep neural networks, we require only a lightweight\nclassifier. The Morse theoretic approach achieves state-of-the-art performance\non standard datasets while offering an interpretable feature vector and\nscalable method for ligand prioritization in early-stage drug discovery.\n","authors":["Alexander M. Tanaka","Aras T. Asaad","Richard Cooper","Vidit Nanda"],"pdf_url":"https://arxiv.org/pdf/2503.04507v1.pdf","comment":"25 pages, 5 main figures, 2 main tables, 6 supplementary figures and\n  4 supplementary tables"},{"id":"http://arxiv.org/abs/2501.00020v2","updated":"2025-03-06T14:52:11Z","published":"2024-12-16T11:35:40Z","title":"Magnetic Field Data Calibration with Transformer Model Using Physical\n  Constraints: A Scalable Method for Satellite Missions, Illustrated by\n  Tianwen-1","summary":"  This study introduces a novel approach that integrates the magnetic field\ndata correction from the Tianwen-1 Mars mission with a neural network\narchitecture constrained by physical principles derived from Maxwell's equation\nequations. By employing a Transformer based model capable of efficiently\nhandling sequential data, the method corrects measurement anomalies caused by\nsatellite dynamics, instrument interference, and environmental noise. As a\nresult, it significantly improves both the accuracy and the physical\nconsistency of the calibrated data. Compared to traditional methods that\nrequire long data segments and manual intervention often taking weeks or even\nmonths to complete this new approach can finish calibration in just minutes to\nhours, and predictions are made within seconds. This innovation not only\naccelerates the process of space weather modeling and planetary magnetospheric\nstudies but also provides a robust framework for future planetary exploration\nand solar wind interaction research.\n","authors":["Beibei Li","Yutian Chi","Yuming Wang"],"pdf_url":"https://arxiv.org/pdf/2501.00020v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04496v1","updated":"2025-03-06T14:44:25Z","published":"2025-03-06T14:44:25Z","title":"Learning Object Placement Programs for Indoor Scene Synthesis with\n  Iterative Self Training","summary":"  Data driven and autoregressive indoor scene synthesis systems generate indoor\nscenes automatically by suggesting and then placing objects one at a time.\nEmpirical observations show that current systems tend to produce incomplete\nnext object location distributions. We introduce a system which addresses this\nproblem. We design a Domain Specific Language (DSL) that specifies functional\nconstraints. Programs from our language take as input a partial scene and\nobject to place. Upon execution they predict possible object placements. We\ndesign a generative model which writes these programs automatically. Available\n3D scene datasets do not contain programs to train on, so we build upon\nprevious work in unsupervised program induction to introduce a new program\nbootstrapping algorithm. In order to quantify our empirical observations we\nintroduce a new evaluation procedure which captures how well a system models\nper-object location distributions. We ask human annotators to label all the\npossible places an object can go in a scene and show that our system produces\nper-object location distributions more consistent with human annotators. Our\nsystem also generates indoor scenes of comparable quality to previous systems\nand while previous systems degrade in performance when training data is sparse,\nour system does not degrade to the same degree.\n","authors":["Adrian Chang","Kai Wang","Yuanbo Li","Manolis Savva","Angel X. Chang","Daniel Ritchie"],"pdf_url":"https://arxiv.org/pdf/2503.04496v1.pdf","comment":"21 pages, 20 figures Subjects: Graphics (cs.GR), Computer Vision and\n  Pattern Recognition (cs.CV), Machine Learning (cs.LG)"},{"id":"http://arxiv.org/abs/2503.04492v1","updated":"2025-03-06T14:40:21Z","published":"2025-03-06T14:40:21Z","title":"Accurate predictive model of band gap with selected important features\n  based on explainable machine learning","summary":"  In the rapidly advancing field of materials informatics, nonlinear machine\nlearning models have demonstrated exceptional predictive capabilities for\nmaterial properties. However, their black-box nature limits interpretability,\nand they may incorporate features that do not contribute to, or even\ndeteriorate, model performance. This study employs explainable ML (XML)\ntechniques, including permutation feature importance and the SHapley Additive\nexPlanation, applied to a pristine support vector regression model designed to\npredict band gaps at the GW level using 18 input features. Guided by\nXML-derived individual feature importance, a simple framework is proposed to\nconstruct reduced-feature predictive models. Model evaluations indicate that an\nXML-guided compact model, consisting of the top five features, achieves\ncomparable accuracy to the pristine model on in-domain datasets while\ndemonstrating superior generalization with lower prediction errors on\nout-of-domain data. Additionally, the study underscores the necessity for\neliminating strongly correlated features to prevent misinterpretation and\noverestimation of feature importance before applying XML. This study highlights\nXML's effectiveness in developing simplified yet highly accurate machine\nlearning models by clarifying feature roles.\n","authors":["Joohwi Lee","Kaito Miyamoto"],"pdf_url":"https://arxiv.org/pdf/2503.04492v1.pdf","comment":"9 pages, 4 figures, SI is included"},{"id":"http://arxiv.org/abs/2503.04483v1","updated":"2025-03-06T14:32:00Z","published":"2025-03-06T14:32:00Z","title":"InfoSEM: A Deep Generative Model with Informative Priors for Gene\n  Regulatory Network Inference","summary":"  Inferring Gene Regulatory Networks (GRNs) from gene expression data is\ncrucial for understanding biological processes. While supervised models are\nreported to achieve high performance for this task, they rely on costly ground\ntruth (GT) labels and risk learning gene-specific biases, such as class\nimbalances of GT interactions, rather than true regulatory mechanisms. To\naddress these issues, we introduce InfoSEM, an unsupervised generative model\nthat leverages textual gene embeddings as informative priors, improving GRN\ninference without GT labels. InfoSEM can also integrate GT labels as an\nadditional prior when available, avoiding biases and further enhancing\nperformance. Additionally, we propose a biologically motivated benchmarking\nframework that better reflects real-world applications such as biomarker\ndiscovery and reveals learned biases of existing supervised methods. InfoSEM\noutperforms existing models by 38.5% across four datasets using textual\nembeddings prior and further boosts performance by 11.1% when integrating\nlabeled data as priors.\n","authors":["Tianyu Cui","Song-Jun Xu","Artem Moskalev","Shuwei Li","Tommaso Mansi","Mangal Prakash","Rui Liao"],"pdf_url":"https://arxiv.org/pdf/2503.04483v1.pdf","comment":"ICLR 2025 AI4NA Oral, ICLR 2025 MLGenX Spotlight, ICLR 2025 LMRL"},{"id":"http://arxiv.org/abs/2503.04482v1","updated":"2025-03-06T14:30:55Z","published":"2025-03-06T14:30:55Z","title":"Generalized Interpolating Discrete Diffusion","summary":"  While state-of-the-art language models achieve impressive results through\nnext-token prediction, they have inherent limitations such as the inability to\nrevise already generated tokens. This has prompted exploration of alternative\napproaches such as discrete diffusion. However, masked diffusion, which has\nemerged as a popular choice due to its simplicity and effectiveness,\nreintroduces this inability to revise words. To overcome this, we generalize\nmasked diffusion and derive the theoretical backbone of a family of general\ninterpolating discrete diffusion (GIDD) processes offering greater flexibility\nin the design of the noising processes. Leveraging a novel diffusion ELBO, we\nachieve compute-matched state-of-the-art performance in diffusion language\nmodeling. Exploiting GIDD's flexibility, we explore a hybrid approach combining\nmasking and uniform noise, leading to improved sample quality and unlocking the\nability for the model to correct its own mistakes, an area where autoregressive\nmodels notoriously have struggled. Our code and models are open-source:\nhttps://github.com/dvruette/gidd/\n","authors":["Dimitri von Rütte","Janis Fluri","Yuhui Ding","Antonio Orvieto","Bernhard Schölkopf","Thomas Hofmann"],"pdf_url":"https://arxiv.org/pdf/2503.04482v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04480v1","updated":"2025-03-06T14:30:15Z","published":"2025-03-06T14:30:15Z","title":"Poisoning Bayesian Inference via Data Deletion and Replication","summary":"  Research in adversarial machine learning (AML) has shown that statistical\nmodels are vulnerable to maliciously altered data. However, despite advances in\nBayesian machine learning models, most AML research remains concentrated on\nclassical techniques. Therefore, we focus on extending the white-box model\npoisoning paradigm to attack generic Bayesian inference, highlighting its\nvulnerability in adversarial contexts. A suite of attacks are developed that\nallow an attacker to steer the Bayesian posterior toward a target distribution\nthrough the strategic deletion and replication of true observations, even when\nonly sampling access to the posterior is available. Analytic properties of\nthese algorithms are proven and their performance is empirically examined in\nboth synthetic and real-world scenarios. With relatively little effort, the\nattacker is able to substantively alter the Bayesian's beliefs and, by\naccepting more risk, they can mold these beliefs to their will. By carefully\nconstructing the adversarial posterior, surgical poisoning is achieved such\nthat only targeted inferences are corrupted and others are minimally disturbed.\n","authors":["Matthieu Carreau","Roi Naveiro","William N. Caballero"],"pdf_url":"https://arxiv.org/pdf/2503.04480v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.13524v4","updated":"2025-03-06T14:27:12Z","published":"2025-02-19T08:21:59Z","title":"MobileViM: A Light-weight and Dimension-independent Vision Mamba for 3D\n  Medical Image Analysis","summary":"  Efficient evaluation of three-dimensional (3D) medical images is crucial for\ndiagnostic and therapeutic practices in healthcare. Recent years have seen a\nsubstantial uptake in applying deep learning and computer vision to analyse and\ninterpret medical images. Traditional approaches, such as convolutional neural\nnetworks (CNNs) and vision transformers (ViTs), face significant computational\nchallenges, prompting the need for architectural advancements. Recent efforts\nhave led to the introduction of novel architectures like the ``Mamba'' model as\nalternative solutions to traditional CNNs or ViTs. The Mamba model excels in\nthe linear processing of one-dimensional data with low computational demands.\nHowever, Mamba's potential for 3D medical image analysis remains underexplored\nand could face significant computational challenges as the dimension increases.\nThis manuscript presents MobileViM, a streamlined architecture for efficient\nsegmentation of 3D medical images. In the MobileViM network, we invent a new\ndimension-independent mechanism and a dual-direction traversing approach to\nincorporate with a vision-Mamba-based framework. MobileViM also features a\ncross-scale bridging technique to improve efficiency and accuracy across\nvarious medical imaging modalities. With these enhancements, MobileViM achieves\nsegmentation speeds exceeding 90 frames per second (FPS) on a single graphics\nprocessing unit (i.e., NVIDIA RTX 4090). This performance is over 24 FPS faster\nthan the state-of-the-art deep learning models for processing 3D images with\nthe same computational resources. In addition, experimental evaluations\ndemonstrate that MobileViM delivers superior performance, with Dice similarity\nscores reaching 92.72%, 86.69%, 80.46%, and 77.43% for PENGWIN, BraTS2024,\nATLAS, and Toothfairy2 datasets, respectively, which significantly surpasses\nexisting models.\n","authors":["Wei Dai","Jun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.13524v4.pdf","comment":"The corresponding author disagrees with the manuscript submitted to\n  arXiv"},{"id":"http://arxiv.org/abs/2503.03454v2","updated":"2025-03-06T14:25:03Z","published":"2025-03-05T12:40:34Z","title":"Data Poisoning Attacks to Locally Differentially Private Range Query\n  Protocols","summary":"  Local Differential Privacy (LDP) has been widely adopted to protect user\nprivacy in decentralized data collection. However, recent studies have revealed\nthat LDP protocols are vulnerable to data poisoning attacks, where malicious\nusers manipulate their reported data to distort aggregated results. In this\nwork, we present the first study on data poisoning attacks targeting LDP range\nquery protocols, focusing on both tree-based and grid-based approaches. We\nidentify three key challenges in executing such attacks, including crafting\nconsistent and effective fake data, maintaining data consistency across levels\nor grids, and preventing server detection. To address the first two challenges,\nwe propose novel attack methods that are provably optimal, including a\ntree-based attack and a grid-based attack, designed to manipulate range query\nresults with high effectiveness. \\textbf{Our key finding is that the common\npost-processing procedure, Norm-Sub, in LDP range query protocols can help the\nattacker massively amplify their attack effectiveness.} In addition, we study a\npotential countermeasure, but also propose an adaptive attack capable of\nevading this defense to address the third challenge. We evaluate our methods\nthrough theoretical analysis and extensive experiments on synthetic and\nreal-world datasets. Our results show that the proposed attacks can\nsignificantly amplify estimations for arbitrary range queries by manipulating a\nsmall fraction of users, providing 5-10x more influence than a normal user to\nthe estimation.\n","authors":["Ting-Wei Liao","Chih-Hsun Lin","Yu-Lin Tsai","Takao Murakami","Chia-Mu Yu","Jun Sakuma","Chun-Ying Huang","Hiroaki Kikuchi"],"pdf_url":"https://arxiv.org/pdf/2503.03454v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04474v1","updated":"2025-03-06T14:24:12Z","published":"2025-03-06T14:24:12Z","title":"Know Thy Judge: On the Robustness Meta-Evaluation of LLM Safety Judges","summary":"  Large Language Model (LLM) based judges form the underpinnings of key safety\nevaluation processes such as offline benchmarking, automated red-teaming, and\nonline guardrailing. This widespread requirement raises the crucial question:\ncan we trust the evaluations of these evaluators? In this paper, we highlight\ntwo critical challenges that are typically overlooked: (i) evaluations in the\nwild where factors like prompt sensitivity and distribution shifts can affect\nperformance and (ii) adversarial attacks that target the judge. We highlight\nthe importance of these through a study of commonly used safety judges, showing\nthat small changes such as the style of the model output can lead to jumps of\nup to 0.24 in the false negative rate on the same dataset, whereas adversarial\nattacks on the model generation can fool some judges into misclassifying 100%\nof harmful generations as safe ones. These findings reveal gaps in commonly\nused meta-evaluation benchmarks and weaknesses in the robustness of current LLM\njudges, indicating that low attack success under certain judges could create a\nfalse sense of security.\n","authors":["Francisco Eiras","Eliott Zemour","Eric Lin","Vaikkunth Mugunthan"],"pdf_url":"https://arxiv.org/pdf/2503.04474v1.pdf","comment":"Accepted to the ICBINB Workshop at ICLR'25"},{"id":"http://arxiv.org/abs/2503.04472v1","updated":"2025-03-06T14:23:06Z","published":"2025-03-06T14:23:06Z","title":"DAST: Difficulty-Adaptive Slow-Thinking for Large Reasoning Models","summary":"  Recent advancements in slow-thinking reasoning models have shown exceptional\nperformance in complex reasoning tasks. However, these models often exhibit\noverthinking-generating redundant reasoning steps for simple problems, leading\nto excessive computational resource usage. While current mitigation strategies\nuniformly reduce reasoning tokens, they risk degrading performance on\nchallenging tasks that require extended reasoning. This paper introduces\nDifficulty-Adaptive Slow-Thinking (DAST), a novel framework that enables models\nto autonomously adjust the length of Chain-of-Thought(CoT) based on problem\ndifficulty. We first propose a Token Length Budget (TLB) metric to quantify\ndifficulty, then leveraging length-aware reward shaping and length preference\noptimization to implement DAST. DAST penalizes overlong responses for simple\ntasks while incentivizing sufficient reasoning for complex problems.\nExperiments on diverse datasets and model scales demonstrate that DAST\neffectively mitigates overthinking (reducing token usage by over 30\\% on\naverage) while preserving reasoning accuracy on complex problems.\n","authors":["Yi Shen","Jian Zhang","Jieyun Huang","Shuming Shi","Wenjing Zhang","Jiangze Yan","Ning Wang","Kai Wang","Shiguo Lian"],"pdf_url":"https://arxiv.org/pdf/2503.04472v1.pdf","comment":"working in progress"},{"id":"http://arxiv.org/abs/2503.04469v1","updated":"2025-03-06T14:19:55Z","published":"2025-03-06T14:19:55Z","title":"An artificially intelligent magnetic resonance spectroscopy\n  quantification method: Comparison between QNet and LCModel on the cloud\n  computing platform CloudBrain-MRS","summary":"  Objctives: This work aimed to statistically compare the metabolite\nquantification of human brain magnetic resonance spectroscopy (MRS) between the\ndeep learning method QNet and the classical method LCModel through an\neasy-to-use intelligent cloud computing platform CloudBrain-MRS. Materials and\nMethods: In this retrospective study, two 3 T MRI scanners Philips Ingenia and\nAchieva collected 61 and 46 in vivo 1H magnetic resonance (MR) spectra of\nhealthy participants, respectively, from the brain region of pregenual anterior\ncingulate cortex from September to October 2021. The analyses of Bland-Altman,\nPearson correlation and reasonability were performed to assess the degree of\nagreement, linear correlation and reasonability between the two quantification\nmethods. Results: Fifteen healthy volunteers (12 females and 3 males, age\nrange: 21-35 years, mean age/standard deviation = 27.4/3.9 years) were\nrecruited. The analyses of Bland-Altman, Pearson correlation and reasonability\nshowed high to good consistency and very strong to moderate correlation between\nthe two methods for quantification of total N-acetylaspartate (tNAA), total\ncholine (tCho), and inositol (Ins) (relative half interval of limits of\nagreement = 3.04%, 9.3%, and 18.5%, respectively; Pearson correlation\ncoefficient r = 0.775, 0.927, and 0.469, respectively). In addition,\nquantification results of QNet are more likely to be closer to the previous\nreported average values than those of LCModel. Conclusion: There were high or\ngood degrees of consistency between the quantification results of QNet and\nLCModel for tNAA, tCho, and Ins, and QNet generally has more reasonable\nquantification than LCModel.\n","authors":["Meijin Lin","Lin Guo","Dicheng Chen","Jianshu Chen","Zhangren Tu","Xu Huang","Jianhua Wang","Ji Qi","Yuan Long","Zhiguo Huang","Di Guo","Xiaobo Qu","Haiwei Han"],"pdf_url":"https://arxiv.org/pdf/2503.04469v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04462v1","updated":"2025-03-06T14:13:59Z","published":"2025-03-06T14:13:59Z","title":"PALo: Learning Posture-Aware Locomotion for Quadruped Robots","summary":"  With the rapid development of embodied intelligence, locomotion control of\nquadruped robots on complex terrains has become a research hotspot. Unlike\ntraditional locomotion control approaches focusing solely on velocity tracking,\nwe pursue to balance the agility and robustness of quadruped robots on diverse\nand complex terrains. To this end, we propose an end-to-end deep reinforcement\nlearning framework for posture-aware locomotion named PALo, which manages to\nhandle simultaneous linear and angular velocity tracking and real-time\nadjustments of body height, pitch, and roll angles. In PALo, the locomotion\ncontrol problem is formulated as a partially observable Markov decision\nprocess, and an asymmetric actor-critic architecture is adopted to overcome the\nsim-to-real challenge. Further, by incorporating customized training curricula,\nPALo achieves agile posture-aware locomotion control in simulated environments\nand successfully transfers to real-world settings without fine-tuning, allowing\nreal-time control of the quadruped robot's locomotion and body posture across\nchallenging terrains. Through in-depth experimental analysis, we identify the\nkey components of PALo that contribute to its performance, further validating\nthe effectiveness of the proposed method. The results of this study provide new\npossibilities for the low-level locomotion control of quadruped robots in\nhigher dimensional command spaces and lay the foundation for future research on\nupper-level modules for embodied intelligence.\n","authors":["Xiangyu Miao","Jun Sun","Hang Lai","Xinpeng Di","Jiahang Cao","Yong Yu","Weinan Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04462v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01684v2","updated":"2025-03-06T14:07:34Z","published":"2025-03-03T15:58:15Z","title":"An Efficient Learning Method to Connect Observables","summary":"  Constructing fast and accurate surrogate models is a key ingredient for\nmaking robust predictions in many topics. We introduce a new model, the\nMultiparameter Eigenvalue Problem (MEP) emulator. The new method connects\nemulators and can make predictions directly from observables to observables. We\npresent that the MEP emulator can be trained with data from Eigenvector\nContinuation (EC) and Parametric Matrix Model (PMM) emulators. A simple\nsimulation on a one-dimensional lattice confirms the performance of the MEP\nemulator. Using $^{28}$O as an example, we also demonstrate that the predictive\nprobability distribution of the target observables can be easily obtained\nthrough the new emulator.\n","authors":["Hang Yu","Takayuki Miyagi"],"pdf_url":"https://arxiv.org/pdf/2503.01684v2.pdf","comment":"5+2 pages, 4 figures, updated acknowledgment"},{"id":"http://arxiv.org/abs/2503.04453v1","updated":"2025-03-06T14:06:50Z","published":"2025-03-06T14:06:50Z","title":"Reproducibility Assessment of Magnetic Resonance Spectroscopy of\n  Pregenual Anterior Cingulate Cortex across Sessions and Vendors via the Cloud\n  Computing Platform CloudBrain-MRS","summary":"  Given the need to elucidate the mechanisms underlying illnesses and their\ntreatment, as well as the lack of harmonization of acquisition and\npost-processing protocols among different magnetic resonance system vendors,\nthis work is to determine if metabolite concentrations obtained from different\nsessions, machine models and even different vendors of 3 T scanners can be\nhighly reproducible and be pooled for diagnostic analysis, which is very\nvaluable for the research of rare diseases. Participants underwent magnetic\nresonance imaging (MRI) scanning once on two separate days within one week (one\nsession per day, each session including two proton magnetic resonance\nspectroscopy (1H-MRS) scans with no more than a 5-minute interval between scans\n(no off-bed activity)) on each machine. were analyzed for reliability of\nwithin- and between- sessions using the coefficient of variation (CV) and\nintraclass correlation coefficient (ICC), and for reproducibility of across the\nmachines using correlation coefficient. As for within- and between- session,\nall CV values for a group of all the first or second scans of a session, or for\na session were almost below 20%, and most of the ICCs for metabolites range\nfrom moderate (0.4-0.59) to excellent (0.75-1), indicating high data\nreliability. When it comes to the reproducibility across the three scanners,\nall Pearson correlation coefficients across the three machines approached 1\nwith most around 0.9, and majority demonstrated statistical significance\n(P<0.01). Additionally, the intra-vendor reproducibility was greater than the\ninter-vendor ones.\n","authors":["Runhan Chen","Meijin Lin","Jianshu Chen","Liangjie Lin","Jiazheng Wang","Xiaoqing Li","Jianhua Wang","Xu Huang","Ling Qian","Shaoxing Liu","Yuan Long","Di Guo","Xiaobo Qu","Haiwei Han"],"pdf_url":"https://arxiv.org/pdf/2503.04453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04451v1","updated":"2025-03-06T14:06:20Z","published":"2025-03-06T14:06:20Z","title":"Privacy Preserving and Robust Aggregation for Cross-Silo Federated\n  Learning in Non-IID Settings","summary":"  Federated Averaging remains the most widely used aggregation strategy in\nfederated learning due to its simplicity and scalability. However, its\nperformance degrades significantly in non-IID data settings, where client\ndistributions are highly imbalanced or skewed. Additionally, it relies on\nclients transmitting metadata, specifically the number of training samples,\nwhich introduces privacy risks and may conflict with regulatory frameworks like\nthe European GDPR. In this paper, we propose a novel aggregation strategy that\naddresses these challenges by introducing class-aware gradient masking. Unlike\ntraditional approaches, our method relies solely on gradient updates,\neliminating the need for any additional client metadata, thereby enhancing\nprivacy protection. Furthermore, our approach validates and dynamically weights\nclient contributions based on class-specific importance, ensuring robustness\nagainst non-IID distributions, convergence prevention, and backdoor attacks.\nExtensive experiments on benchmark datasets demonstrate that our method not\nonly outperforms FedAvg and other widely accepted aggregation strategies in\nnon-IID settings but also preserves model integrity in adversarial scenarios.\nOur results establish the effectiveness of gradient masking as a practical and\nsecure solution for federated learning.\n","authors":["Marco Arazzi","Mert Cihangiroglu","Antonino Nocera"],"pdf_url":"https://arxiv.org/pdf/2503.04451v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04447v1","updated":"2025-03-06T14:02:28Z","published":"2025-03-06T14:02:28Z","title":"A Graph-Partitioning Based Continuous Optimization Approach to\n  Semi-supervised Clustering Problems","summary":"  Semi-supervised clustering is a basic problem in various applications. Most\nexisting methods require knowledge of the ideal cluster number, which is often\ndifficult to obtain in practice. Besides, satisfying the must-link constraints\nis another major challenge for these methods. In this work, we view the\nsemi-supervised clustering task as a partitioning problem on a graph associated\nwith the given dataset, where the similarity matrix includes a scaling\nparameter to reflect the must-link constraints. Utilizing a relaxation\ntechnique, we formulate the graph partitioning problem into a continuous\noptimization model that does not require the exact cluster number, but only an\noverestimate of it. We then propose a block coordinate descent algorithm to\nefficiently solve this model, and establish its convergence result. Based on\nthe obtained solution, we can construct the clusters that theoretically meet\nthe must-link constraints under mild assumptions. Furthermore, we verify the\neffectiveness and efficiency of our proposed method through comprehensive\nnumerical experiments.\n","authors":["Wei Liu","Xin Liu","Michael K. Ng","Zaikun Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.04447v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.08010v2","updated":"2025-03-06T14:01:48Z","published":"2024-02-12T19:18:50Z","title":"Which Frequencies do CNNs Need? Emergent Bottleneck Structure in Feature\n  Learning","summary":"  We describe the emergence of a Convolution Bottleneck (CBN) structure in\nCNNs, where the network uses its first few layers to transform the input\nrepresentation into a representation that is supported only along a few\nfrequencies and channels, before using the last few layers to map back to the\noutputs. We define the CBN rank, which describes the number and type of\nfrequencies that are kept inside the bottleneck, and partially prove that the\nparameter norm required to represent a function $f$ scales as depth times the\nCBN rank $f$. We also show that the parameter norm depends at next order on the\nregularity of $f$. We show that any network with almost optimal parameter norm\nwill exhibit a CBN structure in both the weights and - under the assumption\nthat the network is stable under large learning rate - the activations, which\nmotivates the common practice of down-sampling; and we verify that the CBN\nresults still hold with down-sampling. Finally we use the CBN structure to\ninterpret the functions learned by CNNs on a number of tasks.\n","authors":["Yuxiao Wen","Arthur Jacot"],"pdf_url":"https://arxiv.org/pdf/2402.08010v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2405.17573v2","updated":"2025-03-06T13:47:53Z","published":"2024-05-27T18:15:05Z","title":"Hamiltonian Mechanics of Feature Learning: Bottleneck Structure in Leaky\n  ResNets","summary":"  We study Leaky ResNets, which interpolate between ResNets and Fully-Connected\nnets depending on an 'effective depth' hyper-parameter $\\tilde{L}$. In the\ninfinite depth limit, we study 'representation geodesics' $A_{p}$: continuous\npaths in representation space (similar to NeuralODEs) from input $p=0$ to\noutput $p=1$ that minimize the parameter norm of the network. We give a\nLagrangian and Hamiltonian reformulation, which highlight the importance of two\nterms: a kinetic energy which favors small layer derivatives\n$\\partial_{p}A_{p}$ and a potential energy that favors low-dimensional\nrepresentations, as measured by the 'Cost of Identity'. The balance between\nthese two forces offers an intuitive understanding of feature learning in\nResNets. We leverage this intuition to explain the emergence of a bottleneck\nstructure, as observed in previous work: for large $\\tilde{L}$ the potential\nenergy dominates and leads to a separation of timescales, where the\nrepresentation jumps rapidly from the high dimensional inputs to a\nlow-dimensional representation, move slowly inside the space of low-dimensional\nrepresentations, before jumping back to the potentially high-dimensional\noutputs. Inspired by this phenomenon, we train with an adaptive layer step-size\nto adapt to the separation of timescales.\n","authors":["Arthur Jacot","Alexandre Kaiser"],"pdf_url":"https://arxiv.org/pdf/2405.17573v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.05664v2","updated":"2025-03-06T13:40:09Z","published":"2024-07-08T06:59:29Z","title":"How DNNs break the Curse of Dimensionality: Compositionality and\n  Symmetry Learning","summary":"  We show that deep neural networks (DNNs) can efficiently learn any\ncomposition of functions with bounded $F_{1}$-norm, which allows DNNs to break\nthe curse of dimensionality in ways that shallow networks cannot. More\nspecifically, we derive a generalization bound that combines a covering number\nargument for compositionality, and the $F_{1}$-norm (or the related Barron\nnorm) for large width adaptivity. We show that the global minimizer of the\nregularized loss of DNNs can fit for example the composition of two functions\n$f^{*}=h\\circ g$ from a small number of observations, assuming $g$ is\nsmooth/regular and reduces the dimensionality (e.g. $g$ could be the quotient\nmap of the symmetries of $f^{*}$), so that $h$ can be learned in spite of its\nlow regularity. The measures of regularity we consider is the Sobolev norm with\ndifferent levels of differentiability, which is well adapted to the $F_{1}$\nnorm. We compute scaling laws empirically and observe phase transitions\ndepending on whether $g$ or $h$ is harder to learn, as predicted by our theory.\n","authors":["Arthur Jacot","Seok Hoan Choi","Yuxiao Wen"],"pdf_url":"https://arxiv.org/pdf/2407.05664v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.12261v3","updated":"2025-03-06T13:39:32Z","published":"2024-10-16T05:58:55Z","title":"CATCH: Channel-Aware multivariate Time Series Anomaly Detection via\n  Frequency Patching","summary":"  Anomaly detection in multivariate time series is challenging as heterogeneous\nsubsequence anomalies may occur. Reconstruction-based methods, which focus on\nlearning normal patterns in the frequency domain to detect diverse abnormal\nsubsequences, achieve promising results, while still falling short on capturing\nfine-grained frequency characteristics and channel correlations. To contend\nwith the limitations, we introduce CATCH, a framework based on frequency\npatching. We propose to patchify the frequency domain into frequency bands,\nwhich enhances its ability to capture fine-grained frequency characteristics.\nTo perceive appropriate channel correlations, we propose a Channel Fusion\nModule (CFM), which features a patch-wise mask generator and a masked-attention\nmechanism. Driven by a bi-level multi-objective optimization algorithm, the CFM\nis encouraged to iteratively discover appropriate patch-wise channel\ncorrelations, and to cluster relevant channels while isolating adverse effects\nfrom irrelevant channels. Extensive experiments on 10 real-world datasets and\n12 synthetic datasets demonstrate that CATCH achieves state-of-the-art\nperformance. We make our code and datasets available at\nhttps://github.com/decisionintelligence/CATCH.\n","authors":["Xingjian Wu","Xiangfei Qiu","Zhengyu Li","Yihang Wang","Jilin Hu","Chenjuan Guo","Hui Xiong","Bin Yang"],"pdf_url":"https://arxiv.org/pdf/2410.12261v3.pdf","comment":"Accepted by ICLR 2025"},{"id":"http://arxiv.org/abs/2503.04426v1","updated":"2025-03-06T13:35:59Z","published":"2025-03-06T13:35:59Z","title":"FORTALESA: Fault-Tolerant Reconfigurable Systolic Array for DNN\n  Inference","summary":"  The emergence of Deep Neural Networks (DNNs) in mission- and safety-critical\napplications brings their reliability to the front. High performance demands of\nDNNs require the use of specialized hardware accelerators. Systolic array\narchitecture is widely used in DNN accelerators due to its parallelism and\nregular structure. This work presents a run-time reconfigurable systolic array\narchitecture with three execution modes and four implementation options. All\nfour implementations are evaluated in terms of resource utilization,\nthroughput, and fault tolerance improvement. The proposed architecture is used\nfor reliability enhancement of DNN inference on systolic array through\nheterogeneous mapping of different network layers to different execution modes.\nThe approach is supported by a novel reliability assessment method based on\nfault propagation analysis. It is used for the exploration of the appropriate\nexecution mode-layer mapping for DNN inference. The proposed architecture\nefficiently protects registers and MAC units of systolic array PEs from\ntransient and permanent faults. The reconfigurability feature enables a speedup\nof up to $3\\times$, depending on layer vulnerability. Furthermore, it requires\n$6\\times$ less resources compared to static redundancy and $2.5\\times$ less\nresources compared to the previously proposed solution for transient faults.\n","authors":["Natalia Cherezova","Artur Jutman","Maksim Jenihhin"],"pdf_url":"https://arxiv.org/pdf/2503.04426v1.pdf","comment":"11 pages, 15 figures"},{"id":"http://arxiv.org/abs/2503.04424v1","updated":"2025-03-06T13:32:13Z","published":"2025-03-06T13:32:13Z","title":"Determinant Estimation under Memory Constraints and Neural Scaling Laws","summary":"  Calculating or accurately estimating log-determinants of large positive\nsemi-definite matrices is of fundamental importance in many machine learning\ntasks. While its cubic computational complexity can already be prohibitive, in\nmodern applications, even storing the matrices themselves can pose a memory\nbottleneck. To address this, we derive a novel hierarchical algorithm based on\nblock-wise computation of the LDL decomposition for large-scale log-determinant\ncalculation in memory-constrained settings. In extreme cases where matrices are\nhighly ill-conditioned, accurately computing the full matrix itself may be\ninfeasible. This is particularly relevant when considering kernel matrices at\nscale, including the empirical Neural Tangent Kernel (NTK) of neural networks\ntrained on large datasets. Under the assumption of neural scaling laws in the\ntest error, we show that the ratio of pseudo-determinants satisfies a power-law\nrelationship, allowing us to derive corresponding scaling laws. This enables\naccurate estimation of NTK log-determinants from a tiny fraction of the full\ndataset; in our experiments, this results in a $\\sim$100,000$\\times$ speedup\nwith improved accuracy over competing approximations. Using these techniques,\nwe successfully estimate log-determinants for dense matrices of extreme sizes,\nwhich were previously deemed intractable and inaccessible due to their enormous\nscale and computational demands.\n","authors":["Siavash Ameli","Chris van der Heide","Liam Hodgkinson","Fred Roosta","Michael W. Mahoney"],"pdf_url":"https://arxiv.org/pdf/2503.04424v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.07978v4","updated":"2025-03-06T13:29:24Z","published":"2023-11-14T08:10:14Z","title":"AfroBench: How Good are Large Language Models on African Languages?","summary":"  Large-scale multilingual evaluations, such as MEGA, often include only a\nhandful of African languages due to the scarcity of high-quality evaluation\ndata and the limited discoverability of existing African datasets. This lack of\nrepresentation hinders comprehensive LLM evaluation across a diverse range of\nlanguages and tasks. To address these challenges, we introduce AfroBench -- a\nmulti-task benchmark for evaluating the performance of LLMs across 64 African\nlanguages, 15 tasks and 22 datasets. AfroBench consists of nine natural\nlanguage understanding datasets, six text generation datasets, six knowledge\nand question answering tasks, and one mathematical reasoning task. We present\nresults comparing the performance of prompting LLMs to fine-tuned baselines\nbased on BERT and T5-style models. Our results suggest large gaps in\nperformance between high-resource languages, such as English, and African\nlanguages across most tasks; but performance also varies based on the\navailability of monolingual data resources. Our findings confirm that\nperformance on African languages continues to remain a hurdle for current LLMs,\nunderscoring the need for additional efforts to close this gap.\n  https://mcgill-nlp.github.io/AfroBench/\n","authors":["Jessica Ojo","Odunayo Ogundepo","Akintunde Oladipo","Kelechi Ogueji","Jimmy Lin","Pontus Stenetorp","David Ifeoluwa Adelani"],"pdf_url":"https://arxiv.org/pdf/2311.07978v4.pdf","comment":"Under review"},{"id":"http://arxiv.org/abs/2503.04418v1","updated":"2025-03-06T13:21:38Z","published":"2025-03-06T13:21:38Z","title":"AOLO: Analysis and Optimization For Low-Carbon Oriented Wireless Large\n  Language Model Services","summary":"  Recent advancements in large language models (LLMs) have led to their\nwidespread adoption and large-scale deployment across various domains. However,\ntheir environmental impact, particularly during inference, has become a growing\nconcern due to their substantial energy consumption and carbon footprint.\nExisting research has focused on inference computation alone, overlooking the\nanalysis and optimization of carbon footprint in network-aided LLM service\nsystems. To address this gap, we propose AOLO, a framework for analysis and\noptimization for low-carbon oriented wireless LLM services. AOLO introduces a\ncomprehensive carbon footprint model that quantifies greenhouse gas emissions\nacross the entire LLM service chain, including computational inference and\nwireless communication. Furthermore, we formulate an optimization problem aimed\nat minimizing the overall carbon footprint, which is solved through joint\noptimization of inference outputs and transmit power under\nquality-of-experience and system performance constraints. To achieve this joint\noptimization, we leverage the energy efficiency of spiking neural networks\n(SNNs) by adopting SNN as the actor network and propose a low-carbon-oriented\noptimization algorithm, i.e., SNN-based deep reinforcement learning (SDRL).\nComprehensive simulations demonstrate that SDRL algorithm significantly reduces\noverall carbon footprint, achieving an 18.77% reduction compared to the\nbenchmark soft actor-critic, highlighting its potential for enabling more\nsustainable LLM inference services.\n","authors":["Xiaoqi Wang","Hongyang Du","Yuehong Gao","Dong In Kim"],"pdf_url":"https://arxiv.org/pdf/2503.04418v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04416v1","updated":"2025-03-06T13:18:37Z","published":"2025-03-06T13:18:37Z","title":"Learning Transformer-based World Models with Contrastive Predictive\n  Coding","summary":"  The DreamerV3 algorithm recently obtained remarkable performance across\ndiverse environment domains by learning an accurate world model based on\nRecurrent Neural Networks (RNNs). Following the success of model-based\nreinforcement learning algorithms and the rapid adoption of the Transformer\narchitecture for its superior training efficiency and favorable scaling\nproperties, recent works such as STORM have proposed replacing RNN-based world\nmodels with Transformer-based world models using masked self-attention.\nHowever, despite the improved training efficiency of these methods, their\nimpact on performance remains limited compared to the Dreamer algorithm,\nstruggling to learn competitive Transformer-based world models. In this work,\nwe show that the next state prediction objective adopted in previous approaches\nis insufficient to fully exploit the representation capabilities of\nTransformers. We propose to extend world model predictions to longer time\nhorizons by introducing TWISTER (Transformer-based World model wIth contraSTivE\nRepresentations), a world model using action-conditioned Contrastive Predictive\nCoding to learn high-level temporal feature representations and improve the\nagent performance. TWISTER achieves a human-normalized mean score of 162% on\nthe Atari 100k benchmark, setting a new record among state-of-the-art methods\nthat do not employ look-ahead search.\n","authors":["Maxime Burchi","Radu Timofte"],"pdf_url":"https://arxiv.org/pdf/2503.04416v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04406v1","updated":"2025-03-06T13:00:53Z","published":"2025-03-06T13:00:53Z","title":"Training-Free Graph Filtering via Multimodal Feature Refinement for\n  Extremely Fast Multimodal Recommendation","summary":"  Multimodal recommender systems improve the performance of canonical\nrecommender systems with no item features by utilizing diverse content types\nsuch as text, images, and videos, while alleviating inherent sparsity of\nuser-item interactions and accelerating user engagement. However, current\nneural network-based models often incur significant computational overhead due\nto the complex training process required to learn and integrate information\nfrom multiple modalities. To overcome this limitation, we propose\nMultiModal-Graph Filtering (MM-GF), a training-free method based on the notion\nof graph filtering (GF) for efficient and accurate multimodal recommendations.\nSpecifically, MM-GF first constructs multiple similarity graphs through\nnontrivial multimodal feature refinement such as robust scaling and vector\nshifting by addressing the heterogeneous characteristics across modalities.\nThen, MM-GF optimally fuses multimodal information using linear low-pass\nfilters across different modalities. Extensive experiments on real-world\nbenchmark datasets demonstrate that MM-GF not only improves recommendation\naccuracy by up to 13.35% compared to the best competitor but also dramatically\nreduces computational costs by achieving the runtime of less than 10 seconds.\n","authors":["Yu-Seung Roh","Joo-Young Kim","Jin-Duk Park","Won-Yong Shin"],"pdf_url":"https://arxiv.org/pdf/2503.04406v1.pdf","comment":"10 pages, 6 figures, 6 tables"},{"id":"http://arxiv.org/abs/2503.04404v1","updated":"2025-03-06T12:58:09Z","published":"2025-03-06T12:58:09Z","title":"Temporal Analysis of NetFlow Datasets for Network Intrusion Detection\n  Systems","summary":"  This paper investigates the temporal analysis of NetFlow datasets for machine\nlearning (ML)-based network intrusion detection systems (NIDS). Although many\nprevious studies have highlighted the critical role of temporal features, such\nas inter-packet arrival time and flow length/duration, in NIDS, the currently\navailable NetFlow datasets for NIDS lack these temporal features. This study\naddresses this gap by creating and making publicly available a set of NetFlow\ndatasets that incorporate these temporal features [1]. With these temporal\nfeatures, we provide a comprehensive temporal analysis of NetFlow datasets by\nexamining the distribution of various features over time and presenting\ntime-series representations of NetFlow features. This temporal analysis has not\nbeen previously provided in the existing literature. We also borrowed an idea\nfrom signal processing, time frequency analysis, and tested it to see how\ndifferent the time frequency signal presentations (TFSPs) are for various\nattacks. The results indicate that many attacks have unique patterns, which\ncould help ML models to identify them more easily.\n","authors":["Majed Luay","Siamak Layeghy","Seyedehfaezeh Hosseininoorbin","Mohanad Sarhan","Nour Moustafa","Marius Portmann"],"pdf_url":"https://arxiv.org/pdf/2503.04404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.04398v1","updated":"2025-03-06T12:52:22Z","published":"2025-03-06T12:52:22Z","title":"Speculative MoE: Communication Efficient Parallel MoE Inference with\n  Speculative Token and Expert Pre-scheduling","summary":"  MoE (Mixture of Experts) prevails as a neural architecture that can scale\nmodern transformer-based LLMs (Large Language Models) to unprecedented scales.\nNevertheless, large MoEs' great demands of computing power, memory capacity and\nmemory bandwidth make scalable serving a fundamental challenge and efficient\nparallel inference has become a requisite to attain adequate throughput under\nlatency constraints. DeepSpeed-MoE, one state-of-the-art MoE inference\nframework, adopts a 3D-parallel paradigm including EP (Expert Parallelism), TP\n(Tensor Parallel) and DP (Data Parallelism). However, our analysis shows\nDeepSpeed-MoE's inference efficiency is largely bottlenecked by EP, which is\nimplemented with costly all-to-all collectives to route token activation. Our\nwork aims to boost DeepSpeed-MoE by strategically reducing EP's communication\noverhead with a technique named Speculative MoE. Speculative MoE has two\nspeculative parallelization schemes, speculative token shuffling and\nspeculative expert grouping, which predict outstanding tokens' expert routing\npaths and pre-schedule tokens and experts across devices to losslessly trim\nEP's communication volume. Besides DeepSpeed-MoE, we also build Speculative MoE\ninto a prevailing MoE inference engine SGLang. Experiments show Speculative MoE\ncan significantly boost state-of-the-art MoE inference frameworks on fast\nhomogeneous and slow heterogeneous interconnects.\n","authors":["Yan Li","Pengfei Zheng","Shuang Chen","Zewei Xu","Yunfei Du","Zhengang Wang"],"pdf_url":"https://arxiv.org/pdf/2503.04398v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.13483v3","updated":"2025-03-06T12:51:49Z","published":"2025-01-23T08:57:02Z","title":"Robust Amortized Bayesian Inference with Self-Consistency Losses on\n  Unlabeled Data","summary":"  Neural amortized Bayesian inference (ABI) can solve probabilistic inverse\nproblems orders of magnitude faster than classical methods. However, neural ABI\nis not yet sufficiently robust for widespread and safe applicability. In\nparticular, when performing inference on observations outside of the scope of\nthe simulated data seen during training, for example, because of model\nmisspecification, the posterior approximations are likely to become highly\nbiased. Due to the bad pre-asymptotic behavior of current neural posterior\nestimators in the out-of-simulation regime, the resulting estimation biases\ncannot be fixed in acceptable time by just simulating more training data. In\nthis proof-of-concept paper, we propose a semi-supervised approach that enables\ntraining not only on (labeled) simulated data generated from the model, but\nalso on unlabeled data originating from any source, including real-world data.\nTo achieve the latter, we exploit Bayesian self-consistency properties that can\nbe transformed into strictly proper losses without requiring knowledge of true\nparameter values, that is, without requiring data labels. The results of our\ninitial experiments show remarkable improvements in the robustness of ABI on\nout-of-simulation data. Even if the observed data is far away from both labeled\nand unlabeled training data, inference remains highly accurate. If our findings\nalso generalize to other scenarios and model classes, we believe that our new\nmethod represents a major breakthrough in neural ABI.\n","authors":["Aayush Mishra","Daniel Habermann","Marvin Schmitt","Stefan T. Radev","Paul-Christian Bürkner"],"pdf_url":"https://arxiv.org/pdf/2501.13483v3.pdf","comment":"added acknowledgements"},{"id":"http://arxiv.org/abs/2503.03285v2","updated":"2025-03-06T12:42:37Z","published":"2025-03-05T09:12:16Z","title":"Enhancing Vietnamese VQA through Curriculum Learning on Raw and\n  Augmented Text Representations","summary":"  Visual Question Answering (VQA) is a multimodal task requiring reasoning\nacross textual and visual inputs, which becomes particularly challenging in\nlow-resource languages like Vietnamese due to linguistic variability and the\nlack of high-quality datasets. Traditional methods often rely heavily on\nextensive annotated datasets, computationally expensive pipelines, and large\npre-trained models, specifically in the domain of Vietnamese VQA, limiting\ntheir applicability in such scenarios. To address these limitations, we propose\na training framework that combines a paraphrase-based feature augmentation\nmodule with a dynamic curriculum learning strategy. Explicitly, augmented\nsamples are considered \"easy\" while raw samples are regarded as \"hard\". The\nframework then utilizes a mechanism that dynamically adjusts the ratio of easy\nto hard samples during training, progressively modifying the same dataset to\nincrease its difficulty level. By enabling gradual adaptation to task\ncomplexity, this approach helps the Vietnamese VQA model generalize well, thus\nimproving overall performance. Experimental results show consistent\nimprovements on the OpenViVQA dataset and mixed outcomes on the ViVQA dataset,\nhighlighting both the potential and challenges of our approach in advancing VQA\nfor Vietnamese language.\n","authors":["Khoi Anh Nguyen","Linh Yen Vu","Thang Dinh Duong","Thuan Nguyen Duong","Huy Thanh Nguyen","Vinh Quang Dinh"],"pdf_url":"https://arxiv.org/pdf/2503.03285v2.pdf","comment":"10 pages, 3 figures, AAAI-25 Workshop on Document Understanding and\n  Intelligence"},{"id":"http://arxiv.org/abs/2407.07918v2","updated":"2025-03-06T12:41:21Z","published":"2024-07-07T12:41:40Z","title":"Detecting new obfuscated malware variants: A lightweight and\n  interpretable machine learning approach","summary":"  Machine learning has been successfully applied in developing malware\ndetection systems, with a primary focus on accuracy, and increasing attention\nto reducing computational overhead and improving model interpretability.\nHowever, an important question remains underexplored: How well can machine\nlearning-based models detect entirely new forms of malware not present in the\ntraining data? In this study, we present a machine learning-based system for\ndetecting obfuscated malware that is not only highly accurate, lightweight and\ninterpretable, but also capable of successfully adapting to new types of\nmalware attacks. Our system is capable of detecting 15 malware subtypes despite\nbeing exclusively trained on one malware subtype, namely the Transponder from\nthe Spyware family. This system was built after training 15 distinct random\nforest-based models, each on a different malware subtype from the\nCIC-MalMem-2022 dataset. These models were evaluated against the entire range\nof malware subtypes, including all unseen malware subtypes. To maintain the\nsystem's streamlined nature, training was confined to the top five most\nimportant features, which also enhanced interpretability. The\nTransponder-focused model exhibited high accuracy, exceeding 99.8%, with an\naverage processing speed of 5.7 microseconds per file. We also illustrate how\nthe Shapley additive explanations technique can facilitate the interpretation\nof the model predictions. Our research contributes to advancing malware\ndetection methodologies, pioneering the feasibility of detecting obfuscated\nmalware by exclusively training a model on a single or a few carefully selected\nmalware subtypes and applying it to detect unseen subtypes.\n","authors":["Oladipo A. Madamidola","Felix Ngobigha","Adnane Ez-zizi"],"pdf_url":"https://arxiv.org/pdf/2407.07918v2.pdf","comment":"30 pages (excluding Appendix), 5 figures and 5 tables. Now published\n  in Intelligent Systems with Applications\n  (https://doi.org/10.1016/j.iswa.2024.200472)"},{"id":"http://arxiv.org/abs/2503.04386v1","updated":"2025-03-06T12:37:55Z","published":"2025-03-06T12:37:55Z","title":"Time-varying Factor Augmented Vector Autoregression with Grouped Sparse\n  Autoencoder","summary":"  Recent economic events, including the global financial crisis and COVID-19\npandemic, have exposed limitations in linear Factor Augmented Vector\nAutoregressive (FAVAR) models for forecasting and structural analysis.\nNonlinear dimension techniques, particularly autoencoders, have emerged as\npromising alternatives in a FAVAR framework, but challenges remain in\nidentifiability, interpretability, and integration with traditional nonlinear\ntime series methods. We address these challenges through two contributions.\nFirst, we introduce a Grouped Sparse autoencoder that employs the\nSpike-and-Slab Lasso prior, with parameters under this prior being shared\nacross variables of the same economic category, thereby achieving\nsemi-identifiability and enhancing model interpretability. Second, we\nincorporate time-varying parameters into the VAR component to better capture\nevolving economic dynamics. Our empirical application to the US economy\ndemonstrates that the Grouped Sparse autoencoder produces more interpretable\nfactors through its parsimonious structure; and its combination with\ntime-varying parameter VAR shows superior performance in both point and density\nforecasting. Impulse response analysis reveals that monetary policy shocks\nduring recessions generate more moderate responses with higher uncertainty\ncompared to expansionary periods.\n","authors":["Yiyong Luo","Brooks Paige","Jim Griffin"],"pdf_url":"https://arxiv.org/pdf/2503.04386v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.07527v2","updated":"2025-03-06T12:34:23Z","published":"2025-02-11T13:08:03Z","title":"Nature Language Model: Deciphering the Language of Nature for Scientific\n  Discovery","summary":"  Foundation models have revolutionized natural language processing and\nartificial intelligence, significantly enhancing how machines comprehend and\ngenerate human languages. Inspired by the success of these foundation models,\nresearchers have developed foundation models for individual scientific domains,\nincluding small molecules, materials, proteins, DNA, RNA and even cells.\nHowever, these models are typically trained in isolation, lacking the ability\nto integrate across different scientific domains. Recognizing that entities\nwithin these domains can all be represented as sequences, which together form\nthe \"language of nature\", we introduce Nature Language Model (NatureLM), a\nsequence-based science foundation model designed for scientific discovery.\nPre-trained with data from multiple scientific domains, NatureLM offers a\nunified, versatile model that enables various applications including: (i)\ngenerating and optimizing small molecules, proteins, RNA, and materials using\ntext instructions; (ii) cross-domain generation/design, such as\nprotein-to-molecule and protein-to-RNA generation; and (iii) top performance\nacross different domains, matching or surpassing state-of-the-art specialist\nmodels. NatureLM offers a promising generalist approach for various scientific\ntasks, including drug discovery (hit generation/optimization, ADMET\noptimization, synthesis), novel material design, and the development of\ntherapeutic proteins or nucleotides. We have developed NatureLM models in\ndifferent sizes (1 billion, 8 billion, and 46.7 billion parameters) and\nobserved a clear improvement in performance as the model size increases.\n","authors":["Yingce Xia","Peiran Jin","Shufang Xie","Liang He","Chuan Cao","Renqian Luo","Guoqing Liu","Yue Wang","Zequn Liu","Yuan-Jyue Chen","Zekun Guo","Yeqi Bai","Pan Deng","Yaosen Min","Ziheng Lu","Hongxia Hao","Han Yang","Jielan Li","Chang Liu","Jia Zhang","Jianwei Zhu","Ran Bi","Kehan Wu","Wei Zhang","Kaiyuan Gao","Qizhi Pei","Qian Wang","Xixian Liu","Yanting Li","Houtian Zhu","Yeqing Lu","Mingqian Ma","Zun Wang","Tian Xie","Krzysztof Maziarz","Marwin Segler","Zhao Yang","Zilong Chen","Yu Shi","Shuxin Zheng","Lijun Wu","Chen Hu","Peggy Dai","Tie-Yan Liu","Haiguang Liu","Tao Qin"],"pdf_url":"https://arxiv.org/pdf/2502.07527v2.pdf","comment":"93 pages"},{"id":"http://arxiv.org/abs/2503.04378v1","updated":"2025-03-06T12:30:24Z","published":"2025-03-06T12:30:24Z","title":"Dedicated Feedback and Edit Models Empower Inference-Time Scaling for\n  Open-Ended General-Domain Tasks","summary":"  Inference-Time Scaling has been critical to the success of recent models such\nas OpenAI o1 and DeepSeek R1. However, many techniques used to train models for\ninference-time scaling require tasks to have answers that can be verified,\nlimiting their application to domains such as math, coding and logical\nreasoning. We take inspiration from how humans make first attempts, ask for\ndetailed feedback from others and make improvements based on such feedback\nacross a wide spectrum of open-ended endeavors. To this end, we collect data\nfor and train dedicated Feedback and Edit Models that are capable of performing\ninference-time scaling for open-ended general-domain tasks. In our setup, one\nmodel generates an initial response, which are given feedback by a second\nmodel, that are then used by a third model to edit the response. We show that\nperformance on Arena Hard, a benchmark strongly predictive of Chatbot Arena Elo\ncan be boosted by scaling the number of initial response drafts, effective\nfeedback and edited responses. When scaled optimally, our setup based on 70B\nmodels from the Llama 3 family can reach SoTA performance on Arena Hard at 92.7\nas of 5 Mar 2025, surpassing OpenAI o1-preview-2024-09-12 with 90.4 and\nDeepSeek R1 with 92.3.\n","authors":["Zhilin Wang","Jiaqi Zeng","Olivier Delalleau","Daniel Egert","Ellie Evans","Hoo-Chang Shin","Felipe Soares","Yi Dong","Oleksii Kuchaiev"],"pdf_url":"https://arxiv.org/pdf/2503.04378v1.pdf","comment":"22 pages, 2 figures"},{"id":"http://arxiv.org/abs/2503.04377v1","updated":"2025-03-06T12:28:59Z","published":"2025-03-06T12:28:59Z","title":"How can representation dimension dominate structurally pruned LLMs?","summary":"  Pruning assumes a subnetwork exists in the original deep neural network,\nwhich can achieve comparative model performance with less computation than the\noriginal. However, it is unclear how the model performance varies with the\ndifferent subnetwork extractions. In this paper, we choose the representation\ndimension (or embedding dimension, model dimension, the dimension of the\nresidual stream in the relevant literature) as the entry point to this issue.\nWe investigate the linear transformations in the LLM transformer blocks and\nconsider a specific structured pruning approach, SliceGPT, to extract the\nsubnetworks of different representation dimensions. We mechanistically analyse\nthe activation flow during the model forward passes, and find the\nrepresentation dimension dominates the linear transformations, model\npredictions, and, finally, the model performance. Explicit analytical relations\nare given to calculate the pruned model performance (perplexity and accuracy)\nwithout actual evaluation, and are empirically validated with\nLlama-3-8B-Instruct and Phi-3-mini-4k-Instruct.\n","authors":["Mingxue Xu","Lisa Alazraki","Danilo P. Mandic"],"pdf_url":"https://arxiv.org/pdf/2503.04377v1.pdf","comment":"ICLR 2025 Workshop on Sparsity in LLMs (SLLM)"},{"id":"http://arxiv.org/abs/2502.16532v2","updated":"2025-03-06T12:19:59Z","published":"2025-02-23T10:48:11Z","title":"Deep unrolling for learning optimal spatially varying regularisation\n  parameters for Total Generalised Variation","summary":"  We extend a recently introduced deep unrolling framework for learning\nspatially varying regularisation parameters in inverse imaging problems to the\ncase of Total Generalised Variation (TGV). The framework combines a deep\nconvolutional neural network (CNN) inferring the two spatially varying TGV\nparameters with an unrolled algorithmic scheme that solves the corresponding\nvariational problem. The two subnetworks are jointly trained end-to-end in a\nsupervised fashion and as such the CNN learns to compute those parameters that\ndrive the reconstructed images as close to the ground truth as possible.\nNumerical results in image denoising and MRI reconstruction show a significant\nqualitative and quantitative improvement compared to the best TGV scalar\nparameter case as well as to other approaches employing spatially varying\nparameters computed by unsupervised methods. We also observe that the inferred\nspatially varying parameter maps have a consistent structure near the image\nedges, asking for further theoretical investigations. In particular, the\nparameter that weighs the first-order TGV term has a triple-edge structure with\nalternating high-low-high values whereas the one that weighs the second-order\nterm attains small values in a large neighbourhood around the edges.\n","authors":["Thanh Trung Vu","Andreas Kofler","Kostas Papafitsoros"],"pdf_url":"https://arxiv.org/pdf/2502.16532v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.04446v1","updated":"2025-03-06T14:02:01Z","published":"2025-03-06T14:02:01Z","title":"SMTPD: A New Benchmark for Temporal Prediction of Social Media\n  Popularity","summary":"  Social media popularity prediction task aims to predict the popularity of\nposts on social media platforms, which has a positive driving effect on\napplication scenarios such as content optimization, digital marketing and\nonline advertising. Though many studies have made significant progress, few of\nthem pay much attention to the integration between popularity prediction with\ntemporal alignment. In this paper, with exploring YouTube's multilingual and\nmulti-modal content, we construct a new social media temporal popularity\nprediction benchmark, namely SMTPD, and suggest a baseline framework for\ntemporal popularity prediction. Through data analysis and experiments, we\nverify that temporal alignment and early popularity play crucial roles in\nsocial media popularity prediction for not only deepening the understanding of\ntemporal dynamics of popularity in social media but also offering a suggestion\nabout developing more effective prediction models in this field. Code is\navailable at https://github.com/zhuwei321/SMTPD.\n","authors":["Yijie Xu","Bolun Zheng","Wei Zhu","Hangjia Pan","Yuchen Yao","Ning Xu","Anan Liu","Quan Zhang","Chenggang Yan"],"pdf_url":"https://arxiv.org/pdf/2503.04446v1.pdf","comment":"accept by CVPR 2025"}]},"2025-03-05T00:00:00Z":{"Computation and Language":[{"id":"http://arxiv.org/abs/2410.10877v2","updated":"2025-03-05T23:56:10Z","published":"2024-10-09T10:07:55Z","title":"Improving Data Efficiency via Curating LLM-Driven Rating Systems","summary":"  Instruction tuning is critical for adapting large language models (LLMs) to\ndownstream tasks, and recent studies have demonstrated that small amounts of\nhuman-curated data can outperform larger datasets, challenging traditional data\nscaling laws. While LLM-based data quality rating systems offer a\ncost-effective alternative to human annotation, they often suffer from\ninaccuracies and biases, even in powerful models like GPT-4. In this work, we\nintroduce DS2, a Diversity-aware Score curation method for Data Selection. By\nsystematically modeling error patterns through a score transition matrix, DS2\ncorrects LLM-based scores and promotes diversity in the selected data samples.\nOur approach shows that a curated subset (just 3.3% of the original dataset)\noutperforms full-scale datasets (300k samples) across various machine-alignment\nbenchmarks, and matches or surpasses human-aligned datasets such as LIMA with\nthe same sample size (1k samples). These findings challenge conventional data\nscaling assumptions, highlighting that redundant, low-quality samples can\ndegrade performance and reaffirming that \"more can be less.\"\n","authors":["Jinlong Pang","Jiaheng Wei","Ankit Parag Shah","Zhaowei Zhu","Yaxuan Wang","Chen Qian","Yang Liu","Yujia Bao","Wei Wei"],"pdf_url":"https://arxiv.org/pdf/2410.10877v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.11699v2","updated":"2025-03-05T23:46:26Z","published":"2024-09-18T04:43:41Z","title":"FLARE: Fusing Language Models and Collaborative Architectures for\n  Recommender Enhancement","summary":"  Recent proposals in recommender systems represent items with their textual\ndescription, using a large language model. They show better results on standard\nbenchmarks compared to an item ID-only model, such as Bert4Rec. In this work,\nwe revisit the often-used Bert4Rec baseline and show that with further tuning,\nBert4Rec significantly outperforms previously reported numbers, and in some\ndatasets, is competitive with state-of-the-art models.\n  With revised baselines for item ID-only models, this paper also establishes\nnew competitive results for architectures that combine IDs and textual\ndescriptions. We demonstrate this with Flare (Fusing Language models and\ncollaborative Architectures for Recommender Enhancement). Flare is a novel\nhybrid sequence recommender that integrates a language model with a\ncollaborative filtering model using a Perceiver network.\n  Prior studies focus evaluation on datasets with limited-corpus size, but many\ncommercially-applicable recommender systems common on the web must handle\nlarger corpora. We evaluate Flare on a more realistic dataset with a\nsignificantly larger item vocabulary, introducing new baselines for this\nsetting. This paper also showcases Flare's inherent ability to support\ncritiquing, enabling users to provide feedback and refine recommendations. We\nleverage critiquing as an evaluation method to assess the model's language\nunderstanding and its transferability to the recommendation task.\n","authors":["Liam Hebert","Marialena Kyriakidi","Hubert Pham","Krishna Sayana","James Pine","Sukhdeep Sodhi","Ambarish Jash"],"pdf_url":"https://arxiv.org/pdf/2409.11699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03968v1","updated":"2025-03-05T23:40:10Z","published":"2025-03-05T23:40:10Z","title":"Preliminary Report: Enhancing Role Differentiation in Conversational HCI\n  Through Chromostereopsis","summary":"  We propose leveraging chromostereopsis, a perceptual phenomenon inducing\ndepth perception through color contrast, as a novel approach to visually\ndifferentiating conversational roles in text-based AI interfaces. This method\naims to implicitly communicate role hierarchy and add a subtle sense of\nphysical space.\n","authors":["Matteo Grella"],"pdf_url":"https://arxiv.org/pdf/2503.03968v1.pdf","comment":"Preliminary Report, 8 pages, 1 figures"},{"id":"http://arxiv.org/abs/2503.03962v1","updated":"2025-03-05T23:27:58Z","published":"2025-03-05T23:27:58Z","title":"On the Acquisition of Shared Grammatical Representations in Bilingual\n  Language Models","summary":"  While crosslingual transfer is crucial to contemporary language models'\nmultilingual capabilities, how it occurs is not well understood. In this paper,\nwe ask what happens to a monolingual language model when it begins to be\ntrained on a second language. Specifically, we train small bilingual models for\nwhich we control the amount of data for each language and the order of language\nexposure. To find evidence of shared multilingual representations, we turn to\nstructural priming, a method used to study grammatical representations in\nhumans. We first replicate previous crosslingual structural priming results and\nfind that after controlling for training data quantity and language exposure,\nthere are asymmetrical effects across language pairs and directions. We argue\nthat this asymmetry may shape hypotheses about human structural priming\neffects. We also find that structural priming effects are less robust for less\nsimilar language pairs, highlighting potential limitations of crosslingual\ntransfer learning and shared representations for typologically diverse\nlanguages.\n","authors":["Catherine Arnett","Tyler A. Chang","James A. Michaelov","Benjamin K. Bergen"],"pdf_url":"https://arxiv.org/pdf/2503.03962v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03960v1","updated":"2025-03-05T23:26:12Z","published":"2025-03-05T23:26:12Z","title":"Performance Comparison of Large Language Models on Advanced Calculus\n  Problems","summary":"  This paper presents an in-depth analysis of the performance of seven\ndifferent Large Language Models (LLMs) in solving a diverse set of math\nadvanced calculus problems. The study aims to evaluate these models' accuracy,\nreliability, and problem-solving capabilities, including ChatGPT 4o, Gemini\nAdvanced with 1.5 Pro, Copilot Pro, Claude 3.5 Sonnet, Meta AI, Mistral AI, and\nPerplexity. The assessment was conducted through a series of thirty-two test\nproblems, encompassing a total of 320 points. The problems covered various\ntopics, from vector calculations and geometric interpretations to integral\nevaluations and optimization tasks. The results highlight significant trends\nand patterns in the models' performance, revealing both their strengths and\nweaknesses - for instance, models like ChatGPT 4o and Mistral AI demonstrated\nconsistent accuracy across various problem types, indicating their robustness\nand reliability in mathematical problem-solving, while models such as Gemini\nAdvanced with 1.5 Pro and Meta AI exhibited specific weaknesses, particularly\nin complex problems involving integrals and optimization, suggesting areas for\ntargeted improvements. The study also underscores the importance of\nre-prompting in achieving accurate solutions, as seen in several instances\nwhere models initially provided incorrect answers but corrected them upon\nre-prompting. Overall, this research provides valuable insights into the\ncurrent capabilities and limitations of LLMs in the domain of math calculus,\nwith the detailed analysis of each model's performance on specific problems\noffering a comprehensive understanding of their strengths and areas for\nimprovement, contributing to the ongoing development and refinement of LLM\ntechnology. The findings are particularly relevant for educators, researchers,\nand developers seeking to leverage LLMs for educational and practical\napplications in mathematics.\n","authors":["In Hak Moon"],"pdf_url":"https://arxiv.org/pdf/2503.03960v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2404.08819v3","updated":"2025-03-05T23:00:57Z","published":"2024-04-12T21:30:06Z","title":"The Illusion of State in State-Space Models","summary":"  State-space models (SSMs) have emerged as a potential alternative\narchitecture for building large language models (LLMs) compared to the\npreviously ubiquitous transformer architecture. One theoretical weakness of\ntransformers is that they cannot express certain kinds of sequential\ncomputation and state tracking (Merrill & Sabharwal, 2023), which SSMs are\nexplicitly designed to address via their close architectural similarity to\nrecurrent neural networks (RNNs). But do SSMs truly have an advantage (over\ntransformers) in expressive power for state tracking? Surprisingly, the answer\nis no. Our analysis reveals that the expressive power of SSMs is limited very\nsimilarly to transformers: SSMs cannot express computation outside the\ncomplexity class $\\mathsf{TC}^0$. In particular, this means they cannot solve\nsimple state-tracking problems like permutation composition. It follows that\nSSMs are provably unable to accurately track chess moves with certain notation,\nevaluate code, or track entities in a long narrative. To supplement our formal\nanalysis, we report experiments showing that Mamba-style SSMs indeed struggle\nwith state tracking. Thus, despite its recurrent formulation, the \"state\" in an\nSSM is an illusion: SSMs have similar expressiveness limitations to\nnon-recurrent models like transformers, which may fundamentally limit their\nability to solve real-world state-tracking problems.\n","authors":["William Merrill","Jackson Petty","Ashish Sabharwal"],"pdf_url":"https://arxiv.org/pdf/2404.08819v3.pdf","comment":"To appear at ICML 2024. 9 pages + appendices"},{"id":"http://arxiv.org/abs/2503.03932v1","updated":"2025-03-05T22:05:42Z","published":"2025-03-05T22:05:42Z","title":"Tec-Habilidad: Skill Classification for Bridging Education and\n  Employment","summary":"  Job application and assessment processes have evolved significantly in recent\nyears, largely due to advancements in technology and changes in the way\ncompanies operate. Skill extraction and classification remain an important\ncomponent of the modern hiring process as it provides a more objective way to\nevaluate candidates and automatically align their skills with the job\nrequirements. However, to effectively evaluate the skills, the skill extraction\ntools must recognize varied mentions of skills on resumes, including direct\nmentions, implications, synonyms, acronyms, phrases, and proficiency levels,\nand differentiate between hard and soft skills. While tools like LLMs (Large\nModel Models) help extract and categorize skills from job applications, there's\na lack of comprehensive datasets for evaluating the effectiveness of these\nmodels in accurately identifying and classifying skills in Spanish-language job\napplications. This gap hinders our ability to assess the reliability and\nprecision of the models, which is crucial for ensuring that the selected\ncandidates truly possess the required skills for the job. In this paper, we\ndevelop a Spanish language dataset for skill extraction and classification,\nprovide annotation methodology to distinguish between knowledge, skill, and\nabilities, and provide deep learning baselines to advance robust solutions for\nskill classification.\n","authors":["Sabur Butt","Hector G. Ceballos","Diana P. Madera"],"pdf_url":"https://arxiv.org/pdf/2503.03932v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10254v3","updated":"2025-03-05T21:57:04Z","published":"2024-10-14T08:10:34Z","title":"LoLCATs: On Low-Rank Linearizing of Large Language Models","summary":"  Recent works show we can linearize large language models (LLMs) -- swapping\nthe quadratic attentions of popular Transformer-based LLMs with subquadratic\nanalogs, such as linear attention -- avoiding the expensive pretraining costs.\nHowever, linearizing LLMs often significantly degrades model quality, still\nrequires training over billions of tokens, and remains limited to smaller 1.3B\nto 7B LLMs. We thus propose Low-rank Linear Conversion via Attention Transfer\n(LoLCATs), a simple two-step method that improves LLM linearizing quality with\norders of magnitudes less memory and compute. We base these steps on two\nfindings. First, we can replace an LLM's softmax attentions with\nclosely-approximating linear attentions, simply by training the linear\nattentions to match their softmax counterparts with an output MSE loss\n(\"attention transfer\"). Then, this enables adjusting for approximation errors\nand recovering LLM quality simply with low-rank adaptation (LoRA). LoLCATs\nsignificantly improves linearizing quality, training efficiency, and\nscalability. We significantly reduce the linearizing quality gap and produce\nstate-of-the-art subquadratic LLMs from Llama 3 8B and Mistral 7B v0.1, leading\nto 20+ points of improvement on 5-shot MMLU. Furthermore, LoLCATs does so with\nonly 0.2% of past methods' model parameters and 0.4% of their training tokens.\nFinally, we apply LoLCATs to create the first linearized 70B and 405B LLMs (50x\nlarger than prior work). When compared with prior approaches under the same\ncompute budgets, LoLCATs significantly improves linearizing quality, closing\nthe gap between linearized and original Llama 3.1 70B and 405B LLMs by 77.8%\nand 78.1% on 5-shot MMLU.\n","authors":["Michael Zhang","Simran Arora","Rahul Chalamala","Alan Wu","Benjamin Spector","Aaryan Singhal","Krithik Ramesh","Christopher Ré"],"pdf_url":"https://arxiv.org/pdf/2410.10254v3.pdf","comment":"58 pages, 25 figures, 26 tables, ICLR 2025"},{"id":"http://arxiv.org/abs/2402.14973v4","updated":"2025-03-05T21:42:27Z","published":"2024-02-22T21:22:04Z","title":"GenCeption: Evaluate Vision LLMs with Unlabeled Unimodal Data","summary":"  Multimodal Large Language Models (MLLMs) are typically assessed using\nexpensive annotated multimodal benchmarks, which often lag behind the rapidly\nevolving demands of MLLM evaluation. This paper outlines and validates\nGenCeption, a novel, annotation-free evaluation method that requires only\nunimodal data to measure inter-modality semantic coherence and inversely\nassesses MLLMs' tendency to hallucinate. This approach eliminates the need for\ncostly data annotation, minimizes the risk of training data contamination, is\nexpected to result in slower benchmark saturation, and avoids the illusion of\nemerging abilities. Inspired by the DrawCeption game, GenCeption begins with a\nnon-textual sample and proceeds through iterative description and generation\nsteps. The semantic drift across iterations is quantified using the GC@T\nmetric. While GenCeption is principally applicable to MLLMs across various\nmodalities, this paper focuses on its implementation and validation for Vision\nLLMs (VLLMs). Based on the GenCeption method, we establish the MMECeption\nbenchmark for evaluating VLLMs, and compare the performance of several popular\nVLLMs and human annotators. Our empirical results validate GenCeption's\neffectiveness, demonstrating strong correlations with established VLLM\nbenchmarks. VLLMs still significantly lag behind human performance and struggle\nespecially with text-intensive tasks.\n","authors":["Lele Cao","Valentin Buchner","Zineb Senane","Fangkai Yang"],"pdf_url":"https://arxiv.org/pdf/2402.14973v4.pdf","comment":"Published by Computer Speech & Language\n  (https://doi.org/10.1016/j.csl.2025.101785). Source code and Leaderboard:\n  https://github.com/llcresearch/GenCeption"},{"id":"http://arxiv.org/abs/2503.03920v1","updated":"2025-03-05T21:41:03Z","published":"2025-03-05T21:41:03Z","title":"Personalized Federated Fine-tuning for Heterogeneous Data: An Automatic\n  Rank Learning Approach via Two-Level LoRA","summary":"  We study the task of personalized federated fine-tuning with heterogeneous\ndata in the context of language models, where clients collaboratively fine-tune\na language model (e.g., BERT, GPT) without sharing their local data, achieving\npersonalization simultaneously. While recent efforts have applied\nparameter-efficient fine-tuning techniques like low-rank adaptation (LoRA) in\nfederated settings, they typically use single or multiple independent low-rank\nadapters with predefined maximal and minimal ranks, which may not be optimal\nfor diverse data sources over clients.\n  To address this issue, we propose PF2LoRA, a new personalized federated\nfine-tuning algorithm built on a novel \\emph{automatic rank learning approach\nvia two-level LoRA}. Given the pretrained language model whose weight is\nfrozen, our algorithm aims to learn two levels of adaptation simultaneously:\nthe first level aims to learn a common adapter for all clients, while the\nsecond level fosters individual client personalization. A key advantage of\nPF2LoRA is its ability to adaptively determine a suitable rank based on an\nindividual client's data, rather than relying on a predefined rank that is\nagnostic to data heterogeneity. We present a synthetic example that highlights\nhow PF2LoRA automatically learns the ground-truth rank for each client,\ntailoring the adaptation to match the properties of their individual data.\nNotably, this approach introduces minimal additional memory overhead, as the\nsecond-level adaptation comprises a small number of parameters compared to the\nfirst level. Our experiments on natural language understanding and generation\ntasks demonstrate that PF2LoRA significantly outperforms existing federated\nfine-tuning methods.\n","authors":["Jie Hao","Yuman Wu","Ali Payani","Myungjin Lee","Mingrui Liu"],"pdf_url":"https://arxiv.org/pdf/2503.03920v1.pdf","comment":"28 pages, 5 figures"},{"id":"http://arxiv.org/abs/2410.18653v2","updated":"2025-03-05T21:24:29Z","published":"2024-10-24T11:32:01Z","title":"Towards Better Open-Ended Text Generation: A Multicriteria Evaluation\n  Framework","summary":"  Open-ended text generation has become a prominent task in natural language\nprocessing due to the rise of powerful (large) language models. However,\nevaluating the quality of these models and the employed decoding strategies\nremains challenging because of trade-offs among widely used metrics such as\ncoherence, diversity, and perplexity. Decoding methods often excel in some\nmetrics while underperforming in others, complicating the establishment of a\nclear ranking. In this paper, we present novel ranking strategies within this\nmulticriteria framework. Specifically, we employ benchmarking approaches based\non partial orderings and present a new summary metric designed to balance\nexisting automatic indicators, providing a more holistic evaluation of text\ngeneration quality. Our experiments demonstrate that the proposed methods offer\na robust way to compare decoding strategies, and serve as valuable tools in\nguiding model selection for open-ended text generation tasks. Finally, we\nsuggest future directions for improving evaluation methodologies in text\ngeneration. Our codebase, datasets, and models are publicly available.\n","authors":["Esteban Garces Arias","Hannah Blocher","Julian Rodemann","Meimingwei Li","Christian Heumann","Matthias Aßenmacher"],"pdf_url":"https://arxiv.org/pdf/2410.18653v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.16366v2","updated":"2025-03-05T20:31:47Z","published":"2025-02-22T21:48:48Z","title":"A generative approach to LLM harmfulness detection with special red flag\n  tokens","summary":"  Most safety training methods for large language models (LLMs) based on\nfine-tuning rely on dramatically changing the output distribution of the model\nwhen faced with a harmful request, shifting it from an unsafe answer to a\nrefusal to respond. These methods inherently compromise model capabilities and\nmight make auto-regressive models vulnerable to attacks that make likely an\ninitial token of affirmative response. To avoid that, we propose to expand the\nmodel's vocabulary with a special token we call red flag token (<rf>) and\npropose to fine-tune the model to generate this token at any time harmful\ncontent is generated or about to be generated. This novel safety training\nmethod effectively augments LLMs into generative classifiers of harmfulness at\nall times during the conversation. This method offers several advantages: it\nenables the model to explicitly learn the concept of harmfulness while\nmarginally affecting the generated distribution, thus maintaining the model's\nutility. It also evaluates each generated answer rather than just the input\nprompt and provides a stronger defence against sampling-based attacks. In\naddition, it simplifies the evaluation of the model's robustness and reduces\ncorrelated failures when combined with a classifier. We further show an\nincreased robustness to long contexts, and supervised fine-tuning attacks.\n","authors":["Sophie Xhonneux","David Dobre","Mehrnaz Mofakhami","Leo Schwinn","Gauthier Gidel"],"pdf_url":"https://arxiv.org/pdf/2502.16366v2.pdf","comment":"13 pages, 6 figures"},{"id":"http://arxiv.org/abs/2303.14537v4","updated":"2025-03-05T20:30:05Z","published":"2023-03-25T19:03:57Z","title":"Deep Augmentation: Dropout as Augmentation for Self-Supervised Learning","summary":"  Despite dropout's ubiquity in machine learning, its effectiveness as a form\nof data augmentation remains under-explored. We address two key questions: (i)\nWhen is dropout effective as an augmentation strategy? (ii) Is dropout uniquely\neffective under these conditions? To explore these questions, we propose Deep\nAugmentation, a network- and modality-agnostic method that applies dropout or\nPCA transformations to targeted layers in neural networks. Through extensive\nexperiments on contrastive learning tasks in NLP, computer vision, and graph\nlearning, we find that uniformly applying dropout across layers does not\nconsistently improve performance. Instead, dropout proves most beneficial in\ndeeper layers and can be matched by alternative augmentations (e.g., PCA). We\nalso show that a stop-gradient operation is critical for ensuring dropout\nfunctions effectively as an augmentation, and that performance trends invert\nwhen moving from contrastive tasks to supervised tasks. Our analysis suggests\nthat Deep Augmentation helps mitigate inter-layer co-adaptation -- a notable\nissue in self-supervised learning due to the absence of labeled data. Drawing\non these insights, we outline a procedure for selecting the optimal\naugmentation layer and demonstrate that Deep Augmentation can outperform\ntraditional input-level augmentations. This simple yet powerful approach can be\nseamlessly integrated into a wide range of architectures and modalities,\nyielding notable gains in both performance and generalization.\n","authors":["Rickard Brüel-Gabrielsson","Tongzhou Wang","Manel Baradad","Justin Solomon"],"pdf_url":"https://arxiv.org/pdf/2303.14537v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03874v1","updated":"2025-03-05T20:09:59Z","published":"2025-03-05T20:09:59Z","title":"LEWIS (LayEr WIse Sparsity) -- A Training Free Guided Model Merging\n  Approach","summary":"  As specialized large language models (LLMs) become increasingly prevalent,\nmodel merging methods are being used to combine them to create a single\nmulti-task model without requiring any additional data or training. However,\nthese approaches fall short when the objective of merging is to increase the\ndownstream model's performance on a particular task-specific benchmark. In this\nwork, we propose LEWIS (Layer Wise Sparsity), a guided model-merging framework\nthat uses activation-based layer importance to dynamically adjust layer-wise\ntask-vector sparsity required for the merge process. LEWIS uses a calibration\ndataset to prioritize critical layers during the task-vector pruning process\nrequired for model merging. This approach guides existing merging methods by\npreserving essential layer-wise task-specific knowledge while ensuring the\nmerged model performs the best at benchmarks resembling the calibration\ndataset. Our experiments demonstrate the effectiveness of LEWIS with\nperformance improvements of code instruction-following and math-solving models\ncreated through model merging up to 4 percent and 11.3 percent, respectively,\noutperforming unguided data-less model merging approaches that use\nuniform-sparsity.\n","authors":["Hetarth Chopra","Vidhi Rambhia","Vikram Adve"],"pdf_url":"https://arxiv.org/pdf/2503.03874v1.pdf","comment":"Accepted at ICLR 2025 Workshop: SLLM (Sparsity in Large Language\n  Models)"},{"id":"http://arxiv.org/abs/2406.01566v2","updated":"2025-03-05T20:00:57Z","published":"2024-06-03T17:47:53Z","title":"Helix: Serving Large Language Models over Heterogeneous GPUs and Network\n  via Max-Flow","summary":"  This paper introduces Helix, a distributed system for high-throughput,\nlow-latency large language model (LLM) serving in heterogeneous GPU clusters.\nThe key idea behind Helix is to formulate inference computation of LLMs over\nheterogeneous GPUs and network connections as a max-flow problem on directed,\nweighted graphs, whose nodes represent GPU instances and edges capture both GPU\nand network heterogeneity through their capacities. Helix then uses a mixed\ninteger linear programming (MILP) algorithm to discover highly optimized\nstrategies to serve LLMs on heterogeneous GPUs. This approach allows Helix to\njointly optimize model placement and request scheduling, two highly entangled\ntasks in heterogeneous LLM serving. Our evaluation on several heterogeneous\nclusters ranging from 24 to 42 GPU nodes shows that Helix improves serving\nthroughput by up to 3.3x and reduces prompting and decoding latency by up to\n66% and 24%, respectively, compared to existing approaches. Helix is available\nat https://github.com/Thesys-lab/Helix-ASPLOS25.\n","authors":["Yixuan Mei","Yonghao Zhuang","Xupeng Miao","Juncheng Yang","Zhihao Jia","Rashmi Vinayak"],"pdf_url":"https://arxiv.org/pdf/2406.01566v2.pdf","comment":"ASPLOS 2025"},{"id":"http://arxiv.org/abs/2404.08679v2","updated":"2025-03-05T19:51:23Z","published":"2024-04-07T10:32:49Z","title":"Your Finetuned Large Language Model is Already a Powerful\n  Out-of-distribution Detector","summary":"  We revisit the likelihood ratio between a pretrained large language model\n(LLM) and its finetuned variant as a criterion for out-of-distribution (OOD)\ndetection. The intuition behind such a criterion is that, the pretrained LLM\nhas the prior knowledge about OOD data due to its large amount of training\ndata, and once finetuned with the in-distribution data, the LLM has sufficient\nknowledge to distinguish their difference. Leveraging the power of LLMs, we\nshow that, the likelihood ratio can serve as an effective OOD detection\ncriterion. Moreover, we apply the proposed LLM-based likelihood ratio to detect\nOOD questions in question-answering (QA) systems, which can be used to improve\nthe performance of specialized LLMs for general questions. Given that\nlikelihood can be easily obtained by the loss functions within contemporary\nneural network frameworks, it is straightforward to implement this approach in\npractice. Since both the pretrained LLMs and its various finetuned models are\nwidely available from online platforms such as Hugging Face, our proposed\ncriterion can be effortlessly incorporated for OOD detection without the need\nfor further training. We conduct comprehensive evaluation across on multiple\nsettings, including far OOD, near OOD, spam detection, and QA scenarios, to\ndemonstrate the effectiveness of the method. Code can be found at\nhttps://github.com/andiac/LLMOODratio\n","authors":["Andi Zhang","Tim Z. Xiao","Weiyang Liu","Robert Bamler","Damon Wischik"],"pdf_url":"https://arxiv.org/pdf/2404.08679v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03862v1","updated":"2025-03-05T19:46:04Z","published":"2025-03-05T19:46:04Z","title":"Not-Just-Scaling Laws: Towards a Better Understanding of the Downstream\n  Impact of Language Model Design Decisions","summary":"  Improvements in language model capabilities are often attributed to\nincreasing model size or training data, but in some cases smaller models\ntrained on curated data or with different architectural decisions can\noutperform larger ones trained on more tokens. What accounts for this? To\nquantify the impact of these design choices, we meta-analyze 92 open-source\npretrained models across a wide array of scales, including state-of-the-art\nopen-weights models as well as less performant models and those with less\nconventional design decisions. We find that by incorporating features besides\nmodel size and number of training tokens, we can achieve a relative 3-28%\nincrease in ability to predict downstream performance compared with using scale\nalone. Analysis of model design decisions reveal insights into data\ncomposition, such as the trade-off between language and code tasks at 15-25\\%\ncode, as well as the better performance of some architectural decisions such as\nchoosing rotary over learned embeddings. Broadly, our framework lays a\nfoundation for more systematic investigation of how model development choices\nshape final capabilities.\n","authors":["Emmy Liu","Amanda Bertsch","Lintang Sutawika","Lindia Tjuatja","Patrick Fernandes","Lara Marinov","Michael Chen","Shreya Singhal","Carolin Lawrence","Aditi Raghunathan","Kiril Gashteovski","Graham Neubig"],"pdf_url":"https://arxiv.org/pdf/2503.03862v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03854v1","updated":"2025-03-05T19:36:43Z","published":"2025-03-05T19:36:43Z","title":"Vision-Language Models Struggle to Align Entities across Modalities","summary":"  Cross-modal entity linking refers to the ability to align entities and their\nattributes across different modalities. While cross-modal entity linking is a\nfundamental skill needed for real-world applications such as multimodal code\ngeneration, fake news detection, or scene understanding, it has not been\nthoroughly studied in the literature. In this paper, we introduce a new task\nand benchmark to address this gap. Our benchmark, MATE, consists of 5.5k\nevaluation instances featuring visual scenes aligned with their textual\nrepresentations. To evaluate cross-modal entity linking performance, we design\na question-answering task that involves retrieving one attribute of an object\nin one modality based on a unique attribute of that object in another modality.\nWe evaluate state-of-the-art Vision-Language Models (VLMs) and humans on this\ntask, and find that VLMs struggle significantly compared to humans,\nparticularly as the number of objects in the scene increases. Our analysis also\nshows that, while chain-of-thought prompting can improve VLM performance,\nmodels remain far from achieving human-level proficiency. These findings\nhighlight the need for further research in cross-modal entity linking and show\nthat MATE is a strong benchmark to support that progress.\n","authors":["Iñigo Alonso","Ander Salaberria","Gorka Azkune","Jeremy Barnes","Oier Lopez de Lacalle"],"pdf_url":"https://arxiv.org/pdf/2503.03854v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.05881v3","updated":"2025-03-05T19:34:08Z","published":"2024-06-09T18:40:24Z","title":"LGR2: Language Guided Reward Relabeling for Accelerating Hierarchical\n  Reinforcement Learning","summary":"  Developing interactive systems that utilize natural language instructions to\nsolve complex robotic control tasks has long been a goal of the robotics\ncommunity. While Large Language Models (LLMs) excel at logical reasoning,\nin-context learning, and code generation, translating high-level instructions\ninto low-level robotic actions still remains challenging. Furthermore, solving\nsuch tasks often requires acquiring policies to execute diverse subtasks and\nintegrating them to achieve the final objective. Hierarchical Reinforcement\nLearning (HRL) offers a promising solution for solving such tasks by enabling\ntemporal abstraction and improved exploration. However, HRL suffers from\nnon-stationarity caused by the changing lower-level behaviour, which hinders\neffective policy learning. We propose LGR2, a novel HRL framework that\nmitigates non-stationarity in HRL by using language-guided higher-level rewards\nthat remain unaffected by the changing lower-level policy behaviour. To analyze\nthe efficacy of our approach, we perform empirical analysis to demonstrate that\nLGR2 effectively mitigates non-stationarity in HRL and attains success rates\nexceeding 70% in challenging, sparsely-rewarded robotic navigation and\nmanipulation environments, where other baselines typically fail to show\nsignificant progress. Finally, we perform real-world robotic experiments on\ncomplex tasks and demonstrate that LGR2 consistently outperforms the baselines.\n","authors":["Utsav Singh","Pramit Bhattacharyya","Vinay P. Namboodiri"],"pdf_url":"https://arxiv.org/pdf/2406.05881v3.pdf","comment":null}],"Information Retrieval":[{"id":"http://arxiv.org/abs/2409.11699v2","updated":"2025-03-05T23:46:26Z","published":"2024-09-18T04:43:41Z","title":"FLARE: Fusing Language Models and Collaborative Architectures for\n  Recommender Enhancement","summary":"  Recent proposals in recommender systems represent items with their textual\ndescription, using a large language model. They show better results on standard\nbenchmarks compared to an item ID-only model, such as Bert4Rec. In this work,\nwe revisit the often-used Bert4Rec baseline and show that with further tuning,\nBert4Rec significantly outperforms previously reported numbers, and in some\ndatasets, is competitive with state-of-the-art models.\n  With revised baselines for item ID-only models, this paper also establishes\nnew competitive results for architectures that combine IDs and textual\ndescriptions. We demonstrate this with Flare (Fusing Language models and\ncollaborative Architectures for Recommender Enhancement). Flare is a novel\nhybrid sequence recommender that integrates a language model with a\ncollaborative filtering model using a Perceiver network.\n  Prior studies focus evaluation on datasets with limited-corpus size, but many\ncommercially-applicable recommender systems common on the web must handle\nlarger corpora. We evaluate Flare on a more realistic dataset with a\nsignificantly larger item vocabulary, introducing new baselines for this\nsetting. This paper also showcases Flare's inherent ability to support\ncritiquing, enabling users to provide feedback and refine recommendations. We\nleverage critiquing as an evaluation method to assess the model's language\nunderstanding and its transferability to the recommendation task.\n","authors":["Liam Hebert","Marialena Kyriakidi","Hubert Pham","Krishna Sayana","James Pine","Sukhdeep Sodhi","Ambarish Jash"],"pdf_url":"https://arxiv.org/pdf/2409.11699v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01776v2","updated":"2025-03-05T17:51:09Z","published":"2025-03-03T17:59:48Z","title":"Beyond Matryoshka: Revisiting Sparse Coding for Adaptive Representation","summary":"  Many large-scale systems rely on high-quality deep representations\n(embeddings) to facilitate tasks like retrieval, search, and generative\nmodeling. Matryoshka Representation Learning (MRL) recently emerged as a\nsolution for adaptive embedding lengths, but it requires full model retraining\nand suffers from noticeable performance degradations at short lengths. In this\npaper, we show that sparse coding offers a compelling alternative for achieving\nadaptive representation with minimal overhead and higher fidelity. We propose\nContrastive Sparse Representation (CSR), a method that sparsifies pre-trained\nembeddings into a high-dimensional but selectively activated feature space. By\nleveraging lightweight autoencoding and task-aware contrastive objectives, CSR\npreserves semantic quality while allowing flexible, cost-effective inference at\ndifferent sparsity levels. Extensive experiments on image, text, and multimodal\nbenchmarks demonstrate that CSR consistently outperforms MRL in terms of both\naccuracy and retrieval speed-often by large margins-while also cutting training\ntime to a fraction of that required by MRL. Our results establish sparse coding\nas a powerful paradigm for adaptive representation learning in real-world\napplications where efficiency and fidelity are both paramount. Code is\navailable at https://github.com/neilwen987/CSR_Adaptive_Rep\n","authors":["Tiansheng Wen","Yifei Wang","Zequn Zeng","Zhong Peng","Yudi Su","Xinyang Liu","Bo Chen","Hongwei Liu","Stefanie Jegelka","Chenyu You"],"pdf_url":"https://arxiv.org/pdf/2503.01776v2.pdf","comment":"A novel sparse coding framework designed for learning adaptive\n  representation"},{"id":"http://arxiv.org/abs/2503.03687v1","updated":"2025-03-05T17:28:16Z","published":"2025-03-05T17:28:16Z","title":"Addressing Overprescribing Challenges: Fine-Tuning Large Language Models\n  for Medication Recommendation Tasks","summary":"  Medication recommendation systems have garnered attention within healthcare\nfor their potential to deliver personalized and efficacious drug combinations\nbased on patient's clinical data. However, existing methodologies encounter\nchallenges in adapting to diverse Electronic Health Records (EHR) systems and\neffectively utilizing unstructured data, resulting in limited generalization\ncapabilities and suboptimal performance. Recently, interest is growing in\nharnessing Large Language Models (LLMs) in the medical domain to support\nhealthcare professionals and enhance patient care. Despite the emergence of\nmedical LLMs and their promising results in tasks like medical question\nanswering, their practical applicability in clinical settings, particularly in\nmedication recommendation, often remains underexplored.\n  In this study, we evaluate both general-purpose and medical-specific LLMs for\nmedication recommendation tasks. Our findings reveal that LLMs frequently\nencounter the challenge of overprescribing, leading to heightened clinical\nrisks and diminished medication recommendation accuracy. To address this issue,\nwe propose Language-Assisted Medication Recommendation (LAMO), which employs a\nparameter-efficient fine-tuning approach to tailor open-source LLMs for optimal\nperformance in medication recommendation scenarios. LAMO leverages the wealth\nof clinical information within clinical notes, a resource often underutilized\nin traditional methodologies. As a result of our approach, LAMO outperforms\nprevious state-of-the-art methods by over 10% in internal validation accuracy.\nFurthermore, temporal and external validations demonstrate LAMO's robust\ngeneralization capabilities across various temporal and hospital contexts.\nAdditionally, an out-of-distribution medication recommendation experiment\ndemonstrates LAMO's remarkable accuracy even with medications outside the\ntraining data.\n","authors":["Zihao Zhao","Chenxiao Fan","Chongming Gao","Fuli Feng","Xiangnan He"],"pdf_url":"https://arxiv.org/pdf/2503.03687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.03524v1","updated":"2025-03-05T14:08:53Z","published":"2025-03-05T14:08:53Z","title":"Intrinsic and Extrinsic Factor Disentanglement for Recommendation in\n  Various Context Scenarios","summary":"  In recommender systems, the patterns of user behaviors (e.g., purchase,\nclick) may vary greatly in different contexts (e.g., time and location). This\nis because user behavior is jointly determined by two types of factors:\nintrinsic factors, which reflect consistent user preference, and extrinsic\nfactors, which reflect external incentives that may vary in different contexts.\nDifferentiating between intrinsic and extrinsic factors helps learn user\nbehaviors better. However, existing studies have only considered\ndifferentiating them from a single, pre-defined context (e.g., time or\nlocation), ignoring the fact that a user's extrinsic factors may be influenced\nby the interplay of various contexts at the same time. In this paper, we\npropose the Intrinsic-Extrinsic Disentangled Recommendation (IEDR) model, a\ngeneric framework that differentiates intrinsic from extrinsic factors\nconsidering various contexts simultaneously, enabling more accurate\ndifferentiation of factors and hence the improvement of recommendation\naccuracy. IEDR contains a context-invariant contrastive learning component to\ncapture intrinsic factors, and a disentanglement component to extract extrinsic\nfactors under the interplay of various contexts. The two components work\ntogether to achieve effective factor learning. Extensive experiments on\nreal-world datasets demonstrate IEDR's effectiveness in learning disentangled\nfactors and significantly improving recommendation accuracy by up to 4% in\nNDCG.\n","authors":["Yixin Su","Wei Jiang","Fangquan Lin","Cheng Yang","Sarah M. Erfani","Junhao Gan","Yunxiang Zhao","Ruixuan Li","Rui Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.03524v1.pdf","comment":"32 pages, 13 figures, 11 tables. Accepted by Transactions of\n  Information Systems"},{"id":"http://arxiv.org/abs/2410.23841v2","updated":"2025-03-05T12:10:57Z","published":"2024-10-31T11:47:21Z","title":"Beyond Content Relevance: Evaluating Instruction Following in Retrieval\n  Models","summary":"  Instruction-following capabilities in LLMs have progressed significantly,\nenabling more complex user interactions through detailed prompts. However,\nretrieval systems have not matched these advances, most of them still relies on\ntraditional lexical and semantic matching techniques that fail to fully capture\nuser intent. Recent efforts have introduced instruction-aware retrieval models,\nbut these primarily focus on intrinsic content relevance, which neglects the\nimportance of customized preferences for broader document-level attributes.\nThis study evaluates the instruction-following capabilities of various\nretrieval models beyond content relevance, including LLM-based dense retrieval\nand reranking models. We develop InfoSearch, a novel retrieval evaluation\nbenchmark spanning six document-level attributes: Audience, Keyword, Format,\nLanguage, Length, and Source, and introduce novel metrics -- Strict Instruction\nCompliance Ratio (SICR) and Weighted Instruction Sensitivity Evaluation (WISE)\nto accurately assess the models' responsiveness to instructions. Our findings\nindicate that although fine-tuning models on instruction-aware retrieval\ndatasets and increasing model size enhance performance, most models still fall\nshort of instruction compliance.\n","authors":["Jianqun Zhou","Yuanlei Zheng","Wei Chen","Qianqian Zheng","Hui Su","Wei Zhang","Rui Meng","Xiaoyu Shen"],"pdf_url":"https://arxiv.org/pdf/2410.23841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.09155v2","updated":"2025-03-05T07:48:05Z","published":"2025-02-13T10:36:17Z","title":"Use of Air Quality Sensor Network Data for Real-time Pollution-Aware POI\n  Suggestion","summary":"  This demo paper introduces AirSense-R, a privacy-preserving mobile\napplication that delivers real-time, pollution-aware recommendations for urban\npoints of interest (POIs). By merging live air quality data from AirSENCE\nsensor networks in Bari (Italy) and Cork (Ireland) with user preferences, the\nsystem enables health-conscious decision-making. It employs collaborative\nfiltering for personalization, federated learning for privacy, and a prediction\nengine to detect anomalies and interpolate sparse sensor data. The proposed\nsolution adapts dynamically to urban air quality while safeguarding user\nprivacy. The code and demonstration video are available at\nhttps://github.com/AirtownApp/Airtown-Application.git.\n","authors":["Giuseppe Fasano","Yashar Deldjoo","Tommaso di Noia","Bianca Lau","Sina Adham-Khiabani","Eric Morris","Xia Liu","Ganga Chinna Rao Devarapu","Liam O'Faolain"],"pdf_url":"https://arxiv.org/pdf/2502.09155v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01711v3","updated":"2025-03-05T05:52:00Z","published":"2025-03-03T16:24:36Z","title":"MAPS: Motivation-Aware Personalized Search via LLM-Driven Consultation\n  Alignment","summary":"  Personalized product search aims to retrieve and rank items that match users'\npreferences and search intent. Despite their effectiveness, existing approaches\ntypically assume that users' query fully captures their real motivation.\nHowever, our analysis of a real-world e-commerce platform reveals that users\noften engage in relevant consultations before searching, indicating they refine\nintents through consultations based on motivation and need. The implied\nmotivation in consultations is a key enhancing factor for personalized search.\nThis unexplored area comes with new challenges including aligning contextual\nmotivations with concise queries, bridging the category-text gap, and filtering\nnoise within sequence history. To address these, we propose a Motivation-Aware\nPersonalized Search (MAPS) method. It embeds queries and consultations into a\nunified semantic space via LLMs, utilizes a Mixture of Attention Experts (MoAE)\nto prioritize critical semantics, and introduces dual alignment: (1)\ncontrastive learning aligns consultations, reviews, and product features; (2)\nbidirectional attention integrates motivation-aware embeddings with user\npreferences. Extensive experiments on real and synthetic data show MAPS\noutperforms existing methods in both retrieval and ranking tasks.\n","authors":["Weicong Qin","Yi Xu","Weijie Yu","Chenglei Shen","Ming He","Jianping Fan","Xiao Zhang","Jun Xu"],"pdf_url":"https://arxiv.org/pdf/2503.01711v3.pdf","comment":"added project repository & dataset URL"},{"id":"http://arxiv.org/abs/2503.03165v1","updated":"2025-03-05T04:16:36Z","published":"2025-03-05T04:16:36Z","title":"A Predict-Then-Optimize Customer Allocation Framework for Online Fund\n  Recommendation","summary":"  With the rapid growth of online investment platforms, funds can be\ndistributed to individual customers online. The central issue is to match funds\nwith potential customers under constraints. Most mainstream platforms adopt the\nrecommendation formulation to tackle the problem. However, the traditional\nrecommendation regime has its inherent drawbacks when applying the\nfund-matching problem with multiple constraints. In this paper, we model the\nfund matching under the allocation formulation. We design PTOFA, a\nPredict-Then-Optimize Fund Allocation framework. This data-driven framework\nconsists of two stages, i.e., prediction and optimization, which aim to predict\nexpected revenue based on customer behavior and optimize the impression\nallocation to achieve the maximum revenue under the necessary constraints,\nrespectively. Extensive experiments on real-world datasets from an industrial\nonline investment platform validate the effectiveness and efficiency of our\nsolution. Additionally, the online A/B tests demonstrate PTOFA's effectiveness\nin the real-world fund recommendation scenario.\n","authors":["Xing Tang","Yunpeng Weng","Fuyuan Lyu","Dugang Liu","Xiuqiang He"],"pdf_url":"https://arxiv.org/pdf/2503.03165v1.pdf","comment":"Accepted by DASFAA 2025"},{"id":"http://arxiv.org/abs/2503.02589v2","updated":"2025-03-05T03:28:29Z","published":"2025-03-04T13:12:39Z","title":"MCiteBench: A Benchmark for Multimodal Citation Text Generation in MLLMs","summary":"  Multimodal Large Language Models (MLLMs) have advanced in integrating diverse\nmodalities but frequently suffer from hallucination. A promising solution to\nmitigate this issue is to generate text with citations, providing a transparent\nchain for verification. However, existing work primarily focuses on generating\ncitations for text-only content, overlooking the challenges and opportunities\nof multimodal contexts. To address this gap, we introduce MCiteBench, the first\nbenchmark designed to evaluate and analyze the multimodal citation text\ngeneration ability of MLLMs. Our benchmark comprises data derived from academic\npapers and review-rebuttal interactions, featuring diverse information sources\nand multimodal content. We comprehensively evaluate models from multiple\ndimensions, including citation quality, source reliability, and answer\naccuracy. Through extensive experiments, we observe that MLLMs struggle with\nmultimodal citation text generation. We also conduct deep analyses of models'\nperformance, revealing that the bottleneck lies in attributing the correct\nsources rather than understanding the multimodal content.\n","authors":["Caiyu Hu","Yikai Zhang","Tinghui Zhu","Yiwei Ye","Yanghua Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.02589v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02604v2","updated":"2025-03-05T02:48:49Z","published":"2024-10-03T15:45:15Z","title":"Long-Sequence Recommendation Models Need Decoupled Embeddings","summary":"  Lifelong user behavior sequences are crucial for capturing user interests and\npredicting user responses in modern recommendation systems. A two-stage\nparadigm is typically adopted to handle these long sequences: a subset of\nrelevant behaviors is first searched from the original long sequences via an\nattention mechanism in the first stage and then aggregated with the target item\nto construct a discriminative representation for prediction in the second\nstage. In this work, we identify and characterize, for the first time, a\nneglected deficiency in existing long-sequence recommendation models: a single\nset of embeddings struggles with learning both attention and representation,\nleading to interference between these two processes. Initial attempts to\naddress this issue with some common methods (e.g., linear projections -- a\ntechnique borrowed from language processing) proved ineffective, shedding light\non the unique challenges of recommendation models. To overcome this, we propose\nthe Decoupled Attention and Representation Embeddings (DARE) model, where two\ndistinct embedding tables are initialized and learned separately to fully\ndecouple attention and representation. Extensive experiments and analysis\ndemonstrate that DARE provides more accurate searches of correlated behaviors\nand outperforms baselines with AUC gains up to 0.9% on public datasets and\nnotable improvements on Tencent's advertising platform. Furthermore, decoupling\nembedding spaces allows us to reduce the attention embedding dimension and\naccelerate the search procedure by 50% without significant performance impact,\nenabling more efficient, high-performance online serving. Code in PyTorch for\nexperiments, including model analysis, is available at\nhttps://github.com/thuml/DARE.\n","authors":["Ningya Feng","Junwei Pan","Jialong Wu","Baixu Chen","Ximei Wang","Qian Li","Xian Hu","Jie Jiang","Mingsheng Long"],"pdf_url":"https://arxiv.org/pdf/2410.02604v2.pdf","comment":"ICLR 2025. First three authors contributed equally. Code is available\n  at https://github.com/thuml/DARE"},{"id":"http://arxiv.org/abs/2503.02603v2","updated":"2025-03-05T02:13:38Z","published":"2025-03-04T13:21:47Z","title":"OkraLong: A Flexible Retrieval-Augmented Framework for Long-Text Query\n  Processing","summary":"  Large Language Models (LLMs) encounter challenges in efficiently processing\nlong-text queries, as seen in applications like enterprise document analysis\nand financial report comprehension. While conventional solutions employ\nlong-context processing or Retrieval-Augmented Generation (RAG), they suffer\nfrom prohibitive input expenses or incomplete information. Recent advancements\nadopt context compression and dynamic retrieval loops, but still sacrifice\ncritical details or incur iterative costs. To address these limitations, we\npropose OkraLong, a novel framework that flexibly optimizes the entire\nprocessing workflow. Unlike prior static or coarse-grained adaptive strategies,\nOkraLong adopts fine-grained orchestration through three synergistic\ncomponents: analyzer, organizer and executor. The analyzer characterizes the\ntask states, which guide the organizer in dynamically scheduling the workflow.\nThe executor carries out the execution and generates the final answer.\nExperimental results demonstrate that OkraLong not only enhances answer\naccuracy but also achieves cost-effectiveness across a variety of datasets.\n","authors":["Yulong Hui","Yihao Liu","Yao Lu","Huanchen Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.02603v2.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.08642v2","updated":"2025-03-05T15:55:52Z","published":"2024-10-11T09:10:26Z","title":"More than Memes: A Multimodal Topic Modeling Approach to Conspiracy\n  Theories on Telegram","summary":"  To address the increasing prevalence of (audio-)visual data on social media,\nand to capture the evolving and dynamic nature of this communication,\nresearchers have begun to explore the potential of unsupervised approaches for\nanalyzing multimodal online content. However, existing research often neglects\nvisual content beyond memes, and in addition lacks methods to compare topic\nmodels across modalities. Our study addresses these gaps by applying multimodal\ntopic modeling for analyzing conspiracy theories in German-language Telegram\nchannels. We use BERTopic with CLIP for the analysis of textual and visual data\nin a corpus of ~40, 000 Telegram messages posted in October 2023 in 571\nGerman-language Telegram channels known for disseminating conspiracy theories.\nThrough this dataset, we provide insights into unimodal and multimodal topic\nmodels by analyzing symmetry and intersections of topics across modalities. We\ndemonstrate the variety of textual and visual content shared in the channels\ndiscovered through the topic modeling, and propose a conceptual framework for\nthe analysis of textual and visual discursive strategies in the communication\nof conspiracy theories. We apply the framework in a case study of the topic\ngroup Israel Gaza.\n","authors":["Elisabeth Steffen"],"pdf_url":"https://arxiv.org/pdf/2410.08642v2.pdf","comment":"12 pages, 10 figures"},{"id":"http://arxiv.org/abs/2410.07369v3","updated":"2025-03-05T00:06:53Z","published":"2024-10-09T18:33:06Z","title":"An Undetectable Watermark for Generative Image Models","summary":"  We present the first undetectable watermarking scheme for generative image\nmodels. Undetectability ensures that no efficient adversary can distinguish\nbetween watermarked and un-watermarked images, even after making many adaptive\nqueries. In particular, an undetectable watermark does not degrade image\nquality under any efficiently computable metric. Our scheme works by selecting\nthe initial latents of a diffusion model using a pseudorandom error-correcting\ncode (Christ and Gunn, 2024), a strategy which guarantees undetectability and\nrobustness. We experimentally demonstrate that our watermarks are\nquality-preserving and robust using Stable Diffusion 2.1. Our experiments\nverify that, in contrast to every prior scheme we tested, our watermark does\nnot degrade image quality. Our experiments also demonstrate robustness:\nexisting watermark removal attacks fail to remove our watermark from images\nwithout significantly degrading the quality of the images. Finally, we find\nthat we can robustly encode 512 bits in our watermark, and up to 2500 bits when\nthe images are not subjected to watermark removal attacks. Our code is\navailable at https://github.com/XuandongZhao/PRC-Watermark.\n","authors":["Sam Gunn","Xuandong Zhao","Dawn Song"],"pdf_url":"https://arxiv.org/pdf/2410.07369v3.pdf","comment":"ICLR 2025"}]},"2025-03-04T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.03062v1","updated":"2025-03-04T23:52:49Z","published":"2025-03-04T23:52:49Z","title":"Semi-Supervised In-Context Learning: A Baseline Study","summary":"  Most existing work in data selection for In-Context Learning (ICL) has\nfocused on constructing demonstrations from ground truth annotations, with\nlimited attention given to selecting reliable self-generated annotations. In\nthis work, we propose a three-step semi-supervised ICL framework: annotation\ngeneration, demonstration selection, and semi-supervised inference. Our\nbaseline, Naive-SemiICL, which prompts select high-confidence self-generated\ndemonstrations for ICL prompting, outperforms a 16-shot baseline by an average\nof 9.94% across 16 datasets. We further introduce IterPSD, an annotation\napproach that refines pseudo-demonstrations iteratively, achieving up to 6.8%\nadditional gains in classification tasks. Lastly, we reveal a scaling law for\nsemi-supervised ICL, where models achieve optimal performance with over 1,000\ndemonstrations.\n","authors":["Zhengyao Gu","Henry Peng Zou","Yankai Chen","Aiwei Liu","Weizhi Zhang","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2503.03062v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02993v1","updated":"2025-03-04T20:39:07Z","published":"2025-03-04T20:39:07Z","title":"Zero-Shot Multi-Label Classification of Bangla Documents: Large Decoders\n  Vs. Classic Encoders","summary":"  Bangla, a language spoken by over 300 million native speakers and ranked as\nthe sixth most spoken language worldwide, presents unique challenges in natural\nlanguage processing (NLP) due to its complex morphological characteristics and\nlimited resources. While recent Large Decoder Based models (LLMs), such as GPT,\nLLaMA, and DeepSeek, have demonstrated excellent performance across many NLP\ntasks, their effectiveness in Bangla remains largely unexplored. In this paper,\nwe establish the first benchmark comparing decoder-based LLMs with classic\nencoder-based models for Zero-Shot Multi-Label Classification (Zero-Shot-MLC)\ntask in Bangla. Our evaluation of 32 state-of-the-art models reveals that,\nexisting so-called powerful encoders and decoders still struggle to achieve\nhigh accuracy on the Bangla Zero-Shot-MLC task, suggesting a need for more\nresearch and resources for Bangla NLP.\n","authors":["Souvika Sarkar","Md. Najib Hasan","Santu Karmaker"],"pdf_url":"https://arxiv.org/pdf/2503.02993v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2409.13744v2","updated":"2025-03-04T19:15:49Z","published":"2024-09-11T00:16:17Z","title":"A Simplified Retriever to Improve Accuracy of Phenotype Normalizations\n  by Large Language Models","summary":"  Large language models (LLMs) have shown improved accuracy in phenotype term\nnormalization tasks when augmented with retrievers that suggest candidate\nnormalizations based on term definitions. In this work, we introduce a\nsimplified retriever that enhances LLM accuracy by searching the Human\nPhenotype Ontology (HPO) for candidate matches using contextual word embeddings\nfrom BioBERT without the need for explicit term definitions. Testing this\nmethod on terms derived from the clinical synopses of Online Mendelian\nInheritance in Man (OMIM), we demonstrate that the normalization accuracy of a\nstate-of-the-art LLM increases from a baseline of 62.3% without augmentation to\n90.3% with retriever augmentation. This approach is potentially generalizable\nto other biomedical term normalization tasks and offers an efficient\nalternative to more complex retrieval methods.\n","authors":["Daniel B. Hier","Thanh Son Do","Tayo Obafemi-Ajayi"],"pdf_url":"https://arxiv.org/pdf/2409.13744v2.pdf","comment":"Published by Frontiers in Digital Health"},{"id":"http://arxiv.org/abs/2503.02948v1","updated":"2025-03-04T19:09:48Z","published":"2025-03-04T19:09:48Z","title":"ExpertGenQA: Open-ended QA generation in Specialized Domains","summary":"  Generating high-quality question-answer pairs for specialized technical\ndomains remains challenging, with existing approaches facing a tradeoff between\nleveraging expert examples and achieving topical diversity. We present\nExpertGenQA, a protocol that combines few-shot learning with structured topic\nand style categorization to generate comprehensive domain-specific QA pairs.\nUsing U.S. Federal Railroad Administration documents as a test bed, we\ndemonstrate that ExpertGenQA achieves twice the efficiency of baseline few-shot\napproaches while maintaining $94.4\\%$ topic coverage. Through systematic\nevaluation, we show that current LLM-based judges and reward models exhibit\nstrong bias toward superficial writing styles rather than content quality. Our\nanalysis using Bloom's Taxonomy reveals that ExpertGenQA better preserves the\ncognitive complexity distribution of expert-written questions compared to\ntemplate-based approaches. When used to train retrieval models, our generated\nqueries improve top-1 accuracy by $13.02\\%$ over baseline performance,\ndemonstrating their effectiveness for downstream applications in technical\ndomains.\n","authors":["Haz Sameen Shahgir","Chansong Lim","Jia Chen","Evangelos E. Papalexakis","Yue Dong"],"pdf_url":"https://arxiv.org/pdf/2503.02948v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02922v1","updated":"2025-03-04T18:47:17Z","published":"2025-03-04T18:47:17Z","title":"Optimizing open-domain question answering with graph-based retrieval\n  augmented generation","summary":"  In this work, we benchmark various graph-based retrieval-augmented generation\n(RAG) systems across a broad spectrum of query types, including OLTP-style\n(fact-based) and OLAP-style (thematic) queries, to address the complex demands\nof open-domain question answering (QA). Traditional RAG methods often fall\nshort in handling nuanced, multi-document synthesis tasks. By structuring\nknowledge as graphs, we can facilitate the retrieval of context that captures\ngreater semantic depth and enhances language model operations. We explore\ngraph-based RAG methodologies and introduce TREX, a novel, cost-effective\nalternative that combines graph-based and vector-based retrieval techniques.\nOur benchmarking across four diverse datasets highlights the strengths of\ndifferent RAG methodologies, demonstrates TREX's ability to handle multiple\nopen-domain QA types, and reveals the limitations of current evaluation\nmethods.\n  In a real-world technical support case study, we demonstrate how TREX\nsolutions can surpass conventional vector-based RAG in efficiently synthesizing\ndata from heterogeneous sources. Our findings underscore the potential of\naugmenting large language models with advanced retrieval and orchestration\ncapabilities, advancing scalable, graph-based AI solutions.\n","authors":["Joyce Cahoon","Prerna Singh","Nick Litombe","Jonathan Larson","Ha Trinh","Yiwen Zhu","Andreas Mueller","Fotis Psallidas","Carlo Curino"],"pdf_url":"https://arxiv.org/pdf/2503.02922v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2103.03223v5","updated":"2025-03-04T15:20:55Z","published":"2021-03-04T18:51:06Z","title":"A Comparative Evaluation of Quantification Methods","summary":"  Quantification represents the problem of estimating the distribution of class\nlabels on unseen data. It also represents a growing research field in\nsupervised machine learning, for which a large variety of different algorithms\nhas been proposed in recent years. However, a comprehensive empirical\ncomparison of quantification methods that supports algorithm selection is not\navailable yet. In this work, we close this research gap by conducting a\nthorough empirical performance comparison of 24 different quantification\nmethods on overall more than 40 data sets, considering binary as well as\nmulticlass quantification settings. We observe that no single algorithm\ngenerally outperforms all competitors, but identify a group of methods\nincluding the threshold selection-based Median Sweep and TSMax methods, the DyS\nframework including the HDy method, Forman's mixture model, and Friedman's\nmethod that performs best in the binary setting. For the multiclass setting, we\nobserve that a different, broad group of algorithms yields good performance,\nincluding the HDx method, the Generalized Probabilistic Adjusted Count, the\nreadme method, the energy distance minimization method, the EM algorithm for\nquantification, and Friedman's method. We also find that tuning the underlying\nclassifiers has in most cases only a limited impact on the quantification\nperformance. More generally, we find that the performance on multiclass\nquantification is inferior to the results obtained in the binary setting. Our\nresults can guide practitioners who intend to apply quantification algorithms\nand help researchers to identify opportunities for future research.\n","authors":["Tobias Schumacher","Markus Strohmaier","Florian Lemmerich"],"pdf_url":"https://arxiv.org/pdf/2103.03223v5.pdf","comment":"41 pages, 18 figures, 9 tables"},{"id":"http://arxiv.org/abs/2502.18495v2","updated":"2025-03-04T15:16:52Z","published":"2025-02-19T01:37:24Z","title":"A Comprehensive Survey on Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) is an emerging yet challenging task that\nallows users to search for target images using a multimodal query, comprising a\nreference image and a modification text specifying the user's desired changes\nto the reference image. Given its significant academic and practical value, CIR\nhas become a rapidly growing area of interest in the computer vision and\nmachine learning communities, particularly with the advances in deep learning.\nTo the best of our knowledge, there is currently no comprehensive review of CIR\nto provide a timely overview of this field. Therefore, we synthesize insights\nfrom over 120 publications in top conferences and journals, including ACM TOIS,\nSIGIR, and CVPR In particular, we systematically categorize existing supervised\nCIR and zero-shot CIR models using a fine-grained taxonomy. For a comprehensive\nreview, we also briefly discuss approaches for tasks closely related to CIR,\nsuch as attribute-based CIR and dialog-based CIR. Additionally, we summarize\nbenchmark datasets for evaluation and analyze existing supervised and zero-shot\nCIR methods by comparing experimental results across multiple datasets.\nFurthermore, we present promising future directions in this field, offering\npractical insights for researchers interested in further exploration. The\ncurated collection of related works is maintained and continuously updated in\nhttps://github.com/haokunwen/Awesome-Composed-Image-Retrieval.\n","authors":["Xuemeng Song","Haoqiang Lin","Haokun Wen","Bohan Hou","Mingzhu Xu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2502.18495v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02695v1","updated":"2025-03-04T15:12:18Z","published":"2025-03-04T15:12:18Z","title":"Zero-Shot Complex Question-Answering on Long Scientific Documents","summary":"  With the rapid development in Transformer-based language models, the reading\ncomprehension tasks on short documents and simple questions have been largely\naddressed. Long documents, specifically the scientific documents that are\ndensely packed with knowledge discovered and developed by humans, remain\nrelatively unexplored. These documents often come with a set of complex and\nmore realistic questions, adding to their complexity. We present a zero-shot\npipeline framework that enables social science researchers to perform\nquestion-answering tasks that are complex yet of predetermined question formats\non full-length research papers without requiring machine learning expertise.\nOur approach integrates pre-trained language models to handle challenging\nscenarios including multi-span extraction, multi-hop reasoning, and long-answer\ngeneration. Evaluating on MLPsych, a novel dataset of social psychology papers\nwith annotated complex questions, we demonstrate that our framework achieves\nstrong performance through combination of extractive and generative models.\nThis work advances document understanding capabilities for social sciences\nwhile providing practical tools for researchers.\n","authors":["Wanting Wang"],"pdf_url":"https://arxiv.org/pdf/2503.02695v1.pdf","comment":"AAAI 2025 Workshop on Document Understanding and Intelligence"},{"id":"http://arxiv.org/abs/2503.02674v1","updated":"2025-03-04T14:46:01Z","published":"2025-03-04T14:46:01Z","title":"Towards Robust Expert Finding in Community Question Answering Platforms","summary":"  This paper introduces TUEF, a topic-oriented user-interaction model for fair\nExpert Finding in Community Question Answering (CQA) platforms. The Expert\nFinding task in CQA platforms involves identifying proficient users capable of\nproviding accurate answers to questions from the community. To this aim, TUEF\nimproves the robustness and credibility of the CQA platform through a more\nprecise Expert Finding component. The key idea of TUEF is to exploit diverse\ntypes of information, specifically, content and social information, to identify\nmore precisely experts thus improving the robustness of the task. We assess\nTUEF through reproducible experiments conducted on a large-scale dataset from\nStackOverflow. The results consistently demonstrate that TUEF outperforms\nstate-of-the-art competitors while promoting transparent expert identification.\n","authors":["Maddalena Amendola","Andrea Passarella","Raffaele Perego"],"pdf_url":"https://arxiv.org/pdf/2503.02674v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02671v1","updated":"2025-03-04T14:43:43Z","published":"2025-03-04T14:43:43Z","title":"Are some books better than others?","summary":"  Scholars, awards committees, and laypeople frequently discuss the merit of\nwritten works. Literary professionals and journalists differ in how much\nperspectivism they concede in their book reviews. Here, we quantify how\nstrongly book reviews are determined by the actual book contents vs.\nidiosyncratic reader tendencies. In our analysis of 624,320 numerical and\ntextual book reviews, we find that the contents of professionally published\nbooks are not predictive of a random reader's reading enjoyment. Online reviews\nof popular fiction and non-fiction books carry up to ten times more information\nabout the reviewer than about the book. For books of a preferred genre, readers\nmight be less likely to give low ratings, but still struggle to converge in\ntheir relative assessments. We find that book evaluations generalize more\nacross experienced review writers than casual readers. When discussing specific\nissues with a book, one review text had poor predictability of issues brought\nup in another review of the same book. We conclude that extreme perspectivism\nis a justifiable position when researching literary quality, bestowing literary\nawards, and designing recommendation systems.\n","authors":["Hannes Rosenbusch","Luke Korthals"],"pdf_url":"https://arxiv.org/pdf/2503.02671v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02614v1","updated":"2025-03-04T13:34:19Z","published":"2025-03-04T13:34:19Z","title":"Personalized Generation In Large Model Era: A Survey","summary":"  In the era of large models, content generation is gradually shifting to\nPersonalized Generation (PGen), tailoring content to individual preferences and\nneeds. This paper presents the first comprehensive survey on PGen,\ninvestigating existing research in this rapidly growing field. We conceptualize\nPGen from a unified perspective, systematically formalizing its key components,\ncore objectives, and abstract workflows. Based on this unified perspective, we\npropose a multi-level taxonomy, offering an in-depth review of technical\nadvancements, commonly used datasets, and evaluation metrics across multiple\nmodalities, personalized contexts, and tasks. Moreover, we envision the\npotential applications of PGen and highlight open challenges and promising\ndirections for future exploration. By bridging PGen research across multiple\nmodalities, this survey serves as a valuable resource for fostering knowledge\nsharing and interdisciplinary collaboration, ultimately contributing to a more\npersonalized digital landscape.\n","authors":["Yiyan Xu","Jinghao Zhang","Alireza Salemi","Xinting Hu","Wenjie Wang","Fuli Feng","Hamed Zamani","Xiangnan He","Tat-Seng Chua"],"pdf_url":"https://arxiv.org/pdf/2503.02614v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2311.13121v2","updated":"2025-03-04T12:17:16Z","published":"2023-11-22T02:49:14Z","title":"GENET: Unleashing the Power of Side Information for Recommendation via\n  Hypergraph Pre-training","summary":"  Recommendation with side information has drawn significant research interest\ndue to its potential to mitigate user feedback sparsity. However, existing\nmodels struggle with generalization across diverse domains and types of side\ninformation. In particular, three challenges have not been addressed, and they\nare (1) the diverse formats of side information, including text sequences. (2)\nThe diverse semantics of side information that describes items and users from\nmulti-level in a context different from recommendation systems. (3) The diverse\ncorrelations in side information to measure similarity over multiple objects\nbeyond pairwise relations. In this paper, we introduce GENET (Generalized\nhypErgraph pretraiNing on sidE informaTion), which pre-trains user and item\nrepresentations on feedback-irrelevant side information and fine-tunes the\nrepresentations on user feedback data. GENET leverages pre-training as a means\nto prevent side information from overshadowing critical ID features and\nfeedback signals. It employs a hypergraph framework to accommodate various\ntypes of diverse side information. During pre-training, GENET integrates tasks\nfor hyperlink prediction and self-supervised contrast to capture fine-grained\nsemantics at both local and global levels. Additionally, it introduces a unique\nstrategy to enhance pre-training robustness by perturbing positive samples\nwhile maintaining high-order relations. Extensive experiments demonstrate that\nGENET exhibits strong generalization capabilities, outperforming the SOTA\nmethod by up to 38% in TOP-N recommendation and Sequential recommendation tasks\non various datasets with different side information.\n","authors":["Yang Li","Qi'ao Zhao","Chen Lin","Zhenjie Zhang","Xiaomin Zhu","Jinsong Su"],"pdf_url":"https://arxiv.org/pdf/2311.13121v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02542v1","updated":"2025-03-04T12:12:37Z","published":"2025-03-04T12:12:37Z","title":"Efficient Long Sequential Low-rank Adaptive Attention for Click-through\n  rate Prediction","summary":"  In the context of burgeoning user historical behavior data, Accurate\nclick-through rate(CTR) prediction requires effective modeling of lengthy user\nbehavior sequences. As the volume of such data keeps swelling, the focus of\nresearch has shifted towards developing effective long-term behavior modeling\nmethods to capture latent user interests. Nevertheless, the complexity\nintroduced by large scale data brings about computational hurdles. There is a\npressing need to strike a balance between achieving high model performance and\nmeeting the strict response time requirements of online services. While\nexisting retrieval-based methods (e.g., similarity filtering or attention\napproximation) achieve practical runtime efficiency, they inherently compromise\ninformation fidelity through aggressive sequence truncation or attention\nsparsification. This paper presents a novel attention mechanism. It overcomes\nthe shortcomings of existing methods while ensuring computational efficiency.\nThis mechanism learn compressed representation of sequence with length $L$ via\nlow-rank projection matrices (rank $r \\ll L$), reducing attention complexity\nfrom $O(L)$ to $O(r)$. It also integrates a uniquely designed loss function to\npreserve nonlinearity of attention. In the inference stage, the mechanism\nadopts matrix absorption and prestorage strategies. These strategies enable it\nto effectively satisfy online constraints. Comprehensive offline and online\nexperiments demonstrate that the proposed method outperforms current\nstate-of-the-art solutions.\n","authors":["Xin Song","Xiaochen Li","Jinxin Hu","Hong Wen","Zulong Chen","Yu Zhang","Xiaoyi Zeng","Zhang Jing"],"pdf_url":"https://arxiv.org/pdf/2503.02542v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02453v1","updated":"2025-03-04T10:00:05Z","published":"2025-03-04T10:00:05Z","title":"Sparse Meets Dense: Unified Generative Recommendations with Cascaded\n  Sparse-Dense Representations","summary":"  Generative models have recently gained attention in recommendation systems by\ndirectly predicting item identifiers from user interaction sequences. However,\nexisting methods suffer from significant information loss due to the separation\nof stages such as quantization and sequence modeling, hindering their ability\nto achieve the modeling precision and accuracy of sequential dense retrieval\ntechniques. Integrating generative and dense retrieval methods remains a\ncritical challenge. To address this, we introduce the Cascaded Organized\nBi-Represented generAtive retrieval (COBRA) framework, which innovatively\nintegrates sparse semantic IDs and dense vectors through a cascading process.\nOur method alternates between generating these representations by first\ngenerating sparse IDs, which serve as conditions to aid in the generation of\ndense vectors. End-to-end training enables dynamic refinement of dense\nrepresentations, capturing both semantic insights and collaborative signals\nfrom user-item interactions. During inference, COBRA employs a coarse-to-fine\nstrategy, starting with sparse ID generation and refining them into dense\nvectors via the generative model. We further propose BeamFusion, an innovative\napproach combining beam search with nearest neighbor scores to enhance\ninference flexibility and recommendation diversity. Extensive experiments on\npublic datasets and offline tests validate our method's robustness. Online A/B\ntests on a real-world advertising platform with over 200 million daily users\ndemonstrate substantial improvements in key metrics, highlighting COBRA's\npractical advantages.\n","authors":["Yuhao Yang","Zhi Ji","Zhaopeng Li","Yi Li","Zhonglin Mo","Yue Ding","Kai Chen","Zijian Zhang","Jie Li","Shuanglong Li","Lin Liu"],"pdf_url":"https://arxiv.org/pdf/2503.02453v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02401v1","updated":"2025-03-04T08:43:19Z","published":"2025-03-04T08:43:19Z","title":"Hierarchical Re-ranker Retriever (HRR)","summary":"  Retrieving the right level of context for a given query is a perennial\nchallenge in information retrieval - too large a chunk dilutes semantic\nspecificity, while chunks that are too small lack broader context. This paper\nintroduces the Hierarchical Re-ranker Retriever (HRR), a framework designed to\nachieve both fine-grained and high-level context retrieval for large language\nmodel (LLM) applications. In HRR, documents are split into sentence-level and\nintermediate-level (512 tokens) chunks to maximize vector-search quality for\nboth short and broad queries. We then employ a reranker that operates on these\n512-token chunks, ensuring an optimal balance neither too coarse nor too fine\nfor robust relevance scoring. Finally, top-ranked intermediate chunks are\nmapped to parent chunks (2048 tokens) to provide an LLM with sufficiently large\ncontext.\n","authors":["Ashish Singh","Priti Mohapatra"],"pdf_url":"https://arxiv.org/pdf/2503.02401v1.pdf","comment":"14 pages"},{"id":"http://arxiv.org/abs/2503.02398v1","updated":"2025-03-04T08:41:40Z","published":"2025-03-04T08:41:40Z","title":"PersonaX: A Recommendation Agent Oriented User Modeling Framework for\n  Long Behavior Sequence","summary":"  Recommendation agents leverage large language models for user modeling LLM UM\nto construct textual personas guiding alignment with real users. However\nexisting LLM UM methods struggle with long user generated content UGC due to\ncontext limitations and performance degradation. To address this sampling\nstrategies prioritize relevance or recency are often applied yet they\ninevitably neglect the diverse user interests embedded within the discarded\nbehaviors resulting in incomplete modeling and degraded profiling quality.\nFurthermore relevance based sampling requires real time retrieval forcing the\nuser modeling process to operate online which introduces significant latency\noverhead. In this paper we propose PersonaX an agent agnostic LLM UM framework\nthat tackles these challenges through sub behavior sequence SBS selection and\noffline multi persona construction. PersonaX extracts compact SBS segments\noffline to capture diverse user interests generating fine grained textual\npersonas that are cached for efficient online retrieval. This approach ensures\nthat the user persona used for prompting remains highly relevant to the current\ncontext while eliminating the need for online user modeling. For SBS selection\nwe ensure both efficiency length less than five and high representational\nquality by balancing prototypicality and diversity within the sampled data.\nExtensive experiments validate the effectiveness and versatility of PersonaX in\nhigh quality user profiling. Utilizing only 30 to 50 percent of the behavioral\ndata with a sequence length of 480 integrating PersonaX with AgentCF yields an\nabsolute performance improvement of 3 to 11 percent while integration with\nAgent4Rec results in a gain of 10 to 50 percent. PersonaX as an agent agnostic\nframework sets a new benchmark for scalable user modeling paving the way for\nmore accurate and efficient LLM driven recommendation agents.\n","authors":["Yunxiao Shi","Wujiang Xu","Zeqi Zhang","Xing Zi","Qiang Wu","Min Xu"],"pdf_url":"https://arxiv.org/pdf/2503.02398v1.pdf","comment":"draft paper"},{"id":"http://arxiv.org/abs/2404.14851v4","updated":"2025-03-04T08:38:34Z","published":"2024-04-23T09:05:37Z","title":"From Matching to Generation: A Survey on Generative Information\n  Retrieval","summary":"  Information Retrieval (IR) systems are crucial tools for users to access\ninformation, which have long been dominated by traditional methods relying on\nsimilarity matching. With the advancement of pre-trained language models,\ngenerative information retrieval (GenIR) emerges as a novel paradigm,\nattracting increasing attention. Based on the form of information provided to\nusers, current research in GenIR can be categorized into two aspects:\n\\textbf{(1) Generative Document Retrieval} (GR) leverages the generative\nmodel's parameters for memorizing documents, enabling retrieval by directly\ngenerating relevant document identifiers without explicit indexing. \\textbf{(2)\nReliable Response Generation} employs language models to directly generate\ninformation users seek, breaking the limitations of traditional IR in terms of\ndocument granularity and relevance matching while offering flexibility,\nefficiency, and creativity to meet practical needs. This paper aims to\nsystematically review the latest research progress in GenIR. We will summarize\nthe advancements in GR regarding model training and structure, document\nidentifier, incremental learning, etc., as well as progress in reliable\nresponse generation in aspects of internal knowledge memorization, external\nknowledge augmentation, etc. We also review the evaluation, challenges and\nfuture developments in GenIR systems. This review aims to offer a comprehensive\nreference for researchers, encouraging further development in the GenIR field.\nGithub Repository: https://github.com/RUC-NLPIR/GenIR-Survey\n","authors":["Xiaoxi Li","Jiajie Jin","Yujia Zhou","Yuyao Zhang","Peitian Zhang","Yutao Zhu","Zhicheng Dou"],"pdf_url":"https://arxiv.org/pdf/2404.14851v4.pdf","comment":null},{"id":"http://arxiv.org/abs/2402.11480v5","updated":"2025-03-04T07:56:04Z","published":"2024-02-18T07:06:17Z","title":"Pattern-wise Transparent Sequential Recommendation","summary":"  A transparent decision-making process is essential for developing reliable\nand trustworthy recommender systems. For sequential recommendation, it means\nthat the model can identify key items that account for its recommendation\nresults. However, achieving both interpretability and recommendation\nperformance simultaneously is challenging, especially for models that take the\nentire sequence of items as input without screening. In this paper, we propose\nan interpretable framework (named PTSR) that enables a pattern-wise transparent\ndecision-making process without extra features. It breaks the sequence of items\ninto multi-level patterns that serve as atomic units throughout the\nrecommendation process. The contribution of each pattern to the outcome is\nquantified in the probability space. With a carefully designed score correction\nmechanism, the pattern contribution can be implicitly learned in the absence of\nground-truth key patterns. The final recommended items are those that most key\npatterns strongly endorse. Extensive experiments on five public datasets\ndemonstrate remarkable recommendation performance, while statistical analysis\nand case studies validate the model interpretability.\n","authors":["Kun Ma","Cong Xu","Zeyuan Chen","Wei Zhang"],"pdf_url":"https://arxiv.org/pdf/2402.11480v5.pdf","comment":"This paper has been accepted by IEEE TKDE"},{"id":"http://arxiv.org/abs/2501.05874v2","updated":"2025-03-04T07:29:52Z","published":"2025-01-10T11:17:15Z","title":"VideoRAG: Retrieval-Augmented Generation over Video Corpus","summary":"  Retrieval-Augmented Generation (RAG) is a powerful strategy for improving the\nfactual accuracy of models by retrieving external knowledge relevant to queries\nand incorporating it into the generation process. However, existing approaches\nprimarily focus on text, with some recent advancements considering images, and\nthey largely overlook videos, a rich source of multimodal knowledge capable of\nrepresenting contextual details more effectively than any other modality. While\nvery recent studies explore the use of videos in response generation, they\neither predefine query-associated videos without retrieval or convert videos\ninto textual descriptions losing multimodal richness. To tackle these, we\nintroduce VideoRAG, a framework that not only dynamically retrieves videos\nbased on their relevance with queries but also utilizes both visual and textual\ninformation. The operation of VideoRAG is powered by recent Large Video\nLanguage Models (LVLMs), which enable the direct processing of video content to\nrepresent it for retrieval and the seamless integration of retrieved videos\njointly with queries for response generation. Also, inspired by that the\ncontext size of LVLMs may not be sufficient to process all frames in extremely\nlong videos and not all frames are equally important, we introduce a video\nframe selection mechanism to extract the most informative subset of frames,\nalong with a strategy to extract textual information from videos (as it can aid\nthe understanding of video content) when their subtitles are not available. We\nexperimentally validate the effectiveness of VideoRAG, showcasing that it is\nsuperior to relevant baselines. Code is available at\nhttps://github.com/starsuzi/VideoRAG.\n","authors":["Soyeong Jeong","Kangsan Kim","Jinheon Baek","Sung Ju Hwang"],"pdf_url":"https://arxiv.org/pdf/2501.05874v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2411.17102v2","updated":"2025-03-04T07:04:59Z","published":"2024-11-26T04:39:46Z","title":"Scholar Name Disambiguation with Search-enhanced LLM Across Language","summary":"  The task of scholar name disambiguation is crucial in various real-world\nscenarios, including bibliometric-based candidate evaluation for awards,\napplication material anti-fraud measures, and more. Despite significant\nadvancements, current methods face limitations due to the complexity of\nheterogeneous data, often necessitating extensive human intervention. This\npaper proposes a novel approach by leveraging search-enhanced language models\nacross multiple languages to improve name disambiguation. By utilizing the\npowerful query rewriting, intent recognition, and data indexing capabilities of\nsearch engines, our method can gather richer information for distinguishing\nbetween entities and extracting profiles, resulting in a more comprehensive\ndata dimension. Given the strong cross-language capabilities of large language\nmodels(LLMs), optimizing enhanced retrieval methods with this technology offers\nsubstantial potential for high-efficiency information retrieval and\nutilization. Our experiments demonstrate that incorporating local languages\nsignificantly enhances disambiguation performance, particularly for scholars\nfrom diverse geographic regions. This multi-lingual, search-enhanced\nmethodology offers a promising direction for more efficient and accurate active\nscholar name disambiguation.\n","authors":["Renyu Zhao","Yunxin Chen"],"pdf_url":"https://arxiv.org/pdf/2411.17102v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01469v2","updated":"2025-03-04T06:37:59Z","published":"2025-03-03T12:23:54Z","title":"Hierarchical Causal Transformer with Heterogeneous Information for\n  Expandable Sequential Recommendation","summary":"  Sequential recommendation systems leveraging transformer architectures have\ndemonstrated exceptional capabilities in capturing user behavior patterns. At\nthe core of these systems lies the critical challenge of constructing effective\nitem representations. Traditional approaches employ feature fusion through\nsimple concatenation or basic neural architectures to create uniform\nrepresentation sequences. However, these conventional methods fail to address\nthe intrinsic diversity of item attributes, thereby constraining the\ntransformer's capacity to discern fine-grained patterns and hindering model\nextensibility. Although recent research has begun incorporating user-related\nheterogeneous features into item sequences, the equally crucial item-side\nheterogeneous feature continue to be neglected. To bridge this methodological\ngap, we present HeterRec - an innovative framework featuring two novel\ncomponents: the Heterogeneous Token Flattening Layer (HTFL) and Hierarchical\nCausal Transformer (HCT). HTFL pioneers a sophisticated tokenization mechanism\nthat decomposes items into multi-dimensional token sets and structures them\ninto heterogeneous sequences, enabling scalable performance enhancement through\nmodel expansion. The HCT architecture further enhances pattern discovery\nthrough token-level and item-level attention mechanisms. furthermore, we\ndevelop a Listwise Multi-step Prediction (LMP) objective function to optimize\nlearning process. Rigorous validation, including real-world industrial\nplatforms, confirms HeterRec's state-of-the-art performance in both effective\nand efficiency.\n","authors":["Hao Deng","Haibo Xing","Kanefumi Matsuyama","Yulei Huang","Jinxin Hu","Hong Wen","Jia Xu","Zulong Chen","Yu Zhang","Xiaoyi Zeng","Jing Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.01469v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.11610v2","updated":"2025-03-04T06:28:50Z","published":"2025-02-17T09:54:46Z","title":"Accuracy Assessment of OpenAlex and Clarivate Scholar ID with an\n  LLM-Assisted Benchmark","summary":"  In quantitative SciSci (science of science) studies, accurately identifying\nindividual scholars is paramount for scientific data analysis. However, the\nvariability in how names are represented-due to commonality, abbreviations, and\ndifferent spelling conventions-complicates this task. While identifier systems\nlike ORCID are being developed, many scholars remain unregistered, and numerous\npublications are not included. Scholarly databases such as Clarivate and\nOpenAlex have introduced their own ID systems as preliminary name\ndisambiguation solutions. This study evaluates the effectiveness of these\nsystems across different groups to determine their suitability for various\napplication scenarios. We sampled authors from the top quartile (Q1) of Web of\nScience (WOS) journals based on country, discipline, and number of\ncorresponding author papers. For each group, we selected 100 scholars and\nmeticulously annotated all their papers using a Search-enhanced Large Language\nModel method. Using these annotations, we identified the corresponding IDs in\nOpenAlex and Clarivate, extracted all associated papers, filtered for Q1 WOS\njournals, and calculated precision and recall by comparing against the\nannotated dataset.\n","authors":["Renyu Zhao","Yunxin Chen"],"pdf_url":"https://arxiv.org/pdf/2502.11610v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02298v1","updated":"2025-03-04T05:48:07Z","published":"2025-03-04T05:48:07Z","title":"Towards Explainable Doctor Recommendation with Large Language Models","summary":"  The advent of internet medicine provides patients with unprecedented\nconvenience in searching and communicating with doctors relevant to their\ndiseases and desired treatments online. However, the current doctor\nrecommendation systems fail to fully ensure the professionalism and\ninterpretability of the recommended results. In this work, we formulate doctor\nrecommendation as a ranking task and develop a large language model (LLM)-based\npointwise ranking framework. Our framework ranks doctors according to their\nrelevance regarding specific diseases-treatment pairs in a zero-shot setting.\nThe advantage of our framework lies in its ability to generate precise and\nexplainable doctor ranking results. Additionally, we construct DrRank, a new\nexpertise-driven doctor ranking dataset comprising over 38 disease-treatment\npairs. Experiment results on the DrRank dataset demonstrate that our framework\nsignificantly outperforms the strongest cross-encoder baseline, achieving a\nnotable gain of +5.45 in the NDCG@10 score while maintaining affordable latency\nconsumption. Furthermore, we comprehensively present the fairness analysis\nresults of our framework from three perspectives of different diseases, patient\ngender, and geographical regions. Meanwhile, the interpretability of our\nframework is rigorously verified by three human experts, providing further\nevidence of the reliability of our proposed framework for doctor\nrecommendation.\n","authors":["Ziyang Zeng","Dongyuan Li","Yuqing Yang"],"pdf_url":"https://arxiv.org/pdf/2503.02298v1.pdf","comment":"12 pages, 6 figures, Journal of Biomedical and Health Informatics\n  (JBHI) under review"},{"id":"http://arxiv.org/abs/2501.18210v2","updated":"2025-03-04T04:07:39Z","published":"2025-01-30T08:55:32Z","title":"Hashtag Re-Appropriation for Audience Control on Recommendation-Driven\n  Social Media Xiaohongshu (rednote)","summary":"  Algorithms have played a central role in personalized recommendations on\nsocial media. However, they also present significant obstacles for content\ncreators trying to predict and manage their audience reach. This issue is\nparticularly challenging for marginalized groups seeking to maintain safe\nspaces. Our study explores how women on Xiaohongshu (rednote), a\nrecommendation-driven social platform, proactively re-appropriate hashtags\n(e.g., #Baby Supplemental Food) by using them in posts unrelated to their\nliteral meaning. The hashtags were strategically chosen from topics that would\nbe uninteresting to the male audience they wanted to block. Through a\nmixed-methods approach, we analyzed the practice of hashtag re-appropriation\nbased on 5,800 collected posts and interviewed 24 active users from diverse\nbackgrounds to uncover users' motivations and reactions towards the\nre-appropriation. This practice highlights how users can reclaim agency over\ncontent distribution on recommendation-driven platforms, offering insights into\nself-governance within algorithmic-centered power structures.\n","authors":["Ruyuan Wan","Lingbo Tong","Tiffany Knearem","Toby Jia-Jun Li","Ting-Hao 'Kenneth' Huang","Qunfang Wu"],"pdf_url":"https://arxiv.org/pdf/2501.18210v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02251v1","updated":"2025-03-04T03:57:10Z","published":"2025-03-04T03:57:10Z","title":"Tailoring Table Retrieval from a Field-aware Hybrid Matching Perspective","summary":"  Table retrieval, essential for accessing information through tabular data, is\nless explored compared to text retrieval. The row/column structure and distinct\nfields of tables (including titles, headers, and cells) present unique\nchallenges. For example, different table fields have varying matching\npreferences: cells may favor finer-grained (word/phrase level) matching over\nbroader (sentence/passage level) matching due to their fragmented and detailed\nnature, unlike titles. This necessitates a table-specific retriever to\naccommodate the various matching needs of each table field. Therefore, we\nintroduce a Table-tailored HYbrid Matching rEtriever (THYME), which approaches\ntable retrieval from a field-aware hybrid matching perspective. Empirical\nresults on two table retrieval benchmarks, NQ-TABLES and OTT-QA, show that\nTHYME significantly outperforms state-of-the-art baselines. Comprehensive\nanalyses confirm the differing matching preferences across table fields and\nvalidate the design of THYME.\n","authors":["Da Li","Keping Bi","Jiafeng Guo","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2503.02251v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.15331v2","updated":"2025-03-04T02:18:36Z","published":"2025-02-21T09:34:31Z","title":"Lightweight yet Efficient: An External Attentive Graph Convolutional\n  Network with Positional Prompts for Sequential Recommendation","summary":"  Graph-based Sequential Recommender systems (GSRs) have gained significant\nresearch attention due to their ability to simultaneously handle user-item\ninteractions and sequential relationships between items. Current GSRs often\nutilize composite or in-depth structures for graph encoding (e.g., the Graph\nTransformer). Nevertheless, they have high computational complexity, hindering\nthe deployment on resource-constrained edge devices. Moreover, the relative\nposition encoding in Graph Transformer has difficulty in considering the\ncomplicated positional dependencies within sequence. To this end, we propose an\nExternal Attentive Graph convolutional network with Positional prompts for\nSequential recommendation, namely EA-GPS. Specifically, we first introduce an\nexternal attentive graph convolutional network that linearly measures the\nglobal associations among nodes via two external memory units. Then, we present\na positional prompt-based decoder that explicitly treats the absolute item\npositions as external prompts. By introducing length-adaptive sequential\nmasking and a soft attention network, such a decoder facilitates the model to\ncapture the long-term positional dependencies and contextual relationships\nwithin sequences. Extensive experimental results on five real-world datasets\ndemonstrate that the proposed EA-GPS outperforms the state-of-the-art methods.\nRemarkably, it achieves the superior performance while maintaining a smaller\nparameter size and lower training overhead. The implementation of this work is\npublicly available at https://github.com/ZZY-GraphMiningLab/EA-GPS.\n","authors":["Jinyu Zhang","Chao Li","Zhongying Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.15331v2.pdf","comment":"26 pages, 8 figures, journal paper, accepted by TOIS at 20th\n  February, 2025"},{"id":"http://arxiv.org/abs/2410.11841v2","updated":"2025-03-04T01:02:11Z","published":"2024-10-15T17:59:30Z","title":"GaVaMoE: Gaussian-Variational Gated Mixture of Experts for Explainable\n  Recommendation","summary":"  Large language model-based explainable recommendation (LLM-based ER) systems\nshow promise in generating human-like explanations for recommendations.\nHowever, they face challenges in modeling user-item collaborative preferences,\npersonalizing explanations, and handling sparse user-item interactions. To\naddress these issues, we propose GaVaMoE, a novel Gaussian-Variational Gated\nMixture of Experts framework for explainable recommendation. GaVaMoE introduces\ntwo key components: (1) a rating reconstruction module that employs Variational\nAutoencoder (VAE) with a Gaussian Mixture Model (GMM) to capture complex\nuser-item collaborative preferences, serving as a pre-trained multi-gating\nmechanism; and (2) a set of fine-grained expert models coupled with the\nmulti-gating mechanism for generating highly personalized explanations. The VAE\ncomponent models latent factors in user-item interactions, while the GMM\nclusters users with similar behaviors. Each cluster corresponds to a gate in\nthe multi-gating mechanism, routing user-item pairs to appropriate expert\nmodels. This architecture enables GaVaMoE to generate tailored explanations for\nspecific user types and preferences, mitigating data sparsity by leveraging\nuser similarities. Extensive experiments on three real-world datasets\ndemonstrate that GaVaMoE significantly outperforms existing methods in\nexplanation quality, personalization, and consistency. Notably, GaVaMoE\nexhibits robust performance in scenarios with sparse user-item interactions,\nmaintaining high-quality explanations even for users with limited historical\ndata.\n","authors":["Fei Tang","Yongliang Shen","Hang Zhang","Zeqi Tan","Wenqi Zhang","Zhibiao Huang","Kaitao Song","Weiming Lu","Yueting Zhuang"],"pdf_url":"https://arxiv.org/pdf/2410.11841v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2412.01007v3","updated":"2025-03-04T00:36:44Z","published":"2024-12-01T23:54:12Z","title":"CoRNStack: High-Quality Contrastive Data for Better Code Retrieval and\n  Reranking","summary":"  Effective code retrieval plays a crucial role in advancing code generation,\nbug fixing, and software maintenance, particularly as software systems increase\nin complexity. While current code embedding models have demonstrated promise in\nretrieving code snippets for small-scale, well-defined tasks, they often\nunderperform in more demanding real-world applications such as bug localization\nwithin GitHub repositories. We hypothesize that a key issue is their reliance\non noisy and inconsistent datasets for training, which impedes their ability to\ngeneralize to more complex retrieval scenarios. To address these limitations,\nwe introduce CoRNStack, a large-scale, high-quality contrastive training\ndataset for code that spans multiple programming languages. This dataset is\ncurated using consistency filtering to eliminate noisy positives and is further\nenriched with mined hard negatives, thereby facilitating more effective\nlearning. We demonstrate that contrastive training of embedding models using\nCoRNStack leads to state-of-the-art performance across a variety of code\nretrieval tasks. Furthermore, the dataset can be leveraged for training code\nreranking models, a largely underexplored area compared to text reranking. Our\nfinetuned code reranking model significantly improves the ranking quality over\nthe retrieved results. Finally, by employing our code retriever and reranker\ntogether, we demonstrate significant improvements in function localization for\nGitHub issues, an important component of real-world software development.\n","authors":["Tarun Suresh","Revanth Gangi Reddy","Yifei Xu","Zach Nussbaum","Andriy Mulyar","Brandon Duderstadt","Heng Ji"],"pdf_url":"https://arxiv.org/pdf/2412.01007v3.pdf","comment":"Published as a conference paper at ICLR 2025. First and second author\n  had equal contribution"}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.02823v1","updated":"2025-03-04T17:48:48Z","published":"2025-03-04T17:48:48Z","title":"A Multimodal Symphony: Integrating Taste and Sound through Generative AI","summary":"  In recent decades, neuroscientific and psychological research has traced\ndirect relationships between taste and auditory perceptions. This article\nexplores multimodal generative models capable of converting taste information\ninto music, building on this foundational research. We provide a brief review\nof the state of the art in this field, highlighting key findings and\nmethodologies. We present an experiment in which a fine-tuned version of a\ngenerative music model (MusicGEN) is used to generate music based on detailed\ntaste descriptions provided for each musical piece. The results are promising:\naccording the participants' ($n=111$) evaluation, the fine-tuned model produces\nmusic that more coherently reflects the input taste descriptions compared to\nthe non-fine-tuned model. This study represents a significant step towards\nunderstanding and developing embodied interactions between AI, sound, and\ntaste, opening new possibilities in the field of generative AI. We release our\ndataset, code and pre-trained model at: https://osf.io/xs5jy/.\n","authors":["Matteo Spanio","Massimiliano Zampini","Antonio Rodà","Franco Pierucci"],"pdf_url":"https://arxiv.org/pdf/2503.02823v1.pdf","comment":"17 pages, 6 figures (2 + 2 figures with 2 subfigures each)"},{"id":"http://arxiv.org/abs/2502.18495v2","updated":"2025-03-04T15:16:52Z","published":"2025-02-19T01:37:24Z","title":"A Comprehensive Survey on Composed Image Retrieval","summary":"  Composed Image Retrieval (CIR) is an emerging yet challenging task that\nallows users to search for target images using a multimodal query, comprising a\nreference image and a modification text specifying the user's desired changes\nto the reference image. Given its significant academic and practical value, CIR\nhas become a rapidly growing area of interest in the computer vision and\nmachine learning communities, particularly with the advances in deep learning.\nTo the best of our knowledge, there is currently no comprehensive review of CIR\nto provide a timely overview of this field. Therefore, we synthesize insights\nfrom over 120 publications in top conferences and journals, including ACM TOIS,\nSIGIR, and CVPR In particular, we systematically categorize existing supervised\nCIR and zero-shot CIR models using a fine-grained taxonomy. For a comprehensive\nreview, we also briefly discuss approaches for tasks closely related to CIR,\nsuch as attribute-based CIR and dialog-based CIR. Additionally, we summarize\nbenchmark datasets for evaluation and analyze existing supervised and zero-shot\nCIR methods by comparing experimental results across multiple datasets.\nFurthermore, we present promising future directions in this field, offering\npractical insights for researchers interested in further exploration. The\ncurated collection of related works is maintained and continuously updated in\nhttps://github.com/haokunwen/Awesome-Composed-Image-Retrieval.\n","authors":["Xuemeng Song","Haoqiang Lin","Haokun Wen","Bohan Hou","Mingzhu Xu","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2502.18495v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.02452v1","updated":"2025-03-04T09:57:24Z","published":"2025-03-04T09:57:24Z","title":"2DGS-Avatar: Animatable High-fidelity Clothed Avatar via 2D Gaussian\n  Splatting","summary":"  Real-time rendering of high-fidelity and animatable avatars from monocular\nvideos remains a challenging problem in computer vision and graphics. Over the\npast few years, the Neural Radiance Field (NeRF) has made significant progress\nin rendering quality but behaves poorly in run-time performance due to the low\nefficiency of volumetric rendering. Recently, methods based on 3D Gaussian\nSplatting (3DGS) have shown great potential in fast training and real-time\nrendering. However, they still suffer from artifacts caused by inaccurate\ngeometry. To address these problems, we propose 2DGS-Avatar, a novel approach\nbased on 2D Gaussian Splatting (2DGS) for modeling animatable clothed avatars\nwith high-fidelity and fast training performance. Given monocular RGB videos as\ninput, our method generates an avatar that can be driven by poses and rendered\nin real-time. Compared to 3DGS-based methods, our 2DGS-Avatar retains the\nadvantages of fast training and rendering while also capturing detailed,\ndynamic, and photo-realistic appearances. We conduct abundant experiments on\npopular datasets such as AvatarRex and THuman4.0, demonstrating impressive\nperformance in both qualitative and quantitative metrics.\n","authors":["Qipeng Yan","Mingyang Sun","Lihua Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.02452v1.pdf","comment":"ICVRV 2024"},{"id":"http://arxiv.org/abs/2503.02318v1","updated":"2025-03-04T06:18:34Z","published":"2025-03-04T06:18:34Z","title":"Audio-Reasoner: Improving Reasoning Capability in Large Audio Language\n  Models","summary":"  Recent advancements in multimodal reasoning have largely overlooked the audio\nmodality. We introduce Audio-Reasoner, a large-scale audio language model for\ndeep reasoning in audio tasks. We meticulously curated a large-scale and\ndiverse multi-task audio dataset with simple annotations. Then, we leverage\nclosed-source models to conduct secondary labeling, QA generation, along with\nstructured COT process. These datasets together form a high-quality reasoning\ndataset with 1.2 million reasoning-rich samples, which we name CoTA. Following\ninference scaling principles, we train Audio-Reasoner on CoTA, enabling it to\nachieve great logical capabilities in audio reasoning. Experiments show\nstate-of-the-art performance across key benchmarks, including MMAU-mini\n(+25.42%), AIR-Bench chat/foundation(+14.57%/+10.13%), and MELD (+8.01%). Our\nfindings stress the core of structured CoT training in advancing audio\nreasoning.\n","authors":["Zhifei Xie","Mingbao Lin","Zihang Liu","Pengcheng Wu","Shuicheng Yan","Chunyan Miao"],"pdf_url":"https://arxiv.org/pdf/2503.02318v1.pdf","comment":"Technical report, in process"},{"id":"http://arxiv.org/abs/2310.07236v4","updated":"2025-03-04T03:47:04Z","published":"2023-10-11T06:56:08Z","title":"AdaMesh: Personalized Facial Expressions and Head Poses for Adaptive\n  Speech-Driven 3D Facial Animation","summary":"  Speech-driven 3D facial animation aims at generating facial movements that\nare synchronized with the driving speech, which has been widely explored\nrecently. Existing works mostly neglect the person-specific talking style in\ngeneration, including facial expression and head pose styles. Several works\nintend to capture the personalities by fine-tuning modules. However, limited\ntraining data leads to the lack of vividness. In this work, we propose AdaMesh,\na novel adaptive speech-driven facial animation approach, which learns the\npersonalized talking style from a reference video of about 10 seconds and\ngenerates vivid facial expressions and head poses. Specifically, we propose\nmixture-of-low-rank adaptation (MoLoRA) to fine-tune the expression adapter,\nwhich efficiently captures the facial expression style. For the personalized\npose style, we propose a pose adapter by building a discrete pose prior and\nretrieving the appropriate style embedding with a semantic-aware pose style\nmatrix without fine-tuning. Extensive experimental results show that our\napproach outperforms state-of-the-art methods, preserves the talking style in\nthe reference video, and generates vivid facial animation. The supplementary\nvideo and code will be available at https://adamesh.github.io.\n","authors":["Liyang Chen","Weihong Bao","Shun Lei","Boshi Tang","Zhiyong Wu","Shiyin Kang","Haozhi Huang","Helen Meng"],"pdf_url":"https://arxiv.org/pdf/2310.07236v4.pdf","comment":"Accepted by IEEE Transactions on Multimedia"},{"id":"http://arxiv.org/abs/2503.02199v1","updated":"2025-03-04T02:21:07Z","published":"2025-03-04T02:21:07Z","title":"Words or Vision: Do Vision-Language Models Have Blind Faith in Text?","summary":"  Vision-Language Models (VLMs) excel in integrating visual and textual\ninformation for vision-centric tasks, but their handling of inconsistencies\nbetween modalities is underexplored. We investigate VLMs' modality preferences\nwhen faced with visual data and varied textual inputs in vision-centered\nsettings. By introducing textual variations to four vision-centric tasks and\nevaluating ten Vision-Language Models (VLMs), we discover a \\emph{``blind faith\nin text''} phenomenon: VLMs disproportionately trust textual data over visual\ndata when inconsistencies arise, leading to significant performance drops under\ncorrupted text and raising safety concerns. We analyze factors influencing this\ntext bias, including instruction prompts, language model size, text relevance,\ntoken order, and the interplay between visual and textual certainty. While\ncertain factors, such as scaling up the language model size, slightly mitigate\ntext bias, others like token order can exacerbate it due to positional biases\ninherited from language models. To address this issue, we explore supervised\nfine-tuning with text augmentation and demonstrate its effectiveness in\nreducing text bias. Additionally, we provide a theoretical analysis suggesting\nthat the blind faith in text phenomenon may stem from an imbalance of pure text\nand multi-modal data during training. Our findings highlight the need for\nbalanced training and careful consideration of modality interactions in VLMs to\nenhance their robustness and reliability in handling multi-modal data\ninconsistencies.\n","authors":["Ailin Deng","Tri Cao","Zhirui Chen","Bryan Hooi"],"pdf_url":"https://arxiv.org/pdf/2503.02199v1.pdf","comment":"Accepted to CVPR 2025"},{"id":"http://arxiv.org/abs/2412.17049v2","updated":"2025-03-04T02:14:35Z","published":"2024-12-22T15:00:16Z","title":"Modular Conversational Agents for Surveys and Interviews","summary":"  Surveys and interviews are widely used for collecting insights on emerging or\nhypothetical scenarios. Traditional human-led methods often face challenges\nrelated to cost, scalability, and consistency. Recently, various domains have\nbegun to explore the use of conversational agents (chatbots) powered by\ngenerative artificial intelligence (AI) technologies. However, considering\ndecisions in transportation investments and policies often carry significant\npublic and environmental stakes, surveys and interviews face unique challenges\nin integrating AI agents, underscoring the need for a rigorous,\nresource-efficient approach that enhances participant engagement and ensures\nprivacy. This paper addresses this gap by introducing a modular approach and\nits resulting parameterized process for designing AI agents. We detail the\nsystem architecture, integrating engineered prompts, specialized knowledge\nbases, and customizable, goal-oriented conversational logic. We demonstrate the\nadaptability, generalizability, and efficacy of our modular approach through\nthree empirical studies: (1) travel preference surveys, highlighting\nconditional logic and multimodal (voice, text, and image generation)\ncapabilities; (2) public opinion elicitation on a newly constructed, novel\ninfrastructure project, showcasing question customization and multilingual\n(English and French) capabilities; and (3) expert consultation about the impact\nof technologies on future transportation systems, highlighting real-time,\nclarification request capabilities for open-ended questions, resilience in\nhandling erratic inputs, and efficient transcript postprocessing. The results\nsuggest that the AI agent increases completion rates and response quality.\nFurthermore, the modular approach demonstrates controllability, flexibility,\nand robustness while addressing key ethical, privacy, security, and token\nconsumption concerns.\n","authors":["Jiangbo Yu","Jinhua Zhao","Luis Miranda-Moreno","Matthew Korp"],"pdf_url":"https://arxiv.org/pdf/2412.17049v2.pdf","comment":null}]},"2025-03-03T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2408.01262v5","updated":"2025-03-03T22:45:57Z","published":"2024-08-02T13:35:11Z","title":"RAGEval: Scenario Specific RAG Evaluation Dataset Generation Framework","summary":"  Retrieval-Augmented Generation (RAG) is a powerful approach that enables\nlarge language models (LLMs) to incorporate external knowledge. However,\nevaluating the effectiveness of RAG systems in specialized scenarios remains\nchallenging due to the high costs of data construction and the lack of suitable\nevaluation metrics. This paper introduces RAGEval, a framework designed to\nassess RAG systems across diverse scenarios by generating high-quality\ndocuments, questions, answers, and references through a schema-based pipeline.\nWith a focus on factual accuracy, we propose three novel metrics: Completeness,\nHallucination, and Irrelevance to evaluate LLM generated responses rigorously.\nExperimental results show that RAGEval outperforms zero-shot and one-shot\nmethods in terms of clarity, safety, conformity, and richness of generated\nsamples. Furthermore, the use of LLMs for scoring the proposed metrics\ndemonstrates a high level of consistency with human evaluations. RAGEval\nestablishes a new paradigm for evaluating RAG systems in real-world\napplications. The code and dataset are released at\nhttps://github.com/OpenBMB/RAGEval.\n","authors":["Kunlun Zhu","Yifan Luo","Dingling Xu","Yukun Yan","Zhenghao Liu","Shi Yu","Ruobing Wang","Shuo Wang","Yishan Li","Nan Zhang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2408.01262v5.pdf","comment":"https://github.com/OpenBMB/RAGEval"},{"id":"http://arxiv.org/abs/2502.17494v4","updated":"2025-03-03T22:21:09Z","published":"2025-02-20T22:35:52Z","title":"External Large Foundation Model: How to Efficiently Serve Trillions of\n  Parameters for Online Ads Recommendation","summary":"  Ads recommendation is a prominent service of online advertising systems and\nhas been actively studied. Recent studies indicate that scaling-up and advanced\ndesign of the recommendation model can bring significant performance\nimprovement. However, with a larger model scale, such prior studies have a\nsignificantly increasing gap from industry as they often neglect two\nfundamental challenges in industrial-scale applications. First, training and\ninference budgets are restricted for the model to be served, exceeding which\nmay incur latency and impair user experience. Second, large-volume data arrive\nin a streaming mode with data distributions dynamically shifting, as new\nusers/ads join and existing users/ads leave the system. We propose the External\nLarge Foundation Model (ExFM) framework to address the overlooked challenges.\nSpecifically, we develop external distillation and a data augmentation system\n(DAS) to control the computational cost of training/inference while maintaining\nhigh performance. We design the teacher in a way like a foundation model (FM)\nthat can serve multiple students as vertical models (VMs) to amortize its\nbuilding cost. We propose Auxiliary Head and Student Adapter to mitigate the\ndata distribution gap between FM and VMs caused by the streaming data issue.\nComprehensive experiments on internal industrial-scale applications and public\ndatasets demonstrate significant performance gain by ExFM.\n","authors":["Mingfu Liang","Xi Liu","Rong Jin","Boyang Liu","Qiuling Suo","Qinghai Zhou","Song Zhou","Laming Chen","Hua Zheng","Zhiyuan Li","Shali Jiang","Jiyan Yang","Xiaozhen Xia","Fan Yang","Yasmine Badr","Ellie Wen","Shuyu Xu","Hansey Chen","Zhengyu Zhang","Jade Nie","Chunzhi Yang","Zhichen Zeng","Weilin Zhang","Xingliang Huang","Qianru Li","Shiquan Wang","Evelyn Lyu","Wenjing Lu","Rui Zhang","Wenjun Wang","Jason Rudy","Mengyue Hang","Kai Wang","Yinbin Ma","Shuaiwen Wang","Sihan Zeng","Tongyi Tang","Xiaohan Wei","Longhao Jin","Jamey Zhang","Marcus Chen","Jiayi Zhang","Angie Huang","Chi Zhang","Zhengli Zhao","Jared Yang","Qiang Jin","Xian Chen","Amit Anand Amlesahwaram","Lexi Song","Liang Luo","Yuchen Hao","Nan Xiao","Yavuz Yetim","Luoshang Pan","Gaoxiang Liu","Yuxi Hu","Yuzhen Huang","Jackie Xu","Rich Zhu","Xin Zhang","Yiqun Liu","Hang Yin","Yuxin Chen","Buyun Zhang","Xiaoyi Liu","Xingyuan Wang","Wenguang Mao","Zhijing Li","Qin Huang","Chonglin Sun","Nancy Yu","Shuo Gu","Shupin Mao","Benjamin Au","Jingzheng Qin","Peggy Yao","Jae-Woo Choi","Bin Gao","Ernest Wang","Lei Zhang","Wen-Yen Chen","Ted Lee","Jay Zha","Yi Meng","Alex Gong","Edison Gao","Alireza Vahdatpour","Yiping Han","Yantao Yao","Toshinari Kureha","Shuo Chang","Musharaf Sultan","John Bocharov","Sagar Chordia","Xiaorui Gan","Peng Sun","Rocky Liu","Bo Long","Wenlin Chen","Santanu Kolay","Huayu Li"],"pdf_url":"https://arxiv.org/pdf/2502.17494v4.pdf","comment":"Accepted by the ACM Web Conference (WWW) 2025 Industrial Track as\n  Oral Presentation"},{"id":"http://arxiv.org/abs/2503.02065v1","updated":"2025-03-03T21:39:15Z","published":"2025-03-03T21:39:15Z","title":"Survey Perspective: The Role of Explainable AI in Threat Intelligence","summary":"  The increasing reliance on AI-based security tools in Security Operations\nCenters (SOCs) has transformed threat detection and response, yet analysts\nfrequently struggle with alert overload, false positives, and lack of\ncontextual relevance. The inability to effectively analyze AI-generated\nsecurity alerts lead to inefficiencies in incident response and reduces trust\nin automated decision-making. In this paper, we show results and analysis of\nour investigation of how SOC analysts navigate AI-based alerts, their\nchallenges with current security tools, and how explainability (XAI) integrated\ninto their security workflows has the potential to become an effective decision\nsupport. In this vein, we conducted an industry survey. Using the survey\nresponses, we analyze how security analysts' process, retrieve, and prioritize\nalerts. Our findings indicate that most analysts have not yet adopted\nXAI-integrated tools, but they express high interest in attack attribution,\nconfidence scores, and feature contribution explanations to improve\ninterpretability, and triage efficiency. Based on our findings, we also propose\npractical design recommendations for XAI-enhanced security alert systems,\nenabling AI-based cybersecurity solutions to be more transparent,\ninterpretable, and actionable.\n","authors":["Nidhi Rastogi","Devang Dhanuka","Amulya Saxena","Pranjal Mairal","Le Nguyen"],"pdf_url":"https://arxiv.org/pdf/2503.02065v1.pdf","comment":"5 pages, SIGIR Symposium on IR in Practice (SIRIP), 2025"},{"id":"http://arxiv.org/abs/2410.19302v2","updated":"2025-03-03T21:07:27Z","published":"2024-10-25T04:26:00Z","title":"TEARS: Textual Representations for Scrutable Recommendations","summary":"  Traditional recommender systems rely on high-dimensional (latent) embeddings\nfor modeling user-item interactions, often resulting in opaque representations\nthat lack interpretability. Moreover, these systems offer limited control to\nusers over their recommendations. Inspired by recent work, we introduce TExtuAl\nRepresentations for Scrutable recommendations (TEARS) to address these\nchallenges. Instead of representing a user's interests through a latent\nembedding, TEARS encodes them in natural text, providing transparency and\nallowing users to edit them. To do so, TEARS uses a modern LLM to generate user\nsummaries based on user preferences. We find the summaries capture user\npreferences uniquely. Using these summaries, we take a hybrid approach where we\nuse an optimal transport procedure to align the summaries' representation with\nthe learned representation of a standard VAE for collaborative filtering. We\nfind this approach can surpass the performance of three popular VAE models\nwhile providing user-controllable recommendations. We also analyze the\ncontrollability of TEARS through three simulated user tasks to evaluate the\neffectiveness of a user editing its summary.\n","authors":["Emiliano Penaloza","Olivier Gouvert","Haolun Wu","Laurent Charlin"],"pdf_url":"https://arxiv.org/pdf/2410.19302v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01814v1","updated":"2025-03-03T18:41:59Z","published":"2025-03-03T18:41:59Z","title":"LLMInit: A Free Lunch from Large Language Models for Selective\n  Initialization of Recommendation","summary":"  Collaborative filtering models, particularly graph-based approaches, have\ndemonstrated strong performance in capturing user-item interactions for\nrecommendation systems. However, they continue to struggle in cold-start and\ndata-sparse scenarios. The emergence of large language models (LLMs) like GPT\nand LLaMA presents new possibilities for enhancing recommendation performance,\nespecially in cold-start settings. Despite their promise, LLMs pose challenges\nrelated to scalability and efficiency due to their high computational demands\nand limited ability to model complex user-item relationships effectively. In\nthis work, we introduce a novel perspective on leveraging LLMs for CF model\ninitialization. Through experiments, we uncover an embedding collapse issue\nwhen scaling CF models to larger embedding dimensions. To effectively harness\nlarge-scale LLM embeddings, we propose innovative selective initialization\nstrategies utilizing random, uniform, and variance-based index sampling. Our\ncomprehensive evaluation on multiple real-world datasets demonstrates\nsignificant performance gains across various CF models while maintaining a\nlower computational cost compared to existing LLM-based recommendation\napproaches.\n","authors":["Weizhi Zhang","Liangwei Yang","Wooseong Yang","Henry Peng Zou","Yuqing Liu","Ke Xu","Sourav Medya","Philip S. Yu"],"pdf_url":"https://arxiv.org/pdf/2503.01814v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01763v1","updated":"2025-03-03T17:37:16Z","published":"2025-03-03T17:37:16Z","title":"Retrieval Models Aren't Tool-Savvy: Benchmarking Tool Retrieval for\n  Large Language Models","summary":"  Tool learning aims to augment large language models (LLMs) with diverse\ntools, enabling them to act as agents for solving practical tasks. Due to the\nlimited context length of tool-using LLMs, adopting information retrieval (IR)\nmodels to select useful tools from large toolsets is a critical initial step.\nHowever, the performance of IR models in tool retrieval tasks remains\nunderexplored and unclear. Most tool-use benchmarks simplify this step by\nmanually pre-annotating a small set of relevant tools for each task, which is\nfar from the real-world scenarios. In this paper, we propose ToolRet, a\nheterogeneous tool retrieval benchmark comprising 7.6k diverse retrieval tasks,\nand a corpus of 43k tools, collected from existing datasets. We benchmark six\ntypes of models on ToolRet. Surprisingly, even the models with strong\nperformance in conventional IR benchmarks, exhibit poor performance on ToolRet.\nThis low retrieval quality degrades the task pass rate of tool-use LLMs. As a\nfurther step, we contribute a large-scale training dataset with over 200k\ninstances, which substantially optimizes the tool retrieval ability of IR\nmodels.\n","authors":["Zhengliang Shi","Yuhan Wang","Lingyong Yan","Pengjie Ren","Shuaiqiang Wang","Dawei Yin","Zhaochun Ren"],"pdf_url":"https://arxiv.org/pdf/2503.01763v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01713v1","updated":"2025-03-03T16:25:58Z","published":"2025-03-03T16:25:58Z","title":"SAGE: A Framework of Precise Retrieval for RAG","summary":"  Retrieval-augmented generation (RAG) has demonstrated significant proficiency\nin conducting question-answering (QA) tasks within a specified corpus.\nNonetheless, numerous failure instances of RAG in QA still exist. These\nfailures are not solely attributable to the limitations of Large Language\nModels (LLMs); instead, they predominantly arise from the retrieval of\ninaccurate information for LLMs due to two limitations: (1) Current RAG methods\nsegment the corpus without considering semantics, making it difficult to find\nrelevant context due to impaired correlation between questions and the\nsegments. (2) There is a trade-off between missing essential context with fewer\ncontext retrieved and getting irrelevant context with more context retrieved.\n  In this paper, we introduce a RAG framework (SAGE), to overcome these\nlimitations. First, to address the segmentation issue without considering\nsemantics, we propose to train a semantic segmentation model. This model is\ntrained to segment the corpus into semantically complete chunks. Second, to\nensure that only the most relevant chunks are retrieved while the irrelevant\nones are ignored, we design a chunk selection algorithm to dynamically select\nchunks based on the decreasing speed of the relevance score, leading to a more\nrelevant selection. Third, to further ensure the precision of the retrieved\nchunks, we propose letting LLMs assess whether retrieved chunks are excessive\nor lacking and then adjust the amount of context accordingly. Experiments show\nthat SAGE outperforms baselines by 61.25% in the quality of QA on average.\nMoreover, by avoiding retrieving noisy context, SAGE lowers the cost of the\ntokens consumed in LLM inference and achieves a 49.41% enhancement in cost\nefficiency on average. Additionally, our work offers valuable insights for\nboosting RAG.\n","authors":["Jintao Zhang","Guoliang Li","Jinyang Su"],"pdf_url":"https://arxiv.org/pdf/2503.01713v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01670v1","updated":"2025-03-03T15:42:57Z","published":"2025-03-03T15:42:57Z","title":"Evaluating LLMs' Assessment of Mixed-Context Hallucination Through the\n  Lens of Summarization","summary":"  With the rapid development of large language models (LLMs), LLM-as-a-judge\nhas emerged as a widely adopted approach for text quality evaluation, including\nhallucination evaluation. While previous studies have focused exclusively on\nsingle-context evaluation (e.g., discourse faithfulness or world factuality),\nreal-world hallucinations typically involve mixed contexts, which remains\ninadequately evaluated. In this study, we use summarization as a representative\ntask to comprehensively evaluate LLMs' capability in detecting mixed-context\nhallucinations, specifically distinguishing between factual and non-factual\nhallucinations. Through extensive experiments across direct generation and\nretrieval-based models of varying scales, our main observations are: (1) LLMs'\nintrinsic knowledge introduces inherent biases in hallucination evaluation; (2)\nThese biases particularly impact the detection of factual hallucinations,\nyielding a significant performance bottleneck; (3) The fundamental challenge\nlies in effective knowledge utilization, balancing between LLMs' intrinsic\nknowledge and external context for accurate mixed-context hallucination\nevaluation.\n","authors":["Siya Qi","Rui Cao","Yulan He","Zheng Yuan"],"pdf_url":"https://arxiv.org/pdf/2503.01670v1.pdf","comment":"8 pages, 5 figures for main body"},{"id":"http://arxiv.org/abs/2503.01658v1","updated":"2025-03-03T15:32:02Z","published":"2025-03-03T15:32:02Z","title":"CoPL: Collaborative Preference Learning for Personalizing LLMs","summary":"  Personalizing large language models (LLMs) is important for aligning outputs\nwith diverse user preferences, yet existing methods struggle with flexibility\nand generalization. We propose CoPL (Collaborative Preference Learning), a\ngraph-based collaborative filtering framework that models user-response\nrelationships to enhance preference estimation, particularly in sparse\nannotation settings. By integrating a mixture of LoRA experts, CoPL efficiently\nfine-tunes LLMs while dynamically balancing shared and user-specific\npreferences. Additionally, an optimization-free adaptation strategy enables\ngeneralization to unseen users without fine-tuning. Experiments on\nUltraFeedback-P demonstrate that CoPL outperforms existing personalized reward\nmodels, effectively capturing both common and controversial preferences, making\nit a scalable solution for personalized LLM alignment.\n","authors":["Youngbin Choi","Seunghyuk Cho","Minjong Lee","MoonJeong Park","Yesong Ko","Jungseul Ok","Dongwoo Kim"],"pdf_url":"https://arxiv.org/pdf/2503.01658v1.pdf","comment":"13pages, 4 figures, 6tables"},{"id":"http://arxiv.org/abs/2502.18858v2","updated":"2025-03-03T13:38:50Z","published":"2025-02-26T05:59:45Z","title":"Evaluating Intelligence via Trial and Error","summary":"  Intelligence is a crucial trait for species to find solutions within a\nlimited number of trial-and-error attempts. Building on this idea, we introduce\nSurvival Game as a framework to evaluate intelligence based on the number of\nfailed attempts in a trial-and-error process. Fewer failures indicate higher\nintelligence. When the expectation and variance of failure counts are both\nfinite, it signals the ability to consistently find solutions to new\nchallenges, which we define as the Autonomous Level of intelligence. Using\nSurvival Game, we comprehensively evaluate existing AI systems. Our results\nshow that while AI systems achieve the Autonomous Level in simple tasks, they\nare still far from it in more complex tasks, such as vision, search,\nrecommendation, and language. While scaling current AI technologies might help,\nthis would come at an astronomical cost. Projections suggest that achieving the\nAutonomous Level for general tasks would require $10^{26}$ parameters. To put\nthis into perspective, loading such a massive model requires so many H100 GPUs\nthat their total value is $10^{7}$ times that of Apple Inc.'s market value.\nEven with Moore's Law, supporting such a parameter scale would take $70$ years.\nThis staggering cost highlights the complexity of human tasks and the\ninadequacies of current AI technologies. To further investigate this\nphenomenon, we conduct a theoretical analysis of Survival Game and its\nexperimental results. Our findings suggest that human tasks possess a\ncriticality property. As a result, Autonomous Level requires a deep\nunderstanding of the task's underlying mechanisms. Current AI systems, however,\ndo not fully grasp these mechanisms and instead rely on superficial mimicry,\nmaking it difficult for them to reach an autonomous level. We believe Survival\nGame can not only guide the future development of AI but also offer profound\ninsights into human intelligence.\n","authors":["Jingtao Zhan","Jiahao Zhao","Jiayu Li","Yiqun Liu","Bo Zhang","Qingyao Ai","Jiaxin Mao","Hongning Wang","Min Zhang","Shaoping Ma"],"pdf_url":"https://arxiv.org/pdf/2502.18858v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.07596v2","updated":"2025-03-03T13:27:01Z","published":"2025-01-10T01:42:43Z","title":"Optimize Incompatible Parameters through Compatibility-aware Knowledge\n  Integration","summary":"  Deep neural networks have become foundational to advancements in multiple\ndomains, including recommendation systems, natural language processing, and so\non. Despite their successes, these models often contain incompatible parameters\nthat can be underutilized or detrimental to model performance, particularly\nwhen faced with specific, varying data distributions. Existing research excels\nin removing such parameters or merging the outputs of multiple different\npretrained models. However, the former focuses on efficiency rather than\nperformance, while the latter requires several times more computing and storage\nresources to support inference. In this paper, we set the goal to explicitly\nimprove these incompatible parameters by leveraging the complementary strengths\nof different models, thereby directly enhancing the models without any\nadditional parameters. Specifically, we propose Compatibility-aware Knowledge\nIntegration (CKI), which consists of Parameter Compatibility Assessment and\nParameter Splicing, which are used to evaluate the knowledge content of\nmultiple models and integrate the knowledge into one model, respectively. The\nintegrated model can be used directly for inference or for further fine-tuning.\nWe conduct extensive experiments on various datasets for recommendation and\nlanguage tasks, and the results show that Compatibility-aware Knowledge\nIntegration can effectively optimize incompatible parameters under multiple\ntasks and settings to break through the training limit of the original model\nwithout increasing the inference cost.\n","authors":["Zheqi Lv","Keming Ye","Zishu Wei","Qi Tian","Shengyu Zhang","Wenqiao Zhang","Wenjie Wang","Kun Kuang","Tat-Seng Chua","Fei Wu"],"pdf_url":"https://arxiv.org/pdf/2501.07596v2.pdf","comment":"Published on AAAI'25(Oral): The Annual AAAI Conference on Artificial\n  Intelligence"},{"id":"http://arxiv.org/abs/2503.01442v1","updated":"2025-03-03T11:48:01Z","published":"2025-03-03T11:48:01Z","title":"Leveraging LLMs for Mental Health: Detection and Recommendations from\n  Social Discussions","summary":"  Textual data from social platforms captures various aspects of mental health\nthrough discussions around and across issues, while users reach out for help\nand others sympathize and offer support. We propose a comprehensive framework\nthat leverages Natural Language Processing (NLP) and Generative AI techniques\nto identify and assess mental health disorders, detect their severity, and\ncreate recommendations for behavior change and therapeutic interventions based\non users' posts on Reddit.\n  To classify the disorders, we use rule-based labeling methods as well as\nadvanced pre-trained NLP models to extract nuanced semantic features from the\ndata. We fine-tune domain-adapted and generic pre-trained NLP models based on\npredictions from specialized Large Language Models (LLMs) to improve\nclassification accuracy. Our hybrid approach combines the generalization\ncapabilities of pre-trained models with the domain-specific insights captured\nby LLMs, providing an improved understanding of mental health discourse. Our\nfindings highlight the strengths and limitations of each model, offering\nvaluable insights into their practical applicability.\n  This research potentially facilitates early detection and personalized care\nto aid practitioners and aims to facilitate timely interventions and improve\noverall well-being, thereby contributing to the broader field of mental health\nsurveillance and digital health analytics.\n","authors":["Vaishali Aggarwal","Sachin Thukral","Krushil Patel","Arnab Chatterjee"],"pdf_url":"https://arxiv.org/pdf/2503.01442v1.pdf","comment":"5 pages, 4 figures, 3 tables, to be published in WI-IAT 2024"},{"id":"http://arxiv.org/abs/2503.01362v1","updated":"2025-03-03T09:55:54Z","published":"2025-03-03T09:55:54Z","title":"Streaming Piano Transcription Based on Consistent Onset and Offset\n  Decoding with Sustain Pedal Detection","summary":"  This paper describes a streaming audio-to-MIDI piano transcription approach\nthat aims to sequentially translate a music signal into a sequence of note\nonset and offset events. The sequence-to-sequence nature of this task may call\nfor the computationally-intensive transformer model for better performance,\nwhich has recently been used for offline transcription benchmarks and could be\nextended for streaming transcription with causal attention mechanisms. We\nassume that the performance limitation of this naive approach lies in the\ndecoder. Although time-frequency features useful for onset detection are\nconsiderably different from those for offset detection, the single decoder is\ntrained to output a mixed sequence of onset and offset events without guarantee\nof the correspondence between the onset and offset events of the same note. To\novercome this limitation, we propose a streaming encoder-decoder model that\nuses a convolutional encoder aggregating local acoustic features, followed by\nan autoregressive Transformer decoder detecting a variable number of onset\nevents and another decoder detecting the offset events for the active pitches\nwith validation of the sustain pedal at each time frame. Experiments using the\nMAESTRO dataset showed that the proposed streaming method performed comparably\nwith or even better than the state-of-the-art offline methods while\nsignificantly reducing the computational cost.\n","authors":["Weixing Wei","Jiahao Zhao","Yulun Wu","Kazuyoshi Yoshii"],"pdf_url":"https://arxiv.org/pdf/2503.01362v1.pdf","comment":"Accepted to ISMIR 2024"},{"id":"http://arxiv.org/abs/2503.01334v1","updated":"2025-03-03T09:18:43Z","published":"2025-03-03T09:18:43Z","title":"Composed Multi-modal Retrieval: A Survey of Approaches and Applications","summary":"  With the rapid growth of multi-modal data from social media, short video\nplatforms, and e-commerce, content-based retrieval has become essential for\nefficiently searching and utilizing heterogeneous information. Over time,\nretrieval techniques have evolved from Unimodal Retrieval (UR) to Cross-modal\nRetrieval (CR) and, more recently, to Composed Multi-modal Retrieval (CMR). CMR\nenables users to retrieve images or videos by integrating a reference visual\ninput with textual modifications, enhancing search flexibility and precision.\nThis paper provides a comprehensive review of CMR, covering its fundamental\nchallenges, technical advancements, and categorization into supervised,\nzero-shot, and semi-supervised learning paradigms. We discuss key research\ndirections, including data augmentation, model architecture, and loss\noptimization in supervised CMR, as well as transformation frameworks and\nexternal knowledge integration in zero-shot CMR. Additionally, we highlight the\napplication potential of CMR in composed image retrieval, video retrieval, and\nperson retrieval, which have significant implications for e-commerce, online\nsearch, and public security. Given its ability to refine and personalize search\nexperiences, CMR is poised to become a pivotal technology in next-generation\nretrieval systems. A curated list of related works and resources is available\nat: https://github.com/kkzhang95/Awesome-Composed-Multi-modal-Retrieval\n","authors":["Kun Zhang","Jingyu Li","Zhe Li","Jingjing Zhang"],"pdf_url":"https://arxiv.org/pdf/2503.01334v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01305v1","updated":"2025-03-03T08:43:40Z","published":"2025-03-03T08:43:40Z","title":"HI-Series Algorithms A Hybrid of Substance Diffusion Algorithm and\n  Collaborative Filtering","summary":"  Recommendation systems face the challenge of balancing accuracy and\ndiversity, as traditional collaborative filtering (CF) and network-based\ndiffusion algorithms exhibit complementary limitations. While item-based CF\n(ItemCF) enhances diversity through item similarity, it compromises accuracy.\nConversely, mass diffusion (MD) algorithms prioritize accuracy by favoring\npopular items but lack diversity. To address this trade-off, we propose the\nHI-series algorithms, hybrid models integrating ItemCF with diffusion-based\napproaches (MD, HHP, BHC, BD) through a nonlinear combination controlled by\nparameter $\\epsilon$. This hybridization leverages ItemCF's diversity and MD's\naccuracy, extending to advanced diffusion models (HI-HHP, HI-BHC, HI-BD) for\nenhanced performance. Experiments on MovieLens, Netflix, and RYM datasets\ndemonstrate that HI-series algorithms significantly outperform their base\ncounterparts. In sparse data ($20\\%$ training), HI-MD achieves a\n$0.8\\%$-$4.4\\%$ improvement in F1-score over MD while maintaining higher\ndiversity (Diversity@20: 459 vs. 396 on MovieLens). For dense data ($80\\%$\ntraining), HI-BD improves F1-score by $2.3\\%$-$5.2\\%$ compared to BD, with\ndiversity gains up to $18.6\\%$. Notably, hybrid models consistently enhance\nnovelty in sparse settings and exhibit robust parameter adaptability. The\nresults validate that strategic hybridization effectively breaks the\naccuracy-diversity trade-off, offering a flexible framework for optimizing\nrecommendation systems across data sparsity levels.\n","authors":["Yu Peng","Ya-Hui An"],"pdf_url":"https://arxiv.org/pdf/2503.01305v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01194v1","updated":"2025-03-03T05:41:16Z","published":"2025-03-03T05:41:16Z","title":"Cancer Type, Stage and Prognosis Assessment from Pathology Reports using\n  LLMs","summary":"  Large Language Models (LLMs) have shown significant promise across various\nnatural language processing tasks. However, their application in the field of\npathology, particularly for extracting meaningful insights from unstructured\nmedical texts such as pathology reports, remains underexplored and not well\nquantified. In this project, we leverage state-of-the-art language models,\nincluding the GPT family, Mistral models, and the open-source Llama models, to\nevaluate their performance in comprehensively analyzing pathology reports.\nSpecifically, we assess their performance in cancer type identification, AJCC\nstage determination, and prognosis assessment, encompassing both information\nextraction and higher-order reasoning tasks. Based on a detailed analysis of\ntheir performance metrics in a zero-shot setting, we developed two\ninstruction-tuned models: Path-llama3.1-8B and Path-GPT-4o-mini-FT. These\nmodels demonstrated superior performance in zero-shot cancer type\nidentification, staging, and prognosis assessment compared to the other models\nevaluated.\n","authors":["Rachit Saluja","Jacob Rosenthal","Yoav Artzi","David J. Pisapia","Benjamin L. Liechty","Mert R. Sabuncu"],"pdf_url":"https://arxiv.org/pdf/2503.01194v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01151v1","updated":"2025-03-03T03:57:04Z","published":"2025-03-03T03:57:04Z","title":"ReaderLM-v2: Small Language Model for HTML to Markdown and JSON","summary":"  We present ReaderLM-v2, a compact 1.5 billion parameter language model\ndesigned for efficient web content extraction. Our model processes documents up\nto 512K tokens, transforming messy HTML into clean Markdown or JSON formats\nwith high accuracy -- making it an ideal tool for grounding large language\nmodels. The model's effectiveness results from two key innovations: (1) a\nthree-stage data synthesis pipeline that generates high quality, diverse\ntraining data by iteratively drafting, refining, and critiquing web content\nextraction; and (2) a unified training framework combining continuous\npre-training with multi-objective optimization. Intensive evaluation\ndemonstrates that ReaderLM-v2 outperforms GPT-4o-2024-08-06 and other larger\nmodels by 15-20\\% on carefully curated benchmarks, particularly excelling at\ndocuments exceeding 100K tokens, while maintaining significantly lower\ncomputational requirements.\n","authors":["Feng Wang","Zesheng Shi","Bo Wang","Nan Wang","Han Xiao"],"pdf_url":"https://arxiv.org/pdf/2503.01151v1.pdf","comment":"9 pages, 10-12 refs"}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.01980v1","updated":"2025-03-03T19:01:17Z","published":"2025-03-03T19:01:17Z","title":"Recurrence-Enhanced Vision-and-Language Transformers for Robust\n  Multimodal Document Retrieval","summary":"  Cross-modal retrieval is gaining increasing efficacy and interest from the\nresearch community, thanks to large-scale training, novel architectural and\nlearning designs, and its application in LLMs and multimodal LLMs. In this\npaper, we move a step forward and design an approach that allows for multimodal\nqueries, composed of both an image and a text, and can search within\ncollections of multimodal documents, where images and text are interleaved. Our\nmodel, ReT, employs multi-level representations extracted from different layers\nof both visual and textual backbones, both at the query and document side. To\nallow for multi-level and cross-modal understanding and feature extraction, ReT\nemploys a novel Transformer-based recurrent cell that integrates both textual\nand visual features at different layers, and leverages sigmoidal gates inspired\nby the classical design of LSTMs. Extensive experiments on M2KR and M-BEIR\nbenchmarks show that ReT achieves state-of-the-art performance across diverse\nsettings. Our source code and trained models are publicly available at\nhttps://github.com/aimagelab/ReT.\n","authors":["Davide Caffagni","Sara Sarto","Marcella Cornia","Lorenzo Baraldi","Rita Cucchiara"],"pdf_url":"https://arxiv.org/pdf/2503.01980v1.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2409.18459v2","updated":"2025-03-03T15:04:18Z","published":"2024-09-27T05:43:22Z","title":"FoodMLLM-JP: Leveraging Multimodal Large Language Models for Japanese\n  Recipe Generation","summary":"  Research on food image understanding using recipe data has been a\nlong-standing focus due to the diversity and complexity of the data. Moreover,\nfood is inextricably linked to people's lives, making it a vital research area\nfor practical applications such as dietary management. Recent advancements in\nMultimodal Large Language Models (MLLMs) have demonstrated remarkable\ncapabilities, not only in their vast knowledge but also in their ability to\nhandle languages naturally. While English is predominantly used, they can also\nsupport multiple languages including Japanese. This suggests that MLLMs are\nexpected to significantly improve performance in food image understanding\ntasks. We fine-tuned open MLLMs LLaVA-1.5 and Phi-3 Vision on a Japanese recipe\ndataset and benchmarked their performance against the closed model GPT-4o. We\nthen evaluated the content of generated recipes, including ingredients and\ncooking procedures, using 5,000 evaluation samples that comprehensively cover\nJapanese food culture. Our evaluation demonstrates that the open models trained\non recipe data outperform GPT-4o, the current state-of-the-art model, in\ningredient generation. Our model achieved F1 score of 0.531, surpassing\nGPT-4o's F1 score of 0.481, indicating a higher level of accuracy. Furthermore,\nour model exhibited comparable performance to GPT-4o in generating cooking\nprocedure text.\n","authors":["Yuki Imajuku","Yoko Yamakata","Kiyoharu Aizawa"],"pdf_url":"https://arxiv.org/pdf/2409.18459v2.pdf","comment":"15 pages, 5 figures. We found errors in the calculation of evaluation\n  metrics, which were corrected in this version with\n  $\\color{blue}{\\text{modifications highlighted in blue}}$. Please also see the\n  Appendix"},{"id":"http://arxiv.org/abs/2503.01415v1","updated":"2025-03-03T11:10:37Z","published":"2025-03-03T11:10:37Z","title":"Improving the Efficiency of VVC using Partitioning of Reference Frames","summary":"  In response to the growing demand for high-quality videos, Versatile Video\nCoding (VVC) was released in 2020, building on the hybrid coding architecture\nof its predecessor, HEVC, achieving about 50% bitrate reduction for the same\nvisual quality. It introduces more flexible block partitioning, enhancing\ncompression efficiency at the cost of increased encoding complexity. To make\nefficient use of VVC in practical applications, optimization is essential.\nVVenC, an optimized open-source VVC encoder, introduces multiple presets to\naddress the trade-off between compression efficiency and encoder complexity.\nAlthough an optimized set of encoding tools has been selected for each preset,\nthe rate-distortion (RD) search space in the encoder presets still poses a\nchallenge for efficient encoder implementations. In this paper, we propose\nEarly Termination using Reference Frames (ETRF), which improves the trade-off\nbetween encoding efficiency and time complexity and positions itself as a new\npreset between medium and fast presets. The CTU partitioning map of the\nreference frames in lower temporal layers is employed to accelerate the\nencoding of frames in higher temporal layers. The results show a reduction in\nthe encoding time of around 21% compared to the medium preset. Specifically,\nfor videos with high spatial and temporal complexities, which typically require\nlonger encoding times, the proposed method achieves a better trade-off between\nbitrate savings and encoding time compared to the fast preset.\n","authors":["Kamran Qureshi","Hadi Amirpour","Christian Timmerer"],"pdf_url":"https://arxiv.org/pdf/2503.01415v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01404v1","updated":"2025-03-03T10:58:50Z","published":"2025-03-03T10:58:50Z","title":"Multi-resolution Encoding for HTTP Adaptive Streaming using VVenC","summary":"  HTTP Adaptive Streaming (HAS) is a widely adopted method for delivering video\ncontent over the Internet, requiring each video to be encoded at multiple\nbitrates and resolution pairs, known as representations, to adapt to various\nnetwork conditions and device capabilities. This multi-bitrate encoding\nintroduces significant challenges due to the computational and time-intensive\nnature of encoding multiple representations. Conventional approaches often\nencode these videos independently without leveraging similarities between\ndifferent representations of the same input video. This paper proposes an\naccelerated multi-resolution encoding strategy that utilizes representations of\nlower resolutions as references to speed up the encoding of higher resolutions\nwhen using Versatile Video Coding (VVC); specifically in VVenC, an optimized\nopen-source software implementation. For multi-resolution encoding, a\nmid-bitrate representation serves as the reference, allowing interpolated\nencoded partition data to efficiently guide the partitioning process in higher\nresolutions. The proposed approach uses shared encoding information to reduce\nredundant calculations, optimizing partitioning decisions. Experimental results\ndemonstrate that the proposed technique achieves a reduction of up to 17%\ncompared to medium preset in encoding time across videos of varying\ncomplexities with minimal BDBR/BDT of 0.12 compared to the fast preset.\n","authors":["Kamran Qureshi","Hadi Amirpour","Christian Timmerer"],"pdf_url":"https://arxiv.org/pdf/2503.01404v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01396v1","updated":"2025-03-03T10:52:34Z","published":"2025-03-03T10:52:34Z","title":"CorrNetDroid: Android Malware Detector leveraging a Correlation-based\n  Feature Selection for Network Traffic features","summary":"  Copious mobile operating systems exist in the market, but Android remains the\nuser's choice. Meanwhile, its growing popularity has also attracted malware\ndevelopers. Researchers have proposed various static solutions for Android\nmalware detection. However, stealthier malware evade static analysis. This\nraises the need for a robust Android malware detection system capable of\ndealing with advanced threats and overcoming the shortcomings of static\nanalysis.\n  Hence, this work proposes a dynamic analysis-based Android malware detection\nsystem, CorrNetDroid, that works over network traffic flows. Many traffic\nfeatures exhibit overlapping ranges in normal and malware datasets. Therefore,\nwe first rank the features using two statistical measures, crRelevance and\nNormalized Mean Residue Similarity (NMRS), to assess feature-class and\nfeature-feature correlations. Thereafter, we introduce a novel\ncorrelation-based feature selection algorithm that applies NMRS on crRelevance\nrankings to identify the optimal feature subset for Android malware detection.\n  Experimental results highlight that our model effectively reduces the feature\nset while detecting Android malware with 99.50 percent accuracy when\nconsidering only two network traffic features. Furthermore, our experiments\ndemonstrate that the NMRS-based algorithm on crRelevance rankings outperforms\nstatistical tests such as chi-square, ANOVA, Mann-Whitney U test, and\nKruskal-Wallis test. In addition, our model surpasses various state-of-the-art\nAndroid malware detection techniques in terms of detection accuracy.\n","authors":["Yash Sharma","Anshul Arora"],"pdf_url":"https://arxiv.org/pdf/2503.01396v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01370v1","updated":"2025-03-03T10:07:19Z","published":"2025-03-03T10:07:19Z","title":"Kiss3DGen: Repurposing Image Diffusion Models for 3D Asset Generation","summary":"  Diffusion models have achieved great success in generating 2D images.\nHowever, the quality and generalizability of 3D content generation remain\nlimited. State-of-the-art methods often require large-scale 3D assets for\ntraining, which are challenging to collect. In this work, we introduce\nKiss3DGen (Keep It Simple and Straightforward in 3D Generation), an efficient\nframework for generating, editing, and enhancing 3D objects by repurposing a\nwell-trained 2D image diffusion model for 3D generation. Specifically, we\nfine-tune a diffusion model to generate ''3D Bundle Image'', a tiled\nrepresentation composed of multi-view images and their corresponding normal\nmaps. The normal maps are then used to reconstruct a 3D mesh, and the\nmulti-view images provide texture mapping, resulting in a complete 3D model.\nThis simple method effectively transforms the 3D generation problem into a 2D\nimage generation task, maximizing the utilization of knowledge in pretrained\ndiffusion models. Furthermore, we demonstrate that our Kiss3DGen model is\ncompatible with various diffusion model techniques, enabling advanced features\nsuch as 3D editing, mesh and texture enhancement, etc. Through extensive\nexperiments, we demonstrate the effectiveness of our approach, showcasing its\nability to produce high-quality 3D models efficiently.\n","authors":["Jiantao Lin","Xin Yang","Meixi Chen","Yingjie Xu","Dongyu Yan","Leyi Wu","Xinli Xu","Lie XU","Shunsi Zhang","Ying-Cong Chen"],"pdf_url":"https://arxiv.org/pdf/2503.01370v1.pdf","comment":"The first three authors contributed equally to this work"},{"id":"http://arxiv.org/abs/2503.01362v1","updated":"2025-03-03T09:55:54Z","published":"2025-03-03T09:55:54Z","title":"Streaming Piano Transcription Based on Consistent Onset and Offset\n  Decoding with Sustain Pedal Detection","summary":"  This paper describes a streaming audio-to-MIDI piano transcription approach\nthat aims to sequentially translate a music signal into a sequence of note\nonset and offset events. The sequence-to-sequence nature of this task may call\nfor the computationally-intensive transformer model for better performance,\nwhich has recently been used for offline transcription benchmarks and could be\nextended for streaming transcription with causal attention mechanisms. We\nassume that the performance limitation of this naive approach lies in the\ndecoder. Although time-frequency features useful for onset detection are\nconsiderably different from those for offset detection, the single decoder is\ntrained to output a mixed sequence of onset and offset events without guarantee\nof the correspondence between the onset and offset events of the same note. To\novercome this limitation, we propose a streaming encoder-decoder model that\nuses a convolutional encoder aggregating local acoustic features, followed by\nan autoregressive Transformer decoder detecting a variable number of onset\nevents and another decoder detecting the offset events for the active pitches\nwith validation of the sustain pedal at each time frame. Experiments using the\nMAESTRO dataset showed that the proposed streaming method performed comparably\nwith or even better than the state-of-the-art offline methods while\nsignificantly reducing the computational cost.\n","authors":["Weixing Wei","Jiahao Zhao","Yulun Wu","Kazuyoshi Yoshii"],"pdf_url":"https://arxiv.org/pdf/2503.01362v1.pdf","comment":"Accepted to ISMIR 2024"},{"id":"http://arxiv.org/abs/2503.01175v1","updated":"2025-03-03T04:47:39Z","published":"2025-03-03T04:47:39Z","title":"HOP: Heterogeneous Topology-based Multimodal Entanglement for Co-Speech\n  Gesture Generation","summary":"  Co-speech gestures are crucial non-verbal cues that enhance speech clarity\nand expressiveness in human communication, which have attracted increasing\nattention in multimodal research. While the existing methods have made strides\nin gesture accuracy, challenges remain in generating diverse and coherent\ngestures, as most approaches assume independence among multimodal inputs and\nlack explicit modeling of their interactions. In this work, we propose a novel\nmultimodal learning method named HOP for co-speech gesture generation that\ncaptures the heterogeneous entanglement between gesture motion, audio rhythm,\nand text semantics, enabling the generation of coordinated gestures. By\nleveraging spatiotemporal graph modeling, we achieve the alignment of audio and\naction. Moreover, to enhance modality coherence, we build the audio-text\nsemantic representation based on a reprogramming module, which is beneficial\nfor cross-modality adaptation. Our approach enables the trimodal system to\nlearn each other's features and represent them in the form of topological\nentanglement. Extensive experiments demonstrate that HOP achieves\nstate-of-the-art performance, offering more natural and expressive co-speech\ngesture generation. More information, codes, and demos are available here:\nhttps://star-uu-wang.github.io/HOP/\n","authors":["Hongye Cheng","Tianyu Wang","Guangsi Shi","Zexing Zhao","Yanwei Fu"],"pdf_url":"https://arxiv.org/pdf/2503.01175v1.pdf","comment":"Accepted by CVPR 2025. See https://star-uu-wang.github.io/HOP/"}]},"2025-03-02T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.01031v1","updated":"2025-03-02T21:33:49Z","published":"2025-03-02T21:33:49Z","title":"Can We Find the Code? An Empirical Study of Google Scholar's Code\n  Retrieval","summary":"  Academic codes associated with research papers are valuable resources for\nscholars. In specialized fields outside computer science, code availability is\noften limited, making effective code retrieval essential. Google Scholar is a\ncrucial academic search tool. If a code published in the paper is not\nretrievable via Google Scholar, its accessibility and impact are significantly\nreduced. This study takes the term \"accelerated degradation\" combined with\n\"reliability\" as an example, and finds that, for papers published by Elsevier,\nonly GitHub links included in abstracts are comprehensively retrieved by Google\nScholar. When such links appear within the main body of a paper, even in the\n\"Data Availability\" section, they may be ignored and become unsearchable. These\nfindings highlight the importance of strategically placing GitHub links in\nabstracts to enhance code discoverability on Google Scholar.\n","authors":["Shi-Shun Chen"],"pdf_url":"https://arxiv.org/pdf/2503.01031v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01003v1","updated":"2025-03-02T19:59:41Z","published":"2025-03-02T19:59:41Z","title":"A Semantic Search Pipeline for Causality-driven Adhoc Information\n  Retrieval","summary":"  We present a unsupervised semantic search pipeline for the Causality-driven\nAdhoc Information Retrieval (CAIR-2021) shared task. The CAIR shared task\nexpands traditional information retrieval to support the retrieval of documents\ncontaining the likely causes of a query event. A successful system must be able\nto distinguish between topical documents and documents containing causal\ndescriptions of events that are causally related to the query event. Our\napproach involves aggregating results from multiple query strategies over a\nsemantic and lexical index. The proposed approach leads the CAIR-2021\nleaderboard and outperformed both traditional IR and pure semantic\nembedding-based approaches.\n","authors":["Dhairya Dalal","Sharmi Dev Gupta","Bentolhoda Binaei"],"pdf_url":"https://arxiv.org/pdf/2503.01003v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.01001v1","updated":"2025-03-02T19:43:35Z","published":"2025-03-02T19:43:35Z","title":"Towards An Efficient LLM Training Paradigm for CTR Prediction","summary":"  Large Language Models (LLMs) have demonstrated tremendous potential as the\nnext-generation ranking-based recommendation system. Many recent works have\nshown that LLMs can significantly outperform conventional click-through-rate\n(CTR) prediction approaches. Despite such promising results, the computational\ninefficiency inherent in the current training paradigm makes it particularly\nchallenging to train LLMs for ranking-based recommendation tasks on large\ndatasets. To train LLMs for CTR prediction, most existing studies adopt the\nprevalent ''sliding-window'' paradigm. Given a sequence of $m$ user\ninteractions, a unique training prompt is constructed for each interaction by\ndesignating it as the prediction target along with its preceding $n$\ninteractions serving as context. In turn, the sliding-window paradigm results\nin an overall complexity of $O(mn^2)$ that scales linearly with the length of\nuser interactions. Consequently, a direct adoption to train LLMs with such\nstrategy can result in prohibitively high training costs as the length of\ninteractions grows. To alleviate the computational inefficiency, we propose a\nnovel training paradigm, namely Dynamic Target Isolation (DTI), that\nstructurally parallelizes the training of $k$ (where $k >> 1$) target\ninteractions. Furthermore, we identify two major bottlenecks - hidden-state\nleakage and positional bias overfitting - that limit DTI to only scale up to a\nsmall value of $k$ (e.g., 5) then propose a computationally light solution to\neffectively tackle each. Through extensive experiments on three widely adopted\npublic CTR datasets, we empirically show that DTI reduces training time by an\naverage of $\\textbf{92%}$ (e.g., from $70.5$ hrs to $5.31$ hrs), without\ncompromising CTR prediction performance.\n","authors":["Allen Lin","Renqin Cai","Yun He","Hanchao Yu","Jing Qian","Rui Li","Qifan Wang","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2503.01001v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00999v1","updated":"2025-03-02T19:39:29Z","published":"2025-03-02T19:39:29Z","title":"Federated Conversational Recommender System","summary":"  Conversational Recommender Systems (CRSs) have become increasingly popular as\na powerful tool for providing personalized recommendation experiences. By\ndirectly engaging with users in a conversational manner to learn their current\nand fine-grained preferences, a CRS can quickly derive recommendations that are\nrelevant and justifiable. However, existing conversational recommendation\nsystems (CRSs) typically rely on a centralized training and deployment process,\nwhich involves collecting and storing explicitly-communicated user preferences\nin a centralized repository. These fine-grained user preferences are completely\nhuman-interpretable and can easily be used to infer sensitive information\n(e.g., financial status, political stands, and health information) about the\nuser, if leaked or breached. To address the user privacy concerns in CRS, we\nfirst define a set of privacy protection guidelines for preserving user privacy\nunder the conversational recommendation setting. Based on these guidelines, we\npropose a novel federated conversational recommendation framework that\neffectively reduces the risk of exposing user privacy by (i) de-centralizing\nboth the historical interests estimation stage and the interactive preference\nelicitation stage and (ii) strictly bounding privacy leakage by enforcing\nuser-level differential privacy with meticulously selected privacy budgets.\nThrough extensive experiments, we show that the proposed framework not only\nsatisfies these user privacy protection guidelines, but also enables the system\nto achieve competitive recommendation performance even when compared to the\nstate-of-the-art non-private conversational recommendation approach.\n","authors":["Allen Lin","Jianling Wang","Ziwei Zhu","James Caverlee"],"pdf_url":"https://arxiv.org/pdf/2503.00999v1.pdf","comment":"ECIR 2024"},{"id":"http://arxiv.org/abs/2503.00863v1","updated":"2025-03-02T11:45:50Z","published":"2025-03-02T11:45:50Z","title":"Systematic Literature Review on Clinical Trial Eligibility Matching","summary":"  Clinical trial eligibility matching is a critical yet often labor-intensive\nand error-prone step in medical research, as it ensures that participants meet\nprecise criteria for safe and reliable study outcomes. Recent advances in\nNatural Language Processing (NLP) have shown promise in automating and\nimproving this process by rapidly analyzing large volumes of unstructured\nclinical text and structured electronic health record (EHR) data. In this\npaper, we present a systematic overview of current NLP methodologies applied to\nclinical trial eligibility screening, focusing on data sources, annotation\npractices, machine learning approaches, and real-world implementation\nchallenges. A comprehensive literature search (spanning Google Scholar,\nMendeley, and PubMed from 2015 to 2024) yielded high-quality studies, each\ndemonstrating the potential of techniques such as rule-based systems, named\nentity recognition, contextual embeddings, and ontology-based normalization to\nenhance patient matching accuracy. While results indicate substantial\nimprovements in screening efficiency and precision, limitations persist\nregarding data completeness, annotation consistency, and model scalability\nacross diverse clinical domains. The review highlights how explainable AI and\nstandardized ontologies can bolster clinician trust and broaden adoption.\nLooking ahead, further research into advanced semantic and temporal\nrepresentations, expanded data integration, and rigorous prospective\nevaluations is necessary to fully realize the transformative potential of NLP\nin clinical trial recruitment.\n","authors":["Muhammad Talha Sharif","Abdul Rehman"],"pdf_url":"https://arxiv.org/pdf/2503.00863v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00781v1","updated":"2025-03-02T08:11:07Z","published":"2025-03-02T08:11:07Z","title":"Towards Efficient Educational Chatbots: Benchmarking RAG Frameworks","summary":"  Large Language Models (LLMs) have proven immensely beneficial in education by\ncapturing vast amounts of literature-based information, allowing them to\ngenerate context without relying on external sources. In this paper, we propose\na generative AI-powered GATE question-answering framework (GATE stands for\nGraduate Aptitude Test in Engineering) that leverages LLMs to explain GATE\nsolutions and support students in their exam preparation. We conducted\nextensive benchmarking to select the optimal embedding model and LLM,\nevaluating our framework based on criteria such as latency, faithfulness, and\nrelevance, with additional validation through human evaluation. Our chatbot\nintegrates state-of-the-art embedding models and LLMs to deliver accurate,\ncontext-aware responses. Through rigorous experimentation, we identified\nconfigurations that balance performance and computational efficiency, ensuring\na reliable chatbot to serve students' needs. Additionally, we discuss the\nchallenges faced in data processing and modeling and implemented solutions. Our\nwork explores the application of Retrieval-Augmented Generation (RAG) for GATE\nQ/A explanation tasks, and our findings demonstrate significant improvements in\nretrieval accuracy and response quality. This research offers practical\ninsights for developing effective AI-driven educational tools while\nhighlighting areas for future enhancement in usability and scalability.\n","authors":["Umar Ali Khan","Ekram Khan","Fiza Khan","Athar Ali Moinuddin"],"pdf_url":"https://arxiv.org/pdf/2503.00781v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.10594v2","updated":"2025-03-02T01:19:51Z","published":"2024-10-14T15:04:18Z","title":"VisRAG: Vision-based Retrieval-augmented Generation on Multi-modality\n  Documents","summary":"  Retrieval-augmented generation (RAG) is an effective technique that enables\nlarge language models (LLMs) to utilize external knowledge sources for\ngeneration. However, current RAG systems are solely based on text, rendering it\nimpossible to utilize vision information like layout and images that play\ncrucial roles in real-world multi-modality documents. In this paper, we\nintroduce VisRAG, which tackles this issue by establishing a vision-language\nmodel (VLM)-based RAG pipeline. In this pipeline, instead of first parsing the\ndocument to obtain text, the document is directly embedded using a VLM as an\nimage and then retrieved to enhance the generation of a VLM. Compared to\ntraditional text-based RAG, VisRAG maximizes the retention and utilization of\nthe data information in the original documents, eliminating the information\nloss introduced during the parsing process. We collect both open-source and\nsynthetic data to train the retriever in VisRAG and explore a variety of\ngeneration methods. Experiments demonstrate that VisRAG outperforms traditional\nRAG in both the retrieval and generation stages, achieving a 20--40% end-to-end\nperformance gain over traditional text-based RAG pipeline. Further analysis\nreveals that VisRAG is efficient in utilizing training data and demonstrates\nstrong generalization capability, positioning it as a promising solution for\nRAG on multi-modality documents. Our code and data are available at\nhttps://github.com/openbmb/visrag.\n","authors":["Shi Yu","Chaoyue Tang","Bokai Xu","Junbo Cui","Junhao Ran","Yukun Yan","Zhenghao Liu","Shuo Wang","Xu Han","Zhiyuan Liu","Maosong Sun"],"pdf_url":"https://arxiv.org/pdf/2410.10594v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00674v1","updated":"2025-03-02T00:28:55Z","published":"2025-03-02T00:28:55Z","title":"OrdRankBen: A Novel Ranking Benchmark for Ordinal Relevance in NLP","summary":"  The evaluation of ranking tasks remains a significant challenge in natural\nlanguage processing (NLP), particularly due to the lack of direct labels for\nresults in real-world scenarios. Benchmark datasets play a crucial role in\nproviding standardized testbeds that ensure fair comparisons, enhance\nreproducibility, and enable progress tracking, facilitating rigorous assessment\nand continuous improvement of ranking models. Existing NLP ranking benchmarks\ntypically use binary relevance labels or continuous relevance scores,\nneglecting ordinal relevance scores. However, binary labels oversimplify\nrelevance distinctions, while continuous scores lack a clear ordinal structure,\nmaking it challenging to capture nuanced ranking differences effectively. To\naddress these challenges, we introduce OrdRankBen, a novel benchmark designed\nto capture multi-granularity relevance distinctions. Unlike conventional\nbenchmarks, OrdRankBen incorporates structured ordinal labels, enabling more\nprecise ranking evaluations. Given the absence of suitable datasets for ordinal\nrelevance ranking in NLP, we constructed two datasets with distinct ordinal\nlabel distributions. We further evaluate various models for three model types,\nranking-based language models, general large language models, and\nranking-focused large language models on these datasets. Experimental results\nshow that ordinal relevance modeling provides a more precise evaluation of\nranking models, improving their ability to distinguish multi-granularity\ndifferences among ranked items-crucial for tasks that demand fine-grained\nrelevance differentiation.\n","authors":["Yan Wang","Lingfei Qian","Xueqing Peng","Jimin Huang","Dongji Feng"],"pdf_url":"https://arxiv.org/pdf/2503.00674v1.pdf","comment":"6 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2410.04810v2","updated":"2025-03-02T17:18:04Z","published":"2024-10-07T07:45:18Z","title":"FedBiP: Heterogeneous One-Shot Federated Learning with Personalized\n  Latent Diffusion Models","summary":"  One-Shot Federated Learning (OSFL), a special decentralized machine learning\nparadigm, has recently gained significant attention. OSFL requires only a\nsingle round of client data or model upload, which reduces communication costs\nand mitigates privacy threats compared to traditional FL. Despite these\npromising prospects, existing methods face challenges due to client data\nheterogeneity and limited data quantity when applied to real-world OSFL\nsystems. Recently, Latent Diffusion Models (LDM) have shown remarkable\nadvancements in synthesizing high-quality images through pretraining on\nlarge-scale datasets, thereby presenting a potential solution to overcome these\nissues. However, directly applying pretrained LDM to heterogeneous OSFL results\nin significant distribution shifts in synthetic data, leading to performance\ndegradation in classification models trained on such data. This issue is\nparticularly pronounced in rare domains, such as medical imaging, which are\nunderrepresented in LDM's pretraining data. To address this challenge, we\npropose Federated Bi-Level Personalization (FedBiP), which personalizes the\npretrained LDM at both instance-level and concept-level. Hereby, FedBiP\nsynthesizes images following the client's local data distribution without\ncompromising the privacy regulations. FedBiP is also the first approach to\nsimultaneously address feature space heterogeneity and client data scarcity in\nOSFL. Our method is validated through extensive experiments on three OSFL\nbenchmarks with feature space heterogeneity, as well as on challenging medical\nand satellite image datasets with label heterogeneity. The results demonstrate\nthe effectiveness of FedBiP, which substantially outperforms other OSFL\nmethods.\n","authors":["Haokun Chen","Hang Li","Yao Zhang","Jinhe Bi","Gengyuan Zhang","Yueqi Zhang","Philip Torr","Jindong Gu","Denis Krompass","Volker Tresp"],"pdf_url":"https://arxiv.org/pdf/2410.04810v2.pdf","comment":"CVPR 2025"},{"id":"http://arxiv.org/abs/2408.11915v2","updated":"2025-03-02T15:55:14Z","published":"2024-08-21T18:06:15Z","title":"Video-Foley: Two-Stage Video-To-Sound Generation via Temporal Event\n  Condition For Foley Sound","summary":"  Foley sound synthesis is crucial for multimedia production, enhancing user\nexperience by synchronizing audio and video both temporally and semantically.\nRecent studies on automating this labor-intensive process through\nvideo-to-sound generation face significant challenges. Systems lacking explicit\ntemporal features suffer from poor alignment and controllability, while\ntimestamp-based models require costly and subjective human annotation. We\npropose Video-Foley, a video-to-sound system using Root Mean Square (RMS) as an\nintuitive condition with semantic timbre prompts (audio or text). RMS, a\nframe-level intensity envelope closely related to audio semantics, acts as a\ntemporal event feature to guide audio generation from video. The\nannotation-free self-supervised learning framework consists of two stages,\nVideo2RMS and RMS2Sound, incorporating novel ideas including RMS discretization\nand RMS-ControlNet with a pretrained text-to-audio model. Our extensive\nevaluation shows that Video-Foley achieves state-of-the-art performance in\naudio-visual alignment and controllability for sound timing, intensity, timbre,\nand nuance. Source code, model weights and demos are available on our companion\nwebsite. (https://jnwnlee.github.io/video-foley-demo)\n","authors":["Junwon Lee","Jaekwon Im","Dabin Kim","Juhan Nam"],"pdf_url":"https://arxiv.org/pdf/2408.11915v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2310.18709v4","updated":"2025-03-02T15:37:39Z","published":"2023-10-28T13:37:52Z","title":"Audio-Visual Instance Segmentation","summary":"  In this paper, we propose a new multi-modal task, termed audio-visual\ninstance segmentation (AVIS), which aims to simultaneously identify, segment\nand track individual sounding object instances in audible videos. To facilitate\nthis research, we introduce a high-quality benchmark named AVISeg, containing\nover 90K instance masks from 26 semantic categories in 926 long videos.\nAdditionally, we propose a strong baseline model for this task. Our model first\nlocalizes sound source within each frame, and condenses object-specific\ncontexts into concise tokens. Then it builds long-range audio-visual\ndependencies between these tokens using window-based attention, and tracks\nsounding objects among the entire video sequences. Extensive experiments reveal\nthat our method performs best on AVISeg, surpassing the existing methods from\nrelated tasks. We further conduct the evaluation on several multi-modal large\nmodels. Unfortunately, they exhibits subpar performance on instance-level sound\nsource localization and temporal perception. We expect that AVIS will inspire\nthe community towards a more comprehensive multi-modal understanding. Dataset\nand code is available at https://github.com/ruohaoguo/avis.\n","authors":["Ruohao Guo","Xianghua Ying","Yaru Chen","Dantong Niu","Guangyao Li","Liao Qu","Yanyu Qi","Jinxing Zhou","Bowei Xing","Wenzhen Yue","Ji Shi","Qixun Wang","Peiliang Zhang","Buwen Liang"],"pdf_url":"https://arxiv.org/pdf/2310.18709v4.pdf","comment":"Accepted by CVPR 2025"},{"id":"http://arxiv.org/abs/2410.11817v2","updated":"2025-03-02T07:05:19Z","published":"2024-10-15T17:46:31Z","title":"Improving Long-Text Alignment for Text-to-Image Diffusion Models","summary":"  The rapid advancement of text-to-image (T2I) diffusion models has enabled\nthem to generate unprecedented results from given texts. However, as text\ninputs become longer, existing encoding methods like CLIP face limitations, and\naligning the generated images with long texts becomes challenging. To tackle\nthese issues, we propose LongAlign, which includes a segment-level encoding\nmethod for processing long texts and a decomposed preference optimization\nmethod for effective alignment training. For segment-level encoding, long texts\nare divided into multiple segments and processed separately. This method\novercomes the maximum input length limits of pretrained encoding models. For\npreference optimization, we provide decomposed CLIP-based preference models to\nfine-tune diffusion models. Specifically, to utilize CLIP-based preference\nmodels for T2I alignment, we delve into their scoring mechanisms and find that\nthe preference scores can be decomposed into two components: a text-relevant\npart that measures T2I alignment and a text-irrelevant part that assesses other\nvisual aspects of human preference. Additionally, we find that the\ntext-irrelevant part contributes to a common overfitting problem during\nfine-tuning. To address this, we propose a reweighting strategy that assigns\ndifferent weights to these two components, thereby reducing overfitting and\nenhancing alignment. After fine-tuning $512 \\times 512$ Stable Diffusion (SD)\nv1.5 for about 20 hours using our method, the fine-tuned SD outperforms\nstronger foundation models in T2I alignment, such as PixArt-$\\alpha$ and\nKandinsky v2.2. The code is available at\nhttps://github.com/luping-liu/LongAlign.\n","authors":["Luping Liu","Chao Du","Tianyu Pang","Zehan Wang","Chongxuan Li","Dong Xu"],"pdf_url":"https://arxiv.org/pdf/2410.11817v2.pdf","comment":null}]},"2025-03-01T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.20317v2","updated":"2025-03-01T23:21:58Z","published":"2025-02-27T17:42:52Z","title":"Mixture of Structural-and-Textual Retrieval over Text-rich Graph\n  Knowledge Bases","summary":"  Text-rich Graph Knowledge Bases (TG-KBs) have become increasingly crucial for\nanswering queries by providing textual and structural knowledge. However,\ncurrent retrieval methods often retrieve these two types of knowledge in\nisolation without considering their mutual reinforcement and some hybrid\nmethods even bypass structural retrieval entirely after neighboring\naggregation. To fill in this gap, we propose a Mixture of\nStructural-and-Textual Retrieval (MoR) to retrieve these two types of knowledge\nvia a Planning-Reasoning-Organizing framework. In the Planning stage, MoR\ngenerates textual planning graphs delineating the logic for answering queries.\nFollowing planning graphs, in the Reasoning stage, MoR interweaves structural\ntraversal and textual matching to obtain candidates from TG-KBs. In the\nOrganizing stage, MoR further reranks fetched candidates based on their\nstructural trajectory. Extensive experiments demonstrate the superiority of MoR\nin harmonizing structural and textual retrieval with insights, including uneven\nretrieving performance across different query logics and the benefits of\nintegrating structural trajectories for candidate reranking. Our code is\navailable at https://github.com/Yoega/MoR.\n","authors":["Yongjia Lei","Haoyu Han","Ryan A. Rossi","Franck Dernoncourt","Nedim Lipka","Mahantesh M Halappanavar","Jiliang Tang","Yu Wang"],"pdf_url":"https://arxiv.org/pdf/2502.20317v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2109.12887v5","updated":"2025-03-01T22:46:43Z","published":"2021-09-27T09:17:53Z","title":"ICPE: An Item Cluster-Wise Pareto-Efficient Framework for Recommendation\n  Debiasing","summary":"  Recommender system based on historical user-item interactions is of vital\nimportance for web-based services. However, the observed data used to train the\nrecommender model suffers from severe bias issues. Practically, the item\nfrequency distribution of the dataset is a highly skewed power-law\ndistribution. Interactions of a small fraction of head items account for almost\nthe whole training data. The normal training paradigm from such biased data\ntends to repetitively generate recommendations from the head items, which\nfurther exacerbates the biases and affects the exploration of potentially\ninteresting items from the niche set. In this work, we innovatively explore the\ncentral theme of recommendation debiasing from an item cluster-wise\nmulti-objective optimization perspective. Aiming to balance the learning on\nvarious item clusters that differ in popularity during the training process, we\npropose a model-agnostic framework namely Item Cluster-Wise Pareto-Efficient\nRecommendation (ICPE). In detail, we define our item cluster-wise optimization\ntarget as the recommender model should balance all item clusters that differ in\npopularity, thus we set the model learning on each item cluster as a unique\noptimization objective. To achieve this goal, we first explore items'\npopularity levels from a novel causal reasoning perspective. Then, we devise\npopularity discrepancy-based bisecting clustering to separate the item\nclusters. Next, we adaptively find the overall harmonious gradient direction\nfor cluster-wise optimization objectives from a Pareto-efficient solver.\nFinally, in the prediction stage, we perform counterfactual inference to\nfurther eliminate the impact of global propensity. Extensive experimental\nresults verify the superiorities of ICPE on overall recommendation performance\nand biases elimination.\n","authors":["Yule Wang","Xin Xin","Yue Ding","Yunzhe Li","Dong Wang"],"pdf_url":"https://arxiv.org/pdf/2109.12887v5.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00619v1","updated":"2025-03-01T20:55:28Z","published":"2025-03-01T20:55:28Z","title":"PinLanding: Content-First Keyword Landing Page Generation via\n  Multi-Modal AI for Web-Scale Discovery","summary":"  Online platforms like Pinterest hosting vast content collections\ntraditionally rely on manual curation or user-generated search logs to create\nkeyword landing pages (KLPs) -- topic-centered collection pages that serve as\nentry points for content discovery. While manual curation ensures quality, it\ndoesn't scale to millions of collections, and search log approaches result in\nlimited topic coverage and imprecise content matching. In this paper, we\npresent PinLanding, a novel content-first architecture that transforms the way\nplatforms create topical collections. Instead of deriving topics from user\nbehavior, our system employs a multi-stage pipeline combining vision-language\nmodel (VLM) for attribute extraction, large language model (LLM) for topic\ngeneration, and a CLIP-based dual-encoder architecture for precise content\nmatching. Our model achieves 99.7% Recall@10 on Fashion200K benchmark,\ndemonstrating strong attribute understanding capabilities. In production\ndeployment for search engine optimization with 4.2 million shopping landing\npages, the system achieves a 4X increase in topic coverage and 14.29%\nimprovement in collection attribute precision over the traditional search\nlog-based approach via human evaluation. The architecture can be generalized\nbeyond search traffic to power various user experiences, including content\ndiscovery and recommendations, providing a scalable solution to transform\nunstructured content into curated topical collections across any content\ndomain.\n","authors":["Faye Zhang","Jasmine Wan","Qianyu Cheng","Jinfeng Rao"],"pdf_url":"https://arxiv.org/pdf/2503.00619v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2407.20207v2","updated":"2025-03-01T14:39:33Z","published":"2024-07-29T17:39:08Z","title":"QAEA-DR: A Unified Text Augmentation Framework for Dense Retrieval","summary":"  In dense retrieval, embedding long texts into dense vectors can result in\ninformation loss, leading to inaccurate query-text matching. Additionally,\nlow-quality texts with excessive noise or sparse key information are unlikely\nto align well with relevant queries. Recent studies mainly focus on improving\nthe sentence embedding model or retrieval process. In this work, we introduce a\nnovel text augmentation framework for dense retrieval. This framework\ntransforms raw documents into information-dense text formats, which supplement\nthe original texts to effectively address the aforementioned issues without\nmodifying embedding or retrieval methodologies. Two text representations are\ngenerated via large language models (LLMs) zero-shot prompting: question-answer\npairs and element-driven events. We term this approach QAEA-DR: unifying\nquestion-answer generation and event extraction in a text augmentation\nframework for dense retrieval. To further enhance the quality of generated\ntexts, a scoring-based evaluation and regeneration mechanism is introduced in\nLLM prompting. Our QAEA-DR model has a positive impact on dense retrieval,\nsupported by both theoretical analysis and empirical experiments.\n","authors":["Hongming Tan","Shaoxiong Zhan","Hai Lin","Hai-Tao Zheng","Wai Kin Chan"],"pdf_url":"https://arxiv.org/pdf/2407.20207v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00501v1","updated":"2025-03-01T14:15:00Z","published":"2025-03-01T14:15:00Z","title":"Qilin: A Multimodal Information Retrieval Dataset with APP-level User\n  Sessions","summary":"  User-generated content (UGC) communities, especially those featuring\nmultimodal content, improve user experiences by integrating visual and textual\ninformation into results (or items). The challenge of improving user\nexperiences in complex systems with search and recommendation (S\\&R) services\nhas drawn significant attention from both academia and industry these years.\nHowever, the lack of high-quality datasets has limited the research progress on\nmultimodal S\\&R. To address the growing need for developing better S\\&R\nservices, we present a novel multimodal information retrieval dataset in this\npaper, namely Qilin. The dataset is collected from Xiaohongshu, a popular\nsocial platform with over 300 million monthly active users and an average\nsearch penetration rate of over 70\\%. In contrast to existing datasets,\n\\textsf{Qilin} offers a comprehensive collection of user sessions with\nheterogeneous results like image-text notes, video notes, commercial notes, and\ndirect answers, facilitating the development of advanced multimodal neural\nretrieval models across diverse task settings. To better model user\nsatisfaction and support the analysis of heterogeneous user behaviors, we also\ncollect extensive APP-level contextual signals and genuine user feedback.\nNotably, Qilin contains user-favored answers and their referred results for\nsearch requests triggering the Deep Query Answering (DQA) module. This allows\nnot only the training \\& evaluation of a Retrieval-augmented Generation (RAG)\npipeline, but also the exploration of how such a module would affect users'\nsearch behavior. Through comprehensive analysis and experiments, we provide\ninteresting findings and insights for further improving S\\&R systems. We hope\nthat \\textsf{Qilin} will significantly contribute to the advancement of\nmultimodal content platforms with S\\&R services in the future.\n","authors":["Jia Chen","Qian Dong","Haitao Li","Xiaohui He","Yan Gao","Shaosheng Cao","Yi Wu","Ping Yang","Chen Xu","Yao Hu","Qingyao Ai","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2503.00501v1.pdf","comment":"11 pages"},{"id":"http://arxiv.org/abs/2503.00479v1","updated":"2025-03-01T13:12:41Z","published":"2025-03-01T13:12:41Z","title":"Bayesian Active Learning for Multi-Criteria Comparative Judgement in\n  Educational Assessment","summary":"  Comparative Judgement (CJ) provides an alternative assessment approach by\nevaluating work holistically rather than breaking it into discrete criteria.\nThis method leverages human ability to make nuanced comparisons, yielding more\nreliable and valid assessments. CJ aligns with real-world evaluations, where\noverall quality emerges from the interplay of various elements. However,\nrubrics remain widely used in education, offering structured criteria for\ngrading and detailed feedback. This creates a gap between CJ's holistic ranking\nand the need for criterion-based performance breakdowns.\n  This paper addresses this gap using a Bayesian approach. We build on Bayesian\nCJ (BCJ) by Gray et al., which directly models preferences instead of using\nlikelihoods over total scores, allowing for expected ranks with uncertainty\nestimation. Their entropy-based active learning method selects the most\ninformative pairwise comparisons for assessors. We extend BCJ to handle\nmultiple independent learning outcome (LO) components, defined by a rubric,\nenabling both holistic and component-wise predictive rankings with uncertainty\nestimates. Additionally, we propose a method to aggregate entropies and\nidentify the most informative comparison for assessors. Experiments on\nsynthetic and real data demonstrate our method's effectiveness. Finally, we\naddress a key limitation of BCJ, which is the inability to quantify assessor\nagreement. We show how to derive agreement levels, enhancing transparency in\nassessment.\n","authors":["Andy Gray","Alma Rahat","Tom Crick","Stephen Lindsay"],"pdf_url":"https://arxiv.org/pdf/2503.00479v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00353v1","updated":"2025-03-01T05:05:24Z","published":"2025-03-01T05:05:24Z","title":"U-NIAH: Unified RAG and LLM Evaluation for Long Context\n  Needle-In-A-Haystack","summary":"  Recent advancements in Large Language Models (LLMs) have expanded their\ncontext windows to unprecedented lengths, sparking debates about the necessity\nof Retrieval-Augmented Generation (RAG). To address the fragmented evaluation\nparadigms and limited cases in existing Needle-in-a-Haystack (NIAH), this paper\nintroduces U-NIAH, a unified framework that systematically compares LLMs and\nRAG methods in controlled long context settings. Our framework extends beyond\ntraditional NIAH by incorporating multi-needle, long-needle, and\nneedle-in-needle configurations, along with different retrieval settings, while\nleveraging the synthetic Starlight Academy dataset-a fictional magical\nuniverse-to eliminate biases from pre-trained knowledge. Through extensive\nexperiments, we investigate three research questions: (1) performance\ntrade-offs between LLMs and RAG, (2) error patterns in RAG, and (3) RAG's\nlimitations in complex settings. Our findings show that RAG significantly\nenhances smaller LLMs by mitigating the \"lost-in-the-middle\" effect and\nimproving robustness, achieving an 82.58% win-rate over LLMs. However, we\nobserve that retrieval noise and reverse chunk ordering degrade performance,\nwhile surprisingly, advanced reasoning LLMs exhibit reduced RAG compatibility\ndue to sensitivity to semantic distractors. We identify typical error patterns\nincluding omission due to noise, hallucination under high noise critical\ncondition, and self-doubt behaviors. Our work not only highlights the\ncomplementary roles of RAG and LLMs, but also provides actionable insights for\noptimizing deployments. Code: https://github.com/Tongji-KGLLM/U-NIAH.\n","authors":["Yunfan Gao","Yun Xiong","Wenlong Wu","Zijing Huang","Bohan Li","Haofen Wang"],"pdf_url":"https://arxiv.org/pdf/2503.00353v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00309v1","updated":"2025-03-01T02:39:37Z","published":"2025-03-01T02:39:37Z","title":"Pseudo-Knowledge Graph: Meta-Path Guided Retrieval and In-Graph Text for\n  RAG-Equipped LLM","summary":"  The advent of Large Language Models (LLMs) has revolutionized natural\nlanguage processing. However, these models face challenges in retrieving\nprecise information from vast datasets. Retrieval-Augmented Generation (RAG)\nwas developed to combining LLMs with external information retrieval systems to\nenhance the accuracy and context of responses. Despite improvements, RAG still\nstruggles with comprehensive retrieval in high-volume, low-information-density\ndatabases and lacks relational awareness, leading to fragmented answers.\n  To address this, this paper introduces the Pseudo-Knowledge Graph (PKG)\nframework, designed to overcome these limitations by integrating Meta-path\nRetrieval, In-graph Text and Vector Retrieval into LLMs. By preserving natural\nlanguage text and leveraging various retrieval techniques, the PKG offers a\nricher knowledge representation and improves accuracy in information retrieval.\nExtensive evaluations using Open Compass and MultiHop-RAG benchmarks\ndemonstrate the framework's effectiveness in managing large volumes of data and\ncomplex relationships.\n","authors":["Yuxin Yang","Haoyang Wu","Tao Wang","Jia Yang","Hao Ma","Guojie Luo"],"pdf_url":"https://arxiv.org/pdf/2503.00309v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00278v1","updated":"2025-03-01T01:11:24Z","published":"2025-03-01T01:11:24Z","title":"NeuroLit Navigator: A Neurosymbolic Approach to Scholarly Article\n  Searches for Systematic Reviews","summary":"  The introduction of Large Language Models (LLMs) has significantly impacted\nvarious fields, including education, for example, by enabling the creation of\npersonalized learning materials. However, their use in Systematic Reviews (SRs)\nreveals limitations such as restricted access to specialized vocabularies, lack\nof domain-specific reasoning, and a tendency to generate inaccurate\ninformation. Existing SR tools often rely on traditional NLP methods and fail\nto address these issues adequately. To overcome these challenges, we developed\nthe ``NeuroLit Navigator,'' a system that combines domain-specific LLMs with\nstructured knowledge sources like Medical Subject Headings (MeSH) and the\nUnified Medical Language System (UMLS). This integration enhances query\nformulation, expands search vocabularies, and deepens search scopes, enabling\nmore precise searches. Deployed in multiple universities and tested by over a\ndozen librarians, the NeuroLit Navigator has reduced the time required for\ninitial literature searches by 90\\%. Despite this efficiency, the initial set\nof articles retrieved can vary in relevance and quality. Nonetheless, the\nsystem has greatly improved the reproducibility of search results,\ndemonstrating its potential to support librarians in the SR process.\n","authors":["Vedant Khandelwal","Kaushik Roy","Valerie Lookingbill","Ritvik Garimella","Harshul Surana","Heather Heckman","Amit Sheth"],"pdf_url":"https://arxiv.org/pdf/2503.00278v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2403.17901v3","updated":"2025-03-01T00:49:44Z","published":"2024-03-26T17:43:08Z","title":"Search and Society: Reimagining Information Access for Radical Futures","summary":"  Information retrieval (IR) research must understand and contend with the\nsocial implications of the technology it produces. Instead of adopting a\nreactionary strategy of trying to mitigate potential social harms from emerging\ntechnologies, the community should aim to proactively set the research agenda\nfor the kinds of systems we should build inspired by diverse explicitly stated\nsociotechnical imaginaries. The sociotechnical imaginaries that underpin the\ndesign and development of information access technologies needs to be\nexplicitly articulated, and we need to develop theories of change in context of\nthese diverse perspectives. Our guiding future imaginaries must be informed by\nother academic fields, such as human-computer interaction, information\nsciences, media studies, design, science and technology studies, social\nsciences, humanities, democratic theory, and critical theory, as well as legal\nand policy experts, civil rights and social justice activists, and artists,\namong others. In this perspective paper, we motivate why the community must\nconsider this radical shift in how we do research and what we work on, and\nsketch a path forward towards this transformation.\n","authors":["Bhaskar Mitra"],"pdf_url":"https://arxiv.org/pdf/2403.17901v3.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.00625v1","updated":"2025-03-01T21:28:12Z","published":"2025-03-01T21:28:12Z","title":"Perceptual Visual Quality Assessment: Principles, Methods, and Future\n  Directions","summary":"  As multimedia services such as video streaming, video conferencing, virtual\nreality (VR), and online gaming continue to expand, ensuring high perceptual\nvisual quality becomes a priority to maintain user satisfaction and\ncompetitiveness. However, multimedia content undergoes various distortions\nduring acquisition, compression, transmission, and storage, resulting in the\ndegradation of experienced quality. Thus, perceptual visual quality assessment\n(PVQA), which focuses on evaluating the quality of multimedia content based on\nhuman perception, is essential for optimizing user experiences in advanced\ncommunication systems. Several challenges are involved in the PVQA process,\nincluding diverse characteristics of multimedia content such as image, video,\nVR, point cloud, mesh, multimodality, etc., and complex distortion scenarios as\nwell as viewing conditions. In this paper, we first present an overview of PVQA\nprinciples and methods. This includes both subjective methods, where users\ndirectly rate their experiences, and objective methods, where algorithms\npredict human perception based on measurable factors such as bitrate, frame\nrate, and compression levels. Based on the basics of PVQA, quality predictors\nfor different multimedia data are then introduced. In addition to traditional\nimages and videos, immersive multimedia and generative artificial intelligence\n(GenAI) content are also discussed. Finally, the paper concludes with a\ndiscussion on the future directions of PVQA research.\n","authors":["Wei Zhou","Hadi Amirpour","Christian Timmerer","Guangtao Zhai","Patrick Le Callet","Alan C. Bovik"],"pdf_url":"https://arxiv.org/pdf/2503.00625v1.pdf","comment":"A tutorial and review"},{"id":"http://arxiv.org/abs/2503.00548v1","updated":"2025-03-01T16:31:02Z","published":"2025-03-01T16:31:02Z","title":"Unbiased Video Scene Graph Generation via Visual and Semantic Dual\n  Debiasing","summary":"  Video Scene Graph Generation (VidSGG) aims to capture dynamic relationships\namong entities by sequentially analyzing video frames and integrating visual\nand semantic information. However, VidSGG is challenged by significant biases\nthat skew predictions. To mitigate these biases, we propose a VIsual and\nSemantic Awareness (VISA) framework for unbiased VidSGG. VISA addresses visual\nbias through memory-enhanced temporal integration that enhances object\nrepresentations and concurrently reduces semantic bias by iteratively\nintegrating object features with comprehensive semantic information derived\nfrom triplet relationships. This visual-semantics dual debiasing approach\nresults in more unbiased representations of complex scene dynamics. Extensive\nexperiments demonstrate the effectiveness of our method, where VISA outperforms\nexisting unbiased VidSGG approaches by a substantial margin (e.g., +13.1%\nimprovement in mR@20 and mR@50 for the SGCLS task under Semi Constraint).\n","authors":["Yanjun Li","Zhaoyang Li","Honghui Chen","Lizhi Xu"],"pdf_url":"https://arxiv.org/pdf/2503.00548v1.pdf","comment":"17 pages, 8 figures, CVPR 2025"},{"id":"http://arxiv.org/abs/2503.00455v1","updated":"2025-03-01T11:35:17Z","published":"2025-03-01T11:35:17Z","title":"PodAgent: A Comprehensive Framework for Podcast Generation","summary":"  Existing Existing automatic audio generation methods struggle to generate\npodcast-like audio programs effectively. The key challenges lie in in-depth\ncontent generation, appropriate and expressive voice production. This paper\nproposed PodAgent, a comprehensive framework for creating audio programs.\nPodAgent 1) generates informative topic-discussion content by designing a\nHost-Guest-Writer multi-agent collaboration system, 2) builds a voice pool for\nsuitable voice-role matching and 3) utilizes LLM-enhanced speech synthesis\nmethod to generate expressive conversational speech. Given the absence of\nstandardized evaluation criteria for podcast-like audio generation, we\ndeveloped comprehensive assessment guidelines to effectively evaluate the\nmodel's performance. Experimental results demonstrate PodAgent's effectiveness,\nsignificantly surpassing direct GPT-4 generation in topic-discussion dialogue\ncontent, achieving an 87.4% voice-matching accuracy, and producing more\nexpressive speech through LLM-guided synthesis. Demo page:\nhttps://podcast-agent.github.io/demo/. Source code:\nhttps://github.com/yujxx/PodAgent.\n","authors":["Yujia Xiao","Lei He","Haohan Guo","Fenglong Xie","Tan Lee"],"pdf_url":"https://arxiv.org/pdf/2503.00455v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00374v1","updated":"2025-03-01T07:02:30Z","published":"2025-03-01T07:02:30Z","title":"MIRROR: Multi-Modal Pathological Self-Supervised Representation Learning\n  via Modality Alignment and Retention","summary":"  Histopathology and transcriptomics are fundamental modalities in oncology,\nencapsulating the morphological and molecular aspects of the disease.\nMulti-modal self-supervised learning has demonstrated remarkable potential in\nlearning pathological representations by integrating diverse data sources.\nConventional multi-modal integration methods primarily emphasize modality\nalignment, while paying insufficient attention to retaining the\nmodality-specific structures. However, unlike conventional scenarios where\nmulti-modal inputs share highly overlapping features, histopathology and\ntranscriptomics exhibit pronounced heterogeneity, offering orthogonal yet\ncomplementary insights. Histopathology provides morphological and spatial\ncontext, elucidating tissue architecture and cellular topology, whereas\ntranscriptomics delineates molecular signatures through gene expression\npatterns. This inherent disparity introduces a major challenge in aligning them\nwhile maintaining modality-specific fidelity. To address these challenges, we\npresent MIRROR, a novel multi-modal representation learning method designed to\nfoster both modality alignment and retention. MIRROR employs dedicated encoders\nto extract comprehensive features for each modality, which is further\ncomplemented by a modality alignment module to achieve seamless integration\nbetween phenotype patterns and molecular profiles. Furthermore, a modality\nretention module safeguards unique attributes from each modality, while a style\nclustering module mitigates redundancy and enhances disease-relevant\ninformation by modeling and aligning consistent pathological signatures within\na clustering space. Extensive evaluations on TCGA cohorts for cancer subtyping\nand survival analysis highlight MIRROR's superior performance, demonstrating\nits effectiveness in constructing comprehensive oncological feature\nrepresentations and benefiting the cancer diagnosis.\n","authors":["Tianyi Wang","Jianan Fan","Dingxin Zhang","Dongnan Liu","Yong Xia","Heng Huang","Weidong Cai"],"pdf_url":"https://arxiv.org/pdf/2503.00374v1.pdf","comment":"10 pages, 5 figures, 3 tables"}]},"2025-02-28T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2503.00238v1","updated":"2025-02-28T22:53:56Z","published":"2025-02-28T22:53:56Z","title":"Passage Query Methods for Retrieval and Reranking in Conversational\n  Agents","summary":"  This paper presents our approach to the TREC Interactive Knowledge Assistance\nTrack (iKAT), which focuses on improving conversational information-seeking\n(CIS) systems. While recent advancements in CIS have improved conversational\nagents' ability to assist users, significant challenges remain in understanding\ncontext and retrieving relevant documents across domains and dialogue turns. To\naddress these issues, we extend the Generate-Retrieve-Generate pipeline by\ndeveloping passage queries (PQs) that align with the target document's expected\nformat to improve query-document matching during retrieval. We propose two\nvariations of this approach: Weighted Reranking and Short and Long Passages.\nEach method leverages a Meta Llama model for context understanding and\ngenerating queries and responses. Passage ranking evaluation results show that\nthe Short and Long Passages approach outperformed the organizers' baselines,\nperformed best among Llama-based systems in the track, and achieved results\ncomparable to GPT-4-based systems. These results indicate that the method\neffectively balances efficiency and performance. Findings suggest that PQs\nimprove semantic alignment with target documents and demonstrate their\npotential to improve multi-turn dialogue systems.\n","authors":["Victor De Lima","Grace Hui Yang"],"pdf_url":"https://arxiv.org/pdf/2503.00238v1.pdf","comment":"7 pages, 3 figures. In Proceedings of the Thirty-Third Text Retrieval\n  Conference (TREC 2024), November 18-22, 2024, Rockville, MD, USA"},{"id":"http://arxiv.org/abs/2503.00223v1","updated":"2025-02-28T22:16:42Z","published":"2025-02-28T22:16:42Z","title":"DeepRetrieval: Powerful Query Generation for Information Retrieval with\n  Reinforcement Learning","summary":"  Information retrieval systems are crucial for enabling effective access to\nlarge document collections. Recent approaches have leveraged Large Language\nModels (LLMs) to enhance retrieval performance through query augmentation, but\noften rely on expensive supervised learning or distillation techniques that\nrequire significant computational resources and hand-labeled data. In this\npaper, we introduce DeepRetrieval, a novel reinforcement learning-based\napproach that trains LLMs to perform query augmentation directly through trial\nand error, without requiring supervised data. By using the retrieval recall as\na reward signal, our system learns to generate effective queries that maximize\ndocument retrieval performance. Our preliminary results demonstrate that\nDeepRetrieval significantly outperforms existing state-of-the-art methods,\nincluding the recent LEADS system, achieving 60.82\\% recall on publication\nsearch and 70.84\\% recall on trial search tasks while using a smaller model (3B\nvs. 7B parameters) and requiring no supervision data. These results suggest\nthat our reinforcement learning approach offers a more efficient and effective\nparadigm for information retrieval, potentially changing the landscape of\ndocument retrieval systems. code is available at\nhttps://github.com/pat-jj/DeepRetrieval.\n","authors":["Pengcheng Jiang"],"pdf_url":"https://arxiv.org/pdf/2503.00223v1.pdf","comment":"Ongoing work"},{"id":"http://arxiv.org/abs/2503.00179v1","updated":"2025-02-28T20:49:18Z","published":"2025-02-28T20:49:18Z","title":"Zero-Shot and Efficient Clarification Need Prediction in Conversational\n  Search","summary":"  Clarification need prediction (CNP) is a key task in conversational search,\naiming to predict whether to ask a clarifying question or give an answer to the\ncurrent user query. However, current research on CNP suffers from the issues of\nlimited CNP training data and low efficiency. In this paper, we propose a\nzero-shot and efficient CNP framework (Zef-CNP), in which we first prompt large\nlanguage models (LLMs) in a zero-shot manner to generate two sets of synthetic\nqueries: ambiguous and specific (unambiguous) queries. We then use the\ngenerated queries to train efficient CNP models. Zef-CNP eliminates the need\nfor human-annotated clarification-need labels during training and avoids the\nuse of LLMs with high query latency at query time. To further improve the\ngeneration quality of synthetic queries, we devise a topic-, information-need-,\nand query-aware chain-of-thought (CoT) prompting strategy (TIQ-CoT). Moreover,\nwe enhance TIQ-CoT with counterfactual query generation (CoQu), which guides\nLLMs first to generate a specific/ambiguous query and then sequentially\ngenerate its corresponding ambiguous/specific query. Experimental results show\nthat Zef-CNP achieves superior CNP effectiveness and efficiency compared with\nzero- and few-shot LLM-based CNP predictors.\n","authors":["Lili Lu","Chuan Meng","Federico Ravenda","Mohammad Aliannejadi","Fabio Crestani"],"pdf_url":"https://arxiv.org/pdf/2503.00179v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2410.02642v2","updated":"2025-02-28T19:49:30Z","published":"2024-10-03T16:25:37Z","title":"Attention in Large Language Models Yields Efficient Zero-Shot Re-Rankers","summary":"  Information retrieval (IR) systems have played a vital role in modern digital\nlife and have cemented their continued usefulness in this new era of generative\nAI via retrieval-augmented generation. With strong language processing\ncapabilities and remarkable versatility, large language models (LLMs) have\nbecome popular choices for zero-shot re-ranking in IR systems. So far,\nLLM-based re-ranking methods rely on strong generative capabilities, which\nrestricts their use to either specialized or powerful proprietary models. Given\nthese restrictions, we ask: is autoregressive generation necessary and optimal\nfor LLMs to perform re-ranking? We hypothesize that there are abundant signals\nrelevant to re-ranking within LLMs that might not be used to their full\npotential via generation. To more directly leverage such signals, we propose\nin-context re-ranking (ICR), a novel method that leverages the change in\nattention pattern caused by the search query for accurate and efficient\nre-ranking. To mitigate the intrinsic biases in LLMs, we propose a calibration\nmethod using a content-free query. Due to the absence of generation, ICR only\nrequires two ($O(1)$) forward passes to re-rank $N$ documents, making it\nsubstantially more efficient than generative re-ranking methods that require at\nleast $O(N)$ forward passes. Our novel design also enables ICR to be applied to\nany LLM without specialized training while guaranteeing a well-formed ranking.\nExtensive experiments with two popular open-weight LLMs on standard single-hop\nand multi-hop information retrieval benchmarks show that ICR outperforms\nRankGPT while cutting the latency by more than 60% in practice. Through\ndetailed analyses, we show that ICR's performance is specially strong on tasks\nthat require more complex re-ranking signals. Our findings call for further\nexploration on novel ways of utilizing open-weight LLMs beyond text generation.\n","authors":["Shijie Chen","Bernal Jiménez Gutiérrez","Yu Su"],"pdf_url":"https://arxiv.org/pdf/2410.02642v2.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.21195v1","updated":"2025-02-28T16:14:00Z","published":"2025-02-28T16:14:00Z","title":"Joint Modeling in Recommendations: A Survey","summary":"  In today's digital landscape, Deep Recommender Systems (DRS) play a crucial\nrole in navigating and customizing online content for individual preferences.\nHowever, conventional methods, which mainly depend on single recommendation\ntask, scenario, data modality and user behavior, are increasingly seen as\ninsufficient due to their inability to accurately reflect users' complex and\nchanging preferences. This gap underscores the need for joint modeling\napproaches, which are central to overcoming these limitations by integrating\ndiverse tasks, scenarios, modalities, and behaviors in the recommendation\nprocess, thus promising significant enhancements in recommendation precision,\nefficiency, and customization. In this paper, we comprehensively survey the\njoint modeling methods in recommendations. We begin by defining the scope of\njoint modeling through four distinct dimensions: multi-task, multi-scenario,\nmulti-modal, and multi-behavior modeling. Subsequently, we examine these\nmethods in depth, identifying and summarizing their underlying paradigms based\non the latest advancements and potential research trajectories. Ultimately, we\nhighlight several promising avenues for future exploration in joint modeling\nfor recommendations and provide a concise conclusion to our findings.\n","authors":["Xiangyu Zhao","Yichao Wang","Bo Chen","Jingtong Gao","Yuhao Wang","Xiaopeng Li","Pengyue Jia","Qidong Liu","Huifeng Guo","Ruiming Tang"],"pdf_url":"https://arxiv.org/pdf/2502.21195v1.pdf","comment":"arXiv admin note: text overlap with arXiv:2302.03525"},{"id":"http://arxiv.org/abs/2502.21112v1","updated":"2025-02-28T14:52:25Z","published":"2025-02-28T14:52:25Z","title":"Optimizing Large Language Models for ESG Activity Detection in Financial\n  Texts","summary":"  The integration of Environmental, Social, and Governance (ESG) factors into\ncorporate decision-making is a fundamental aspect of sustainable finance.\nHowever, ensuring that business practices align with evolving regulatory\nframeworks remains a persistent challenge. AI-driven solutions for\nautomatically assessing the alignment of sustainability reports and\nnon-financial disclosures with specific ESG activities could greatly support\nthis process. Yet, this task remains complex due to the limitations of\ngeneral-purpose Large Language Models (LLMs) in domain-specific contexts and\nthe scarcity of structured, high-quality datasets. In this paper, we\ninvestigate the ability of current-generation LLMs to identify text related to\nenvironmental activities. Furthermore, we demonstrate that their performance\ncan be significantly enhanced through fine-tuning on a combination of original\nand synthetically generated data. To this end, we introduce ESG-Activities, a\nbenchmark dataset containing 1,325 labelled text segments classified according\nto the EU ESG taxonomy. Our experimental results show that fine-tuning on\nESG-Activities significantly enhances classification accuracy, with open models\nsuch as Llama 7B and Gemma 7B outperforming large proprietary solutions in\nspecific configurations. These findings have important implications for\nfinancial analysts, policymakers, and AI researchers seeking to enhance ESG\ntransparency and compliance through advanced natural language processing\ntechniques.\n","authors":["Mattia Birti","Francesco Osborne","Andrea Maurino"],"pdf_url":"https://arxiv.org/pdf/2502.21112v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.21067v1","updated":"2025-02-28T14:03:04Z","published":"2025-02-28T14:03:04Z","title":"Fast 3D point clouds retrieval for Large-scale 3D Place Recognition","summary":"  Retrieval in 3D point clouds is a challenging task that consists in\nretrieving the most similar point clouds to a given query within a reference of\n3D points. Current methods focus on comparing descriptors of point clouds in\norder to identify similar ones. Due to the complexity of this latter step, here\nwe focus on the acceleration of the retrieval by adapting the Differentiable\nSearch Index (DSI), a transformer-based approach initially designed for text\ninformation retrieval, for 3D point clouds retrieval. Our approach generates 1D\nidentifiers based on the point descriptors, enabling direct retrieval in\nconstant time. To adapt DSI to 3D data, we integrate Vision Transformers to map\ndescriptors to these identifiers while incorporating positional and semantic\nencoding. The approach is evaluated for place recognition on a public benchmark\ncomparing its retrieval capabilities against state-of-the-art methods, in terms\nof quality and speed of returned point clouds.\n","authors":["Chahine-Nicolas Zede","Laurent Carrafa","Valérie Gouet-Brunet"],"pdf_url":"https://arxiv.org/pdf/2502.21067v1.pdf","comment":"8 pages, 1 figures"},{"id":"http://arxiv.org/abs/2411.12064v2","updated":"2025-02-28T13:56:56Z","published":"2024-11-18T21:10:14Z","title":"TSPRank: Bridging Pairwise and Listwise Methods with a Bilinear\n  Travelling Salesman Model","summary":"  Traditional Learning-To-Rank (LETOR) approaches, including pairwise methods\nlike RankNet and LambdaMART, often fall short by solely focusing on pairwise\ncomparisons, leading to sub-optimal global rankings. Conversely, deep learning\nbased listwise methods, while aiming to optimise entire lists, require complex\ntuning and yield only marginal improvements over robust pairwise models. To\novercome these limitations, we introduce Travelling Salesman Problem Rank\n(TSPRank), a hybrid pairwise-listwise ranking method. TSPRank reframes the\nranking problem as a Travelling Salesman Problem (TSP), a well-known\ncombinatorial optimisation challenge that has been extensively studied for its\nnumerous solution algorithms and applications. This approach enables the\nmodelling of pairwise relationships and leverages combinatorial optimisation to\ndetermine the listwise ranking. This approach can be directly integrated as an\nadditional component into embeddings generated by existing backbone models to\nenhance ranking performance. Our extensive experiments across three backbone\nmodels on diverse tasks, including stock ranking, information retrieval, and\nhistorical events ordering, demonstrate that TSPRank significantly outperforms\nboth pure pairwise and listwise methods. Our qualitative analysis reveals that\nTSPRank's main advantage over existing methods is its ability to harness global\ninformation better while ranking. TSPRank's robustness and superior performance\nacross different domains highlight its potential as a versatile and effective\nLETOR solution.\n","authors":["Weixian Waylon Li","Yftah Ziser","Yifei Xie","Shay B. Cohen","Tiejun Ma"],"pdf_url":"https://arxiv.org/pdf/2411.12064v2.pdf","comment":"Accepted to ACM SIGKDD 2025 Research Track. The code and preprocessed\n  data are available at https://github.com/waylonli/TSPRank-KDD2025"},{"id":"http://arxiv.org/abs/2502.21024v1","updated":"2025-02-28T13:06:25Z","published":"2025-02-28T13:06:25Z","title":"Extending Dense Passage Retrieval with Temporal Information","summary":"  Temporal awareness is crucial in many information retrieval tasks,\nparticularly in scenarios where the relevance of documents depends on their\nalignment with the query's temporal context. Traditional retrieval methods such\nas BM25 and Dense Passage Retrieval (DPR) excel at capturing lexical and\nsemantic relevance but fall short in addressing time-sensitive queries. To\nbridge this gap, we introduce the temporal retrieval model that integrates\nexplicit temporal signals by incorporating query timestamps and document dates\ninto the representation space. Our approach ensures that retrieved passages are\nnot only topically relevant but also temporally aligned with user intent. We\nevaluate our approach on two large-scale benchmark datasets, ArchivalQA and\nChroniclingAmericaQA, achieving substantial performance gains over standard\nretrieval baselines. In particular, our model improves Top-1 retrieval accuracy\nby 6.63% and NDCG@10 by 3.79% on ArchivalQA, while yielding a 9.56% boost in\nTop-1 retrieval accuracy and 4.68% in NDCG@10 on ChroniclingAmericaQA.\nAdditionally, we introduce a time-sensitive negative sampling strategy, which\nrefines the model's ability to distinguish between temporally relevant and\nirrelevant documents during training. Our findings highlight the importance of\nexplicitly modeling time in retrieval systems and set a new standard for\nhandling temporally grounded queries.\n","authors":["Abdelrahman Abdallah","Bhawna Piryani","Jonas Wallat","Avishek Anand","Adam Jatowt"],"pdf_url":"https://arxiv.org/pdf/2502.21024v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2401.17723v2","updated":"2025-02-28T12:56:52Z","published":"2024-01-31T10:35:53Z","title":"LoRec: Large Language Model for Robust Sequential Recommendation against\n  Poisoning Attacks","summary":"  Sequential recommender systems stand out for their ability to capture users'\ndynamic interests and the patterns of item-to-item transitions. However, the\ninherent openness of sequential recommender systems renders them vulnerable to\npoisoning attacks, where fraudulent users are injected into the training data\nto manipulate learned patterns. Traditional defense strategies predominantly\ndepend on predefined assumptions or rules extracted from specific known\nattacks, limiting their generalizability to unknown attack types. To solve the\nabove problems, considering the rich open-world knowledge encapsulated in Large\nLanguage Models (LLMs), our research initially focuses on the capabilities of\nLLMs in the detection of unknown fraudulent activities within recommender\nsystems, a strategy we denote as LLM4Dec. Empirical evaluations demonstrate the\nsubstantial capability of LLMs in identifying unknown fraudsters, leveraging\ntheir expansive, open-world knowledge.\n  Building upon this, we propose the integration of LLMs into defense\nstrategies to extend their effectiveness beyond the confines of known attacks.\nWe propose LoRec, an advanced framework that employs LLM-Enhanced Calibration\nto strengthen the robustness of sequential recommender systems against\npoisoning attacks. LoRec integrates an LLM-enhanced CalibraTor (LCT) that\nrefines the training process of sequential recommender systems with knowledge\nderived from LLMs, applying a user-wise reweighting to diminish the impact of\nfraudsters injected by attacks. By incorporating LLMs' open-world knowledge,\nthe LCT effectively converts the limited, specific priors or rules into a more\ngeneral pattern of fraudsters, offering improved defenses against poisoning\nattacks. Our comprehensive experiments validate that LoRec, as a general\nframework, significantly strengthens the robustness of sequential recommender\nsystems.\n","authors":["Kaike Zhang","Qi Cao","Yunfan Wu","Fei Sun","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2401.17723v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20995v1","updated":"2025-02-28T12:32:53Z","published":"2025-02-28T12:32:53Z","title":"The RAG Paradox: A Black-Box Attack Exploiting Unintentional\n  Vulnerabilities in Retrieval-Augmented Generation Systems","summary":"  With the growing adoption of retrieval-augmented generation (RAG) systems,\nrecent studies have introduced attack methods aimed at degrading their\nperformance. However, these methods rely on unrealistic white-box assumptions,\nsuch as attackers having access to RAG systems' internal processes. To address\nthis issue, we introduce a realistic black-box attack scenario based on the RAG\nparadox, where RAG systems inadvertently expose vulnerabilities while\nattempting to enhance trustworthiness. Because RAG systems reference external\ndocuments during response generation, our attack targets these sources without\nrequiring internal access. Our approach first identifies the external sources\ndisclosed by RAG systems and then automatically generates poisoned documents\nwith misinformation designed to match these sources. Finally, these poisoned\ndocuments are newly published on the disclosed sources, disrupting the RAG\nsystem's response generation process. Both offline and online experiments\nconfirm that this attack significantly reduces RAG performance without\nrequiring internal access. Furthermore, from an insider perspective within the\nRAG system, we propose a re-ranking method that acts as a fundamental\nsafeguard, offering minimal protection against unforeseen attacks.\n","authors":["Chanwoo Choi","Jinsoo Kim","Sukmin Cho","Soyeong Jeong","Buru Chang"],"pdf_url":"https://arxiv.org/pdf/2502.20995v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2501.08248v2","updated":"2025-02-28T11:40:20Z","published":"2025-01-14T16:38:33Z","title":"Eliciting In-context Retrieval and Reasoning for Long-context Large\n  Language Models","summary":"  Recent advancements in long-context language models (LCLMs) promise to\ntransform Retrieval-Augmented Generation (RAG) by simplifying pipelines. With\ntheir expanded context windows, LCLMs can process entire knowledge bases and\nperform retrieval and reasoning directly -- a capability we define as\nIn-Context Retrieval and Reasoning (ICR^2). However, existing benchmarks like\nLOFT often overestimate LCLM performance by providing overly simplified\ncontexts. To address this, we introduce ICR^2, a benchmark that evaluates LCLMs\nin more realistic scenarios by including confounding passages retrieved with\nstrong retrievers. We then propose three methods to enhance LCLM performance:\n(1) retrieve-then-generate fine-tuning, (2) retrieval-attention-probing, which\nuses attention heads to filter and de-noise long contexts during decoding, and\n(3) joint retrieval head training alongside the generation head. Our evaluation\nof five well-known LCLMs on LOFT and ICR^2 demonstrates significant gains with\nour best approach applied to Mistral-7B: +17 and +15 points by Exact Match on\nLOFT, and +13 and +2 points on ICR^2, compared to vanilla RAG and supervised\nfine-tuning, respectively. It even outperforms GPT-4-Turbo on most tasks\ndespite being a much smaller model.\n","authors":["Yifu Qiu","Varun Embar","Yizhe Zhang","Navdeep Jaitly","Shay B. Cohen","Benjamin Han"],"pdf_url":"https://arxiv.org/pdf/2501.08248v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20937v1","updated":"2025-02-28T10:46:56Z","published":"2025-02-28T10:46:56Z","title":"Variations in Relevance Judgments and the Shelf Life of Test Collections","summary":"  The fundamental property of Cranfield-style evaluations, that system rankings\nare stable even when assessors disagree on individual relevance decisions, was\nvalidated on traditional test collections. However, the paradigm shift towards\nneural retrieval models affected the characteristics of modern test\ncollections, e.g., documents are short, judged with four grades of relevance,\nand information needs have no descriptions or narratives. Under these changes,\nit is unclear whether assessor disagreement remains negligible for system\ncomparisons. We investigate this aspect under the additional condition that the\nfew modern test collections are heavily re-used. Given more possible query\ninterpretations due to less formalized information needs, an ''expiration\ndate'' for test collections might be needed if top-effectiveness requires\noverfitting to a single interpretation of relevance. We run a reproducibility\nstudy and re-annotate the relevance judgments of the 2019 TREC Deep Learning\ntrack. We can reproduce prior work in the neural retrieval setting, showing\nthat assessor disagreement does not affect system rankings. However, we observe\nthat some models substantially degrade with our new relevance judgments, and\nsome have already reached the effectiveness of humans as rankers, providing\nevidence that test collections can expire.\n","authors":["Andrew Parry","Maik Fröbe","Harrisen Scells","Ferdinand Schlatt","Guglielmo Faggioli","Saber Zerhoudi","Sean MacAvaney","Eugene Yang"],"pdf_url":"https://arxiv.org/pdf/2502.20937v1.pdf","comment":"11 pages, 6 tables, 5 figures"},{"id":"http://arxiv.org/abs/2502.20936v1","updated":"2025-02-28T10:46:52Z","published":"2025-02-28T10:46:52Z","title":"WebFAQ: A Multilingual Collection of Natural Q&A Datasets for Dense\n  Retrieval","summary":"  We present WebFAQ, a large-scale collection of open-domain question answering\ndatasets derived from FAQ-style schema.org annotations. In total, the data\ncollection consists of 96 million natural question-answer (QA) pairs across 75\nlanguages, including 47 million (49%) non-English samples. WebFAQ further\nserves as the foundation for 20 monolingual retrieval benchmarks with a total\nsize of 11.2 million QA pairs (5.9 million non-English). These datasets are\ncarefully curated through refined filtering and near-duplicate detection,\nyielding high-quality resources for training and evaluating multilingual dense\nretrieval models. To empirically confirm WebFAQ's efficacy, we use the\ncollected QAs to fine-tune an in-domain pretrained XLM-RoBERTa model. Through\nthis process of dataset-specific fine-tuning, the model achieves significant\nretrieval performance gains, which generalize - beyond WebFAQ - to other\nmultilingual retrieval benchmarks evaluated in zero-shot setting. Last but not\nleast, we utilize WebFAQ to construct a set of QA-aligned bilingual corpora\nspanning over 1000 language pairs using state-of-the-art bitext mining and\nautomated LLM-assessed translation evaluation. Due to our advanced, automated\nmethod of bitext dataset generation, the resulting bilingual corpora\ndemonstrate higher translation quality compared to similar datasets. WebFAQ and\nall associated resources are publicly available on GitHub and HuggingFace.\n","authors":["Michael Dinzinger","Laura Caspari","Kanishka Ghosh Dastidar","Jelena Mitrović","Michael Granitzer"],"pdf_url":"https://arxiv.org/pdf/2502.20936v1.pdf","comment":"10 pages, 3 figures, 7 tables"},{"id":"http://arxiv.org/abs/2407.01449v6","updated":"2025-02-28T08:51:57Z","published":"2024-06-27T15:45:29Z","title":"ColPali: Efficient Document Retrieval with Vision Language Models","summary":"  Documents are visually rich structures that convey information through text,\nbut also figures, page layouts, tables, or even fonts. Since modern retrieval\nsystems mainly rely on the textual information they extract from document pages\nto index documents -often through lengthy and brittle processes-, they struggle\nto exploit key visual cues efficiently. This limits their capabilities in many\npractical document retrieval applications such as Retrieval Augmented\nGeneration (RAG). To benchmark current systems on visually rich document\nretrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe,\ncomposed of various page-level retrieval tasks spanning multiple domains,\nlanguages, and practical settings. The inherent complexity and performance\nshortcomings of modern systems motivate a new concept; doing document retrieval\nby directly embedding the images of the document pages. We release ColPali, a\nVision Language Model trained to produce high-quality multi-vector embeddings\nfrom images of document pages. Combined with a late interaction matching\nmechanism, ColPali largely outperforms modern document retrieval pipelines\nwhile being drastically simpler, faster and end-to-end trainable. We release\nmodels, data, code and benchmarks under open licenses at https://hf.co/vidore.\n","authors":["Manuel Faysse","Hugues Sibille","Tony Wu","Bilel Omrani","Gautier Viaud","Céline Hudelot","Pierre Colombo"],"pdf_url":"https://arxiv.org/pdf/2407.01449v6.pdf","comment":"Published as a conference paper at ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20826v1","updated":"2025-02-28T08:12:23Z","published":"2025-02-28T08:12:23Z","title":"CoTMR: Chain-of-Thought Multi-Scale Reasoning for Training-Free\n  Zero-Shot Composed Image Retrieval","summary":"  Zero-Shot Composed Image Retrieval (ZS-CIR) aims to retrieve target images by\nintegrating information from a composed query (reference image and modification\ntext) without training samples. Existing methods primarily combine caption\nmodels and large language models (LLMs) to generate target captions based on\ncomposed queries but face various issues such as incompatibility, visual\ninformation loss, and insufficient reasoning. In this work, we propose CoTMR, a\ntraining-free framework crafted for ZS-CIR with novel Chain-of-thought (CoT)\nand Multi-scale Reasoning. Instead of relying on caption models for modality\ntransformation, CoTMR employs the Large Vision-Language Model (LVLM) to achieve\nunified understanding and reasoning for composed queries. To enhance the\nreasoning reliability, we devise CIRCoT, which guides the LVLM through a\nstep-by-step inference process using predefined subtasks. Considering that\nexisting approaches focus solely on global-level reasoning, our CoTMR\nincorporates multi-scale reasoning to achieve more comprehensive inference via\nfine-grained predictions about the presence or absence of key elements at the\nobject scale. Further, we design a Multi-Grained Scoring (MGS) mechanism, which\nintegrates CLIP similarity scores of the above reasoning outputs with candidate\nimages to realize precise retrieval. Extensive experiments demonstrate that our\nCoTMR not only drastically outperforms previous methods across four prominent\nbenchmarks but also offers appealing interpretability.\n","authors":["Zelong Sun","Dong Jing","Zhiwu Lu"],"pdf_url":"https://arxiv.org/pdf/2502.20826v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20695v1","updated":"2025-02-28T04:03:23Z","published":"2025-02-28T04:03:23Z","title":"Scalable Overload-Aware Graph-Based Index Construction for\n  10-Billion-Scale Vector Similarity Search","summary":"  Approximate Nearest Neighbor Search (ANNS) is essential for modern\ndata-driven applications that require efficient retrieval of top-k results from\nmassive vector databases. Although existing graph-based ANNS algorithms achieve\na high recall rate on billion-scale datasets, their slow construction speed and\nlimited scalability hinder their applicability to large-scale industrial\nscenarios. In this paper, we introduce SOGAIC, the first Scalable\nOverload-Aware Graph-Based ANNS Index Construction system tailored for\nultra-large-scale vector databases: 1) We propose a dynamic data partitioning\nalgorithm with overload constraints that adaptively introduces overlaps among\nsubsets; 2) To enable efficient distributed subgraph construction, we employ a\nload-balancing task scheduling framework combined with an agglomerative merging\nstrategy; 3) Extensive experiments on various datasets demonstrate a reduction\nof 47.3% in average construction time compared to existing methods. The\nproposed method has also been successfully deployed in a real-world industrial\nsearch engine, managing over 10 billion daily updated vectors and serving\nhundreds of millions of users.\n","authors":["Yang Shi","Yiping Sun","Jiaolong Du","Xiaocheng Zhong","Zhiyong Wang","Yao Hu"],"pdf_url":"https://arxiv.org/pdf/2502.20695v1.pdf","comment":"Accepted by WWW'25"},{"id":"http://arxiv.org/abs/2502.20687v1","updated":"2025-02-28T03:40:37Z","published":"2025-02-28T03:40:37Z","title":"Unleashing the Potential of Two-Tower Models: Diffusion-Based\n  Cross-Interaction for Large-Scale Matching","summary":"  Two-tower models are widely adopted in the industrial-scale matching stage\nacross a broad range of application domains, such as content recommendations,\nadvertisement systems, and search engines. This model efficiently handles\nlarge-scale candidate item screening by separating user and item\nrepresentations. However, the decoupling network also leads to a neglect of\npotential information interaction between the user and item representations.\nCurrent state-of-the-art (SOTA) approaches include adding a shallow fully\nconnected layer(i.e., COLD), which is limited by performance and can only be\nused in the ranking stage. For performance considerations, another approach\nattempts to capture historical positive interaction information from the other\ntower by regarding them as the input features(i.e., DAT). Later research showed\nthat the gains achieved by this method are still limited because of lacking the\nguidance on the next user intent. To address the aforementioned challenges, we\npropose a \"cross-interaction decoupling architecture\" within our matching\nparadigm. This user-tower architecture leverages a diffusion module to\nreconstruct the next positive intention representation and employs a\nmixed-attention module to facilitate comprehensive cross-interaction. During\nthe next positive intention generation, we further enhance the accuracy of its\nreconstruction by explicitly extracting the temporal drift within user behavior\nsequences. Experiments on two real-world datasets and one industrial dataset\ndemonstrate that our method outperforms the SOTA two-tower models\nsignificantly, and our diffusion approach outperforms other generative models\nin reconstructing item representations.\n","authors":["Yihan Wang","Fei Xiong","Zhexin Han","Qi Song","Kaiqiao Zhan","Ben Wang"],"pdf_url":"https://arxiv.org/pdf/2502.20687v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.00944v3","updated":"2025-02-28T03:23:52Z","published":"2024-06-03T02:56:14Z","title":"A Theory for Token-Level Harmonization in Retrieval-Augmented Generation","summary":"  Retrieval-augmented generation (RAG) utilizes retrieved texts to enhance\nlarge language models (LLMs). Studies show that while RAG provides valuable\nexternal information (benefit), it may also mislead LLMs (detriment) with noisy\nor incorrect retrieved texts. Although many existing methods attempt to\npreserve benefit and avoid detriment, they lack a theoretical explanation for\nRAG. The benefit and detriment in the next token prediction of RAG remain a\nblack box that cannot be quantified or compared in an explainable manner, so\nexisting methods are data-driven, need additional utility evaluators or\npost-hoc. This paper takes the first step towards providing a theory to explain\nand trade off the benefit and detriment in RAG. First, we model RAG as the\nfusion between distribution of LLMs knowledge and distribution of retrieved\ntexts. Then, we formalize the trade-off between the value of external knowledge\n(benefit) and its potential risk of misleading LLMs (detriment) in next token\nprediction of RAG by distribution difference in this fusion. Finally, we prove\nthat the actual effect of RAG on the token, which is the comparison between\nbenefit and detriment, can be predicted without any training or accessing the\nutility of retrieval. Based on our theory, we propose a practical novel method,\nTok-RAG, which achieves collaborative generation between the pure LLM and RAG\nat token level to preserve benefit and avoid detriment. Experiments in\nreal-world tasks using LLMs such as OPT, LLaMA-2, and Mistral show the\neffectiveness of our method and support our theoretical findings.\n","authors":["Shicheng Xu","Liang Pang","Huawei Shen","Xueqi Cheng"],"pdf_url":"https://arxiv.org/pdf/2406.00944v3.pdf","comment":"ICLR 2025"},{"id":"http://arxiv.org/abs/2502.20640v1","updated":"2025-02-28T01:46:32Z","published":"2025-02-28T01:46:32Z","title":"LexRAG: Benchmarking Retrieval-Augmented Generation in Multi-Turn Legal\n  Consultation Conversation","summary":"  Retrieval-augmented generation (RAG) has proven highly effective in improving\nlarge language models (LLMs) across various domains. However, there is no\nbenchmark specifically designed to assess the effectiveness of RAG in the legal\ndomain, which restricts progress in this area. To fill this gap, we propose\nLexRAG, the first benchmark to evaluate RAG systems for multi-turn legal\nconsultations. LexRAG consists of 1,013 multi-turn dialogue samples and 17,228\ncandidate legal articles. Each sample is annotated by legal experts and\nconsists of five rounds of progressive questioning. LexRAG includes two key\ntasks: (1) Conversational knowledge retrieval, requiring accurate retrieval of\nrelevant legal articles based on multi-turn context. (2) Response generation,\nfocusing on producing legally sound answers. To ensure reliable\nreproducibility, we develop LexiT, a legal RAG toolkit that provides a\ncomprehensive implementation of RAG system components tailored for the legal\ndomain. Additionally, we introduce an LLM-as-a-judge evaluation pipeline to\nenable detailed and effective assessment. Through experimental analysis of\nvarious LLMs and retrieval methods, we reveal the key limitations of existing\nRAG systems in handling legal consultation conversations. LexRAG establishes a\nnew benchmark for the practical application of RAG systems in the legal domain,\nwith its code and data available at https://github.com/CSHaitao/LexRAG.\n","authors":["Haitao Li","Yifan Chen","Yiran Hu","Qingyao Ai","Junjie Chen","Xiaoyu Yang","Jianhui Yang","Yueyue Wu","Zeyang Liu","Yiqun Liu"],"pdf_url":"https://arxiv.org/pdf/2502.20640v1.pdf","comment":"10 pages"}],"Multimedia":[{"id":"http://arxiv.org/abs/2502.20904v1","updated":"2025-02-28T10:01:39Z","published":"2025-02-28T10:01:39Z","title":"DiffBrush:Just Painting the Art by Your Hands","summary":"  The rapid development of image generation and editing algorithms in recent\nyears has enabled ordinary user to produce realistic images. However, the\ncurrent AI painting ecosystem predominantly relies on text-driven diffusion\nmodels (T2I), which pose challenges in accurately capturing user requirements.\nFurthermore, achieving compatibility with other modalities incurs substantial\ntraining costs. To this end, we introduce DiffBrush, which is compatible with\nT2I models and allows users to draw and edit images. By manipulating and\nadapting the internal representation of the diffusion model, DiffBrush guides\nthe model-generated images to converge towards the user's hand-drawn sketches\nfor user's specific needs without additional training. DiffBrush achieves\ncontrol over the color, semantic, and instance of objects in images by\ncontinuously guiding the latent and instance-level attention map during the\ndenoising process of the diffusion model. Besides, we propose a latent\nregeneration, which refines the randomly sampled noise in the diffusion model,\nobtaining a better image generation layout. Finally, users only need to roughly\ndraw the mask of the instance (acceptable colors) on the canvas, DiffBrush can\nnaturally generate the corresponding instance at the corresponding location.\n","authors":["Jiaming Chu","Lei Jin","Tao Wang","Junliang Xing","Jian Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.20904v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20858v1","updated":"2025-02-28T09:01:30Z","published":"2025-02-28T09:01:30Z","title":"EyEar: Learning Audio Synchronized Human Gaze Trajectory Based on\n  Physics-Informed Dynamics","summary":"  Imitating how humans move their gaze in a visual scene is a vital research\nproblem for both visual understanding and psychology, kindling crucial\napplications such as building alive virtual characters. Previous studies aim to\npredict gaze trajectories when humans are free-viewing an image, searching for\nrequired targets, or looking for clues to answer questions in an image. While\nthese tasks focus on visual-centric scenarios, humans move their gaze also\nalong with audio signal inputs in more common scenarios. To fill this gap, we\nintroduce a new task that predicts human gaze trajectories in a visual scene\nwith synchronized audio inputs and provide a new dataset containing 20k gaze\npoints from 8 subjects. To effectively integrate audio information and simulate\nthe dynamic process of human gaze motion, we propose a novel learning framework\ncalled EyEar (Eye moving while Ear listening) based on physics-informed\ndynamics, which considers three key factors to predict gazes: eye inherent\nmotion tendency, vision salient attraction, and audio semantic attraction. We\nalso propose a probability density score to overcome the high individual\nvariability of gaze trajectories, thereby improving the stabilization of\noptimization and the reliability of the evaluation. Experimental results show\nthat EyEar outperforms all the baselines in the context of all evaluation\nmetrics, thanks to the proposed components in the learning model.\n","authors":["Xiaochuan Liu","Xin Cheng","Yuchong Sun","Xiaoxue Wu","Ruihua Song","Hao Sun","Denghao Zhang"],"pdf_url":"https://arxiv.org/pdf/2502.20858v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20811v1","updated":"2025-02-28T07:53:40Z","published":"2025-02-28T07:53:40Z","title":"HAIC: Improving Human Action Understanding and Generation with Better\n  Captions for Multi-modal Large Language Models","summary":"  Recent Multi-modal Large Language Models (MLLMs) have made great progress in\nvideo understanding. However, their performance on videos involving human\nactions is still limited by the lack of high-quality data. To address this, we\nintroduce a two-stage data annotation pipeline. First, we design strategies to\naccumulate videos featuring clear human actions from the Internet. Second,\nvideos are annotated in a standardized caption format that uses human\nattributes to distinguish individuals and chronologically details their actions\nand interactions. Through this pipeline, we curate two datasets, namely\nHAICTrain and HAICBench. \\textbf{HAICTrain} comprises 126K video-caption pairs\ngenerated by Gemini-Pro and verified for training purposes. Meanwhile,\n\\textbf{HAICBench} includes 500 manually annotated video-caption pairs and\n1,400 QA pairs, for a comprehensive evaluation of human action understanding.\nExperimental results demonstrate that training with HAICTrain not only\nsignificantly enhances human understanding abilities across 4 benchmarks, but\ncan also improve text-to-video generation results. Both the HAICTrain and\nHAICBench are released at https://huggingface.co/datasets/KuaishouHAIC/HAIC.\n","authors":["Xiao Wang","Jingyun Hua","Weihong Lin","Yuanxing Zhang","Fuzheng Zhang","Jianlong Wu","Di Zhang","Liqiang Nie"],"pdf_url":"https://arxiv.org/pdf/2502.20811v1.pdf","comment":null}]},"2025-02-27T00:00:00Z":{"Information Retrieval":[{"id":"http://arxiv.org/abs/2502.20582v1","updated":"2025-02-27T22:48:35Z","published":"2025-02-27T22:48:35Z","title":"CS-PaperSum: A Large-Scale Dataset of AI-Generated Summaries for\n  Scientific Papers","summary":"  The rapid expansion of scientific literature in computer science presents\nchallenges in tracking research trends and extracting key insights. Existing\ndatasets provide metadata but lack structured summaries that capture core\ncontributions and methodologies. We introduce CS-PaperSum, a large-scale\ndataset of 91,919 papers from 31 top-tier computer science conferences,\nenriched with AI-generated structured summaries using ChatGPT. To assess\nsummary quality, we conduct embedding alignment analysis and keyword overlap\nanalysis, demonstrating strong preservation of key concepts. We further present\na case study on AI research trends, highlighting shifts in methodologies and\ninterdisciplinary crossovers, including the rise of self-supervised learning,\nretrieval-augmented generation, and multimodal AI. Our dataset enables\nautomated literature analysis, research trend forecasting, and AI-driven\nscientific discovery, providing a valuable resource for researchers,\npolicymakers, and scientific information retrieval systems.\n","authors":["Javin Liu","Aryan Vats","Zihao He"],"pdf_url":"https://arxiv.org/pdf/2502.20582v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20541v1","updated":"2025-02-27T21:40:22Z","published":"2025-02-27T21:40:22Z","title":"NANOGPT: A Query-Driven Large Language Model Retrieval-Augmented\n  Generation System for Nanotechnology Research","summary":"  This paper presents the development and application of a Large Language Model\nRetrieval-Augmented Generation (LLM-RAG) system tailored for nanotechnology\nresearch. The system leverages the capabilities of a sophisticated language\nmodel to serve as an intelligent research assistant, enhancing the efficiency\nand comprehensiveness of literature reviews in the nanotechnology domain.\nCentral to this LLM-RAG system is its advanced query backend retrieval\nmechanism, which integrates data from multiple reputable sources. The system\nretrieves relevant literature by utilizing Google Scholar's advanced search,\nand scraping open-access papers from Elsevier, Springer Nature, and ACS\nPublications. This multifaceted approach ensures a broad and diverse collection\nof up-to-date scholarly articles and papers. The proposed system demonstrates\nsignificant potential in aiding researchers by providing a streamlined,\naccurate, and exhaustive literature retrieval process, thereby accelerating\nresearch advancements in nanotechnology. The effectiveness of the LLM-RAG\nsystem is validated through rigorous testing, illustrating its capability to\nsignificantly reduce the time and effort required for comprehensive literature\nreviews, while maintaining high accuracy, query relevance and outperforming\nstandard, publicly available LLMS.\n","authors":["Achuth Chandrasekhar","Omid Barati Farimani","Olabode T. Ajenifujah","Janghoon Ock","Amir Barati Farimani"],"pdf_url":"https://arxiv.org/pdf/2502.20541v1.pdf","comment":"61 pages, 3 figures"},{"id":"http://arxiv.org/abs/2502.20315v1","updated":"2025-02-27T17:41:49Z","published":"2025-02-27T17:41:49Z","title":"LangProBe: a Language Programs Benchmark","summary":"  Composing language models (LMs) into multi-step language programs and\nautomatically optimizing their modular prompts is now a mainstream paradigm for\nbuilding AI systems, but the tradeoffs in this space have only scarcely been\nstudied before. We introduce LangProBe, the first large-scale benchmark for\nevaluating the architectures and optimization strategies for language programs,\nwith over 2000 combinations of tasks, architectures, optimizers, and choices of\nLMs. Using LangProBe, we are the first to study the impact of program\narchitectures and optimizers (and their compositions together and with\ndifferent models) on tradeoffs of quality and cost. We find that optimized\nlanguage programs offer strong cost--quality Pareto improvement over raw calls\nto models, but simultaneously demonstrate that human judgment (or empirical\ndecisions) about which compositions to pursue is still necessary for best\nperformance. We will open source the code and evaluation data for LangProBe.\n","authors":["Shangyin Tan","Lakshya A Agrawal","Arnav Singhvi","Liheng Lai","Michael J Ryan","Dan Klein","Omar Khattab","Koushik Sen","Matei Zaharia"],"pdf_url":"https://arxiv.org/pdf/2502.20315v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2503.00072v1","updated":"2025-02-27T17:29:53Z","published":"2025-02-27T17:29:53Z","title":"Enhancing Collaborative Filtering-Based Course Recommendations by\n  Exploiting Time-to-Event Information with Survival Analysis","summary":"  Massive Open Online Courses (MOOCs) are emerging as a popular alternative to\ntraditional education, offering learners the flexibility to access a wide range\nof courses from various disciplines, anytime and anywhere. Despite this\naccessibility, a significant number of enrollments in MOOCs result in dropouts.\nTo enhance learner engagement, it is crucial to recommend courses that align\nwith their preferences and needs. Course Recommender Systems (RSs) can play an\nimportant role in this by modeling learners' preferences based on their\nprevious interactions within the MOOC platform. Time-to-dropout and\ntime-to-completion in MOOCs, like other time-to-event prediction tasks, can be\neffectively modeled using survival analysis (SA) methods. In this study, we\napply SA methods to improve collaborative filtering recommendation performance\nby considering time-to-event in the context of MOOCs. Our proposed approach\ndemonstrates superior performance compared to collaborative filtering methods\ntrained based on learners' interactions with MOOCs, as evidenced by two\nperformance measures on three publicly available datasets. The findings\nunderscore the potential of integrating SA methods with RSs to enhance\npersonalization in MOOCs.\n","authors":["Alireza Gharahighehi","Achilleas Ghinis","Michela Venturini","Frederik Cornillie","Celine Vens"],"pdf_url":"https://arxiv.org/pdf/2503.00072v1.pdf","comment":"19 pages, 1 figure"},{"id":"http://arxiv.org/abs/2404.06470v2","updated":"2025-02-27T17:26:00Z","published":"2024-04-09T17:17:48Z","title":"A Dataset and Framework for Learning State-invariant Object\n  Representations","summary":"  We add one more invariance - the state invariance - to the more commonly used\nother invariances for learning object representations for recognition and\nretrieval. By state invariance, we mean robust with respect to changes in the\nstructural form of the objects, such as when an umbrella is folded, or when an\nitem of clothing is tossed on the floor. In this work, we present a novel\ndataset, ObjectsWithStateChange, which captures state and pose variations in\nthe object images recorded from arbitrary viewpoints. We believe that this\ndataset will facilitate research in fine-grained object recognition and\nretrieval of 3D objects that are capable of state changes. The goal of such\nresearch would be to train models capable of learning discriminative object\nembeddings that remain invariant to state changes while also staying invariant\nto transformations induced by changes in viewpoint, pose, illumination, etc. A\nmajor challenge in this regard is that instances of different objects (both\nwithin and across different categories) under various state changes may share\nsimilar visual characteristics and therefore may be close to one another in the\nlearned embedding space, which would make it more difficult to discriminate\nbetween them. To address this, we propose a curriculum learning strategy that\nprogressively selects object pairs with smaller inter-object distances in the\nlearned embedding space during the training phase. This approach gradually\nsamples harder-to-distinguish examples of visually similar objects, both within\nand across different categories. Our ablation related to the role played by\ncurriculum learning indicates an improvement in object recognition accuracy of\n7.9% and retrieval mAP of 9.2% over the state-of-the-art on our new dataset, as\nwell as three other challenging multi-view datasets such as ModelNet40,\nObjectPI, and FG3D.\n","authors":["Rohan Sarkar","Avinash Kak"],"pdf_url":"https://arxiv.org/pdf/2404.06470v2.pdf","comment":"This work has been submitted to the IEEE for possible publication"},{"id":"http://arxiv.org/abs/2311.01314v3","updated":"2025-02-27T16:36:08Z","published":"2023-11-02T15:31:12Z","title":"Recommendations by Concise User Profiles from Review Text","summary":"  Recommender systems perform well for popular items and users with ample\ninteractions (likes, ratings etc.). This work addresses the difficult and\nunderexplored case of users who have very sparse interactions but post\ninformative review texts. This setting naturally calls for encoding\nuser-specific text with large language models (LLM). However, feeding the full\ntext of all reviews through an LLM has a weak signal-to-noise ratio and incurs\nhigh costs of processed tokens. This paper addresses these two issues. It\npresents a light-weight framework, called CUP, which first computes concise\nuser profiles and feeds only these into the training of transformer-based\nrecommenders. For user profiles, we devise various techniques to select the\nmost informative cues from noisy reviews. Experiments, with book reviews data,\nshow that fine-tuning a small language model with judiciously constructed\nprofiles achieves the best performance, even in comparison to LLM-generated\nrankings.\n","authors":["Ghazaleh Haratinezhad Torbati","Anna Tigunova","Andrew Yates","Gerhard Weikum"],"pdf_url":"https://arxiv.org/pdf/2311.01314v3.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20204v1","updated":"2025-02-27T15:45:16Z","published":"2025-02-27T15:45:16Z","title":"Granite Embedding Models","summary":"  We introduce the Granite Embedding models, a family of encoder-based\nembedding models designed for retrieval tasks, spanning dense-retrieval and\nsparse retrieval architectures, with both English and Multilingual\ncapabilities. This report provides the technical details of training these\nhighly effective 12 layer embedding models, along with their efficient 6 layer\ndistilled counterparts. Extensive evaluations show that the models, developed\nwith techniques like retrieval oriented pretraining, contrastive finetuning,\nknowledge distillation, and model merging significantly outperform publicly\navailable models of similar sizes on both internal IBM retrieval and search\ntasks, and have equivalent performance on widely used information retrieval\nbenchmarks, while being trained on high-quality data suitable for enterprise\nuse. We publicly release all our Granite Embedding models under the Apache 2.0\nlicense, allowing both research and commercial use at\nhttps://huggingface.co/collections/ibm-granite.\n","authors":["Parul Awasthy","Aashka Trivedi","Yulong Li","Mihaela Bornea","David Cox","Abraham Daniels","Martin Franz","Gabe Goodhart","Bhavani Iyer","Vishwajeet Kumar","Luis Lastras","Scott McCarley","Rudra Murthy","Vignesh P","Sara Rosenthal","Salim Roukos","Jaydeep Sen","Sukriti Sharma","Avirup Sil","Kate Soule","Arafat Sultan","Radu Florian"],"pdf_url":"https://arxiv.org/pdf/2502.20204v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.20188v1","updated":"2025-02-27T15:23:18Z","published":"2025-02-27T15:23:18Z","title":"Bisecting K-Means in RAG for Enhancing Question-Answering Tasks\n  Performance in Telecommunications","summary":"  Question-answering tasks in the telecom domain are still reasonably\nunexplored in the literature, primarily due to the field's rapid changes and\nevolving standards. This work presents a novel Retrieval-Augmented Generation\nframework explicitly designed for the telecommunication domain, focusing on\ndatasets composed of 3GPP documents. The framework introduces the use of the\nBisecting K-Means clustering technique to organize the embedding vectors by\ncontents, facilitating more efficient information retrieval. By leveraging this\nclustering technique, the system pre-selects a subset of clusters that are most\nsimilar to the user's query, enhancing the relevance of the retrieved\ninformation. Aiming for models with lower computational cost for inference, the\nframework was tested using Small Language Models, demonstrating improved\nperformance with an accuracy of 66.12% on phi-2 and 72.13% on phi-3 fine-tuned\nmodels, and reduced training time.\n","authors":["Pedro Sousa","Cláudio Klautau Mello","Frank B. Morte","Luis F. Solis Navarro"],"pdf_url":"https://arxiv.org/pdf/2502.20188v1.pdf","comment":"6 pages, 8 figures, accepted at GLOBECOM WORKSHOPS 2024"},{"id":"http://arxiv.org/abs/2502.19962v1","updated":"2025-02-27T10:38:03Z","published":"2025-02-27T10:38:03Z","title":"ReCon: Enhancing True Correspondence Discrimination through Relation\n  Consistency for Robust Noisy Correspondence Learning","summary":"  Can we accurately identify the true correspondences from multimodal datasets\ncontaining mismatched data pairs? Existing methods primarily emphasize the\nsimilarity matching between the representations of objects across modalities,\npotentially neglecting the crucial relation consistency within modalities that\nare particularly important for distinguishing the true and false\ncorrespondences. Such an omission often runs the risk of misidentifying\nnegatives as positives, thus leading to unanticipated performance degradation.\nTo address this problem, we propose a general Relation Consistency learning\nframework, namely ReCon, to accurately discriminate the true correspondences\namong the multimodal data and thus effectively mitigate the adverse impact\ncaused by mismatches. Specifically, ReCon leverages a novel relation\nconsistency learning to ensure the dual-alignment, respectively of, the\ncross-modal relation consistency between different modalities and the\nintra-modal relation consistency within modalities. Thanks to such dual\nconstrains on relations, ReCon significantly enhances its effectiveness for\ntrue correspondence discrimination and therefore reliably filters out the\nmismatched pairs to mitigate the risks of wrong supervisions. Extensive\nexperiments on three widely-used benchmark datasets, including Flickr30K,\nMS-COCO, and Conceptual Captions, are conducted to demonstrate the\neffectiveness and superiority of ReCon compared with other SOTAs. The code is\navailable at: https://github.com/qxzha/ReCon.\n","authors":["Quanxing Zha","Xin Liu","Shu-Juan Peng","Yiu-ming Cheung","Xing Xu","Nannan Wang"],"pdf_url":"https://arxiv.org/pdf/2502.19962v1.pdf","comment":"10 pages, 4 figures, Accepted by CVPR2025"},{"id":"http://arxiv.org/abs/2406.18962v2","updated":"2025-02-27T09:41:12Z","published":"2024-06-27T07:45:17Z","title":"Multi-modal Food Recommendation using Clustering and Self-supervised\n  Learning","summary":"  Food recommendation systems serve as pivotal components in the realm of\ndigital lifestyle services, designed to assist users in discovering recipes and\nfood items that resonate with their unique dietary predilections. Typically,\nmulti-modal descriptions offer an exhaustive profile for each recipe, thereby\nensuring recommendations that are both personalized and accurate. Our\npreliminary investigation of two datasets indicates that pre-trained\nmulti-modal dense representations might precipitate a deterioration in\nperformance compared to ID features when encapsulating interactive\nrelationships. This observation implies that ID features possess a relative\nsuperiority in modeling interactive collaborative signals. Consequently,\ncontemporary cutting-edge methodologies augment ID features with multi-modal\ninformation as supplementary features, overlooking the latent semantic\nrelations between recipes. To rectify this, we present CLUSSL, a novel food\nrecommendation framework that employs clustering and self-supervised learning.\nSpecifically, CLUSSL formulates a modality-specific graph tailored to each\nmodality with discrete/continuous features, thereby transforming semantic\nfeatures into structural representation. Furthermore, CLUSSL procures recipe\nrepresentations pertinent to different modalities via graph convolutional\noperations. A self-supervised learning objective is proposed to foster\nindependence between recipe representations derived from different unimodal\ngraphs. Comprehensive experiments on real-world datasets substantiate that\nCLUSSL consistently surpasses state-of-the-art recommendation benchmarks in\nperformance.\n","authors":["Yixin Zhang","Xin Zhou","Qianwen Meng","Fanglin Zhu","Yonghui Xu","Zhiqi Shen","Lizhen Cui"],"pdf_url":"https://arxiv.org/pdf/2406.18962v2.pdf","comment":"Working paper"},{"id":"http://arxiv.org/abs/2412.11758v2","updated":"2025-02-27T09:02:20Z","published":"2024-12-16T13:22:34Z","title":"Establishing a Foundation for Tetun Text Ad-Hoc Retrieval: Stemming,\n  Indexing, Retrieval, and Ranking","summary":"  Searching for information on the internet and digital platforms to satisfy an\ninformation need requires effective retrieval solutions. However, such\nsolutions are not yet available for Tetun, making it challenging to find\nrelevant documents for text-based search queries in this language. To address\nthese challenges, this study investigates Tetun text retrieval with a focus on\nthe ad-hoc retrieval task. It begins by developing essential language resources\n-- including a list of stopwords, a stemmer, and a test collection -- which\nserve as foundational components for solutions tailored to Tetun text\nretrieval. Various strategies are then explored using both document titles and\ncontent to evaluate retrieval effectiveness. The results show that retrieving\ndocument titles, after removing hyphens and apostrophes without applying\nstemming, significantly improves retrieval performance compared to the\nbaseline. Efficiency increases by 31.37%, while effectiveness achieves an\naverage gain of 9.40% in MAP@10 and 30.35% in nDCG@10 with DFR BM25. Beyond the\ntop-10 cutoff point, Hiemstra LM demonstrates strong performance across various\nretrieval strategies and evaluation metrics. Contributions of this work include\nthe development of Labadain-Stopwords (a list of 160 Tetun stopwords),\nLabadain-Stemmer (a Tetun stemmer with three variants), and\nLabadain-Avaliad\\'or (a Tetun test collection containing 59 topics, 33,550\ndocuments, and 5,900 qrels).\n","authors":["Gabriel de Jesus","Sérgio Nunes"],"pdf_url":"https://arxiv.org/pdf/2412.11758v2.pdf","comment":"Refine the title and improve the content (version 2)"},{"id":"http://arxiv.org/abs/2406.19271v2","updated":"2025-02-27T07:17:52Z","published":"2024-06-27T15:37:57Z","title":"AutoPureData: Automated Filtering of Undesirable Web Data to Update LLM\n  Knowledge","summary":"  Up-to-date and reliable language models are consistently sought after and are\nessential in various applications. Typically, models are trained on a fixed\ndataset and then deployed globally. However, the knowledge of the models\nbecomes outdated. Enabling automatic updation of AI knowledge using web data\ninvolves significant concerns regarding the model's safety and quality due to a\nthreat from unsafe and undesirable text across the web. The purity of new data\nwas essential for updating knowledge of language models to maintain their\nreliability. This paper proposes AutoPureData, a system that automatically\ncollects and purifies web data. The system loaded a sample of web data.\nUtilizing existing trusted AI models, it successfully eliminated unsafe text\nwith an accuracy of 97% and undesirable text with an accuracy of 86%,\ndemonstrating the system's effectiveness in purifying the data. The system\nensures that only meaningful and safe text can be used to update LLM knowledge.\nThe pure text was then optimized and stored in a vector database for future\nquerying. It was found that LLM can fetch new data from the vector DB. The LLM\nwrites the RAG query in English, even if the user's query is in another\nlanguage, proving that the system can perform cross-lingual retrieval. This\npaper proposes a method to maintain the accuracy and relevance of up-to-date\nlanguage models by ensuring that only purified data was used to update LLM\nknowledge. This work contributes to updating knowledge of chatbots using\nmeaningful and safe text, enhancing their utility across various industries,\nand potentially reducing the risks associated with outputs caused by unsafe or\nimpure data. Code is available at github.com/Pro-GenAI/AutoPureData.\n","authors":["Praneeth Vadlapati"],"pdf_url":"https://arxiv.org/pdf/2406.19271v2.pdf","comment":"Final version"},{"id":"http://arxiv.org/abs/2502.17057v2","updated":"2025-02-27T06:00:36Z","published":"2025-02-24T11:15:41Z","title":"LLM-QE: Improving Query Expansion by Aligning Large Language Models with\n  Ranking Preferences","summary":"  Query expansion plays a crucial role in information retrieval, which aims to\nbridge the semantic gap between queries and documents to improve matching\nperformance. This paper introduces LLM-QE, a novel approach that leverages\nLarge Language Models (LLMs) to generate document-based query expansions,\nthereby enhancing dense retrieval models. Unlike traditional methods, LLM-QE\ndesigns both rank-based and answer-based rewards and uses these reward models\nto optimize LLMs to align with the ranking preferences of both retrievers and\nLLMs, thus mitigating the hallucination of LLMs during query expansion. Our\nexperiments on the zero-shot dense retrieval model, Contriever, demonstrate the\neffectiveness of LLM-QE, achieving an improvement of over 8%. Furthermore, by\nincorporating answer-based reward modeling, LLM-QE generates more relevant and\nprecise information related to the documents, rather than simply producing\nredundant tokens to maximize rank-based rewards. Notably, LLM-QE also improves\nthe training process of dense retrievers, achieving a more than 5% improvement\nafter fine-tuning. All codes are available at https://github.com/NEUIR/LLM-QE.\n","authors":["Sijia Yao","Pengcheng Huang","Zhenghao Liu","Yu Gu","Yukun Yan","Shi Yu","Ge Yu"],"pdf_url":"https://arxiv.org/pdf/2502.17057v2.pdf","comment":"13 pages, 5 tables, 4 figures"},{"id":"http://arxiv.org/abs/2502.18470v2","updated":"2025-02-27T05:17:57Z","published":"2025-02-04T01:30:06Z","title":"Spatial-RAG: Spatial Retrieval Augmented Generation for Real-World\n  Spatial Reasoning Questions","summary":"  Spatial reasoning remains a challenge for Large Language Models (LLMs), which\nstruggle with spatial data retrieval and reasoning. We propose Spatial\nRetrieval-Augmented Generation (Spatial-RAG), a framework that extends RAG to\nspatial tasks by integrating sparse spatial retrieval (spatial databases) and\ndense semantic retrieval (LLM-based similarity). A multi-objective ranking\nstrategy balances spatial constraints and semantic relevance, while an\nLLM-guided generator ensures coherent responses. Experiments on a real-world\ntourism dataset show that Spatial-RAG significantly improves spatial question\nanswering, bridging the gap between LLMs and spatial intelligence.\n","authors":["Dazhou Yu","Riyang Bao","Gengchen Mai","Liang Zhao"],"pdf_url":"https://arxiv.org/pdf/2502.18470v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19722v1","updated":"2025-02-27T03:24:57Z","published":"2025-02-27T03:24:57Z","title":"Few-Shot Multilingual Open-Domain QA from 5 Examples","summary":"  Recent approaches to multilingual open-domain question answering (MLODQA)\nhave achieved promising results given abundant language-specific training data.\nHowever, the considerable annotation cost limits the application of these\nmethods for underrepresented languages. We introduce a \\emph{few-shot learning}\napproach to synthesise large-scale multilingual data from large language models\n(LLMs). Our method begins with large-scale self-supervised pre-training using\nWikiData, followed by training on high-quality synthetic multilingual data\ngenerated by prompting LLMs with few-shot supervision. The final model,\n\\textsc{FsModQA}, significantly outperforms existing few-shot and supervised\nbaselines in MLODQA and cross-lingual and monolingual retrieval. We further\nshow our method can be extended for effective zero-shot adaptation to new\nlanguages through a \\emph{cross-lingual prompting} strategy with only\nEnglish-supervised data, making it a general and applicable solution for MLODQA\ntasks without costly large-scale annotation.\n","authors":["Fan Jiang","Tom Drummond","Trevor Cohn"],"pdf_url":"https://arxiv.org/pdf/2502.19722v1.pdf","comment":"Accepted by TACL; pre-MIT Press publication version"},{"id":"http://arxiv.org/abs/2502.15694v2","updated":"2025-02-27T03:08:16Z","published":"2024-12-31T02:44:38Z","title":"Image Fusion for Cross-Domain Sequential Recommendation","summary":"  Cross-Domain Sequential Recommendation (CDSR) aims to predict future user\ninteractions based on historical interactions across multiple domains. The key\nchallenge in CDSR is effectively capturing cross-domain user preferences by\nfully leveraging both intra-sequence and inter-sequence item interactions. In\nthis paper, we propose a novel method, Image Fusion for Cross-Domain Sequential\nRecommendation (IFCDSR), which incorporates item image information to better\ncapture visual preferences. Our approach integrates a frozen CLIP model to\ngenerate image embeddings, enriching original item embeddings with visual data\nfrom both intra-sequence and inter-sequence interactions. Additionally, we\nemploy a multiple attention layer to capture cross-domain interests, enabling\njoint learning of single-domain and cross-domain user preferences. To validate\nthe effectiveness of IFCDSR, we re-partitioned four e-commerce datasets and\nconducted extensive experiments. Results demonstrate that IFCDSR significantly\noutperforms existing methods.\n","authors":["Wangyu Wu","Siqi Song","Xianglin Qiu","Xiaowei Huang","Fei Ma","Jimin Xiao"],"pdf_url":"https://arxiv.org/pdf/2502.15694v2.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19712v1","updated":"2025-02-27T03:07:49Z","published":"2025-02-27T03:07:49Z","title":"Teaching Dense Retrieval Models to Specialize with Listwise Distillation\n  and LLM Data Augmentation","summary":"  While the current state-of-the-art dense retrieval models exhibit strong\nout-of-domain generalization, they might fail to capture nuanced\ndomain-specific knowledge. In principle, fine-tuning these models for\nspecialized retrieval tasks should yield higher effectiveness than relying on a\none-size-fits-all model, but in practice, results can disappoint. We show that\nstandard fine-tuning methods using an InfoNCE loss can unexpectedly degrade\neffectiveness rather than improve it, even for domain-specific scenarios. This\nholds true even when applying widely adopted techniques such as hard-negative\nmining and negative de-noising. To address this, we explore a training strategy\nthat uses listwise distillation from a teacher cross-encoder, leveraging rich\nrelevance signals to fine-tune the retriever. We further explore synthetic\nquery generation using large language models. Through listwise distillation and\ntraining with a diverse set of queries ranging from natural user searches and\nfactual claims to keyword-based queries, we achieve consistent effectiveness\ngains across multiple datasets. Our results also reveal that synthetic queries\ncan rival human-written queries in training utility. However, we also identify\nlimitations, particularly in the effectiveness of cross-encoder teachers as a\nbottleneck. We release our code and scripts to encourage further research.\n","authors":["Manveer Singh Tamber","Suleman Kazi","Vivek Sourabh","Jimmy Lin"],"pdf_url":"https://arxiv.org/pdf/2502.19712v1.pdf","comment":null}],"Multimedia":[{"id":"http://arxiv.org/abs/2503.00071v1","updated":"2025-02-27T17:28:12Z","published":"2025-02-27T17:28:12Z","title":"I see what you mean: Co-Speech Gestures for Reference Resolution in\n  Multimodal Dialogue","summary":"  In face-to-face interaction, we use multiple modalities, including speech and\ngestures, to communicate information and resolve references to objects.\nHowever, how representational co-speech gestures refer to objects remains\nunderstudied from a computational perspective. In this work, we address this\ngap by introducing a multimodal reference resolution task centred on\nrepresentational gestures, while simultaneously tackling the challenge of\nlearning robust gesture embeddings. We propose a self-supervised pre-training\napproach to gesture representation learning that grounds body movements in\nspoken language. Our experiments show that the learned embeddings align with\nexpert annotations and have significant predictive power. Moreover, reference\nresolution accuracy further improves when (1) using multimodal gesture\nrepresentations, even when speech is unavailable at inference time, and (2)\nleveraging dialogue history. Overall, our findings highlight the complementary\nroles of gesture and speech in reference resolution, offering a step towards\nmore naturalistic models of human-machine interaction.\n","authors":["Esam Ghaleb","Bulat Khaertdinov","Aslı Özyürek","Raquel Fernández"],"pdf_url":"https://arxiv.org/pdf/2503.00071v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2502.19937v1","updated":"2025-02-27T10:04:47Z","published":"2025-02-27T10:04:47Z","title":"Image Referenced Sketch Colorization Based on Animation Creation\n  Workflow","summary":"  Sketch colorization plays an important role in animation and digital\nillustration production tasks. However, existing methods still meet problems in\nthat text-guided methods fail to provide accurate color and style reference,\nhint-guided methods still involve manual operation, and image-referenced\nmethods are prone to cause artifacts. To address these limitations, we propose\na diffusion-based framework inspired by real-world animation production\nworkflows. Our approach leverages the sketch as the spatial guidance and an RGB\nimage as the color reference, and separately extracts foreground and background\nfrom the reference image with spatial masks. Particularly, we introduce a split\ncross-attention mechanism with LoRA (Low-Rank Adaptation) modules. They are\ntrained separately with foreground and background regions to control the\ncorresponding embeddings for keys and values in cross-attention. This design\nallows the diffusion model to integrate information from foreground and\nbackground independently, preventing interference and eliminating the spatial\nartifacts. During inference, we design switchable inference modes for diverse\nuse scenarios by changing modules activated in the framework. Extensive\nqualitative and quantitative experiments, along with user studies, demonstrate\nour advantages over existing methods in generating high-qualigy artifact-free\nresults with geometric mismatched references. Ablation studies further confirm\nthe effectiveness of each component. Codes are available at https://github.com/\ntellurion-kanata/colorizeDiffusion.\n","authors":["Dingkun Yan","Xinrui Wang","Zhuoru Li","Suguru Saito","Yusuke Iwasawa","Yutaka Matsuo","Jiaxian Guo"],"pdf_url":"https://arxiv.org/pdf/2502.19937v1.pdf","comment":null},{"id":"http://arxiv.org/abs/2406.19859v4","updated":"2025-02-27T08:36:29Z","published":"2024-06-28T11:58:26Z","title":"MetaDesigner: Advancing Artistic Typography Through AI-Driven,\n  User-Centric, and Multilingual WordArt Synthesis","summary":"  MetaDesigner introduces a transformative framework for artistic typography\nsynthesis, powered by Large Language Models (LLMs) and grounded in a\nuser-centric design paradigm. Its foundation is a multi-agent system comprising\nthe Pipeline, Glyph, and Texture agents, which collectively orchestrate the\ncreation of customizable WordArt, ranging from semantic enhancements to\nintricate textural elements. A central feedback mechanism leverages insights\nfrom both multimodal models and user evaluations, enabling iterative refinement\nof design parameters. Through this iterative process, MetaDesigner dynamically\nadjusts hyperparameters to align with user-defined stylistic and thematic\npreferences, consistently delivering WordArt that excels in visual quality and\ncontextual resonance. Empirical evaluations underscore the system's versatility\nand effectiveness across diverse WordArt applications, yielding outputs that\nare both aesthetically compelling and context-sensitive.\n","authors":["Jun-Yan He","Zhi-Qi Cheng","Chenyang Li","Jingdong Sun","Qi He","Wangmeng Xiang","Hanyuan Chen","Jin-Peng Lan","Xianhui Lin","Kang Zhu","Bin Luo","Yifeng Geng","Xuansong Xie","Alexander G. Hauptmann"],"pdf_url":"https://arxiv.org/pdf/2406.19859v4.pdf","comment":"Accepted by ICLR 2025, Project:\n  https://modelscope.cn/studios/WordArt/WordArt"},{"id":"http://arxiv.org/abs/2502.19834v1","updated":"2025-02-27T07:14:11Z","published":"2025-02-27T07:14:11Z","title":"Knowledge Bridger: Towards Training-free Missing Multi-modality\n  Completion","summary":"  Previous successful approaches to missing modality completion rely on\ncarefully designed fusion techniques and extensive pre-training on complete\ndata, which can limit their generalizability in out-of-domain (OOD) scenarios.\nIn this study, we pose a new challenge: can we develop a missing modality\ncompletion model that is both resource-efficient and robust to OOD\ngeneralization? To address this, we present a training-free framework for\nmissing modality completion that leverages large multimodal models (LMMs). Our\napproach, termed the \"Knowledge Bridger\", is modality-agnostic and integrates\ngeneration and ranking of missing modalities. By defining domain-specific\npriors, our method automatically extracts structured information from available\nmodalities to construct knowledge graphs. These extracted graphs connect the\nmissing modality generation and ranking modules through the LMM, resulting in\nhigh-quality imputations of missing modalities. Experimental results across\nboth general and medical domains show that our approach consistently\noutperforms competing methods, including in OOD generalization. Additionally,\nour knowledge-driven generation and ranking techniques demonstrate superiority\nover variants that directly employ LMMs for generation and ranking, offering\ninsights that may be valuable for applications in other domains.\n","authors":["Guanzhou Ke","Shengfeng He","Xiao Li Wang","Bo Wang","Guoqing Chao","Yuanyang Zhang","Yi Xie","HeXing Su"],"pdf_url":"https://arxiv.org/pdf/2502.19834v1.pdf","comment":"Accepted to CVPR 2025"}]}}